
<head>
  <title>Technosocial Development in an Open Society</title>
  <script type="text/javascript" src="/static/noddy.writer.js"></script>
</head>

<style type="text/css">
.embossed{
  margin-top:120px;
  margin-bottom:100px;
}
#content{
  max-width:700px;
  margin:100px auto 200px auto;
}
</style>



<script type="text/javascript">
jQuery(document).ready(function () {

  noddy.writer({url:'{{api}}/es/phd/reference/_search?dev=false'});

  //$('#content').oap_refs({'appendto':'#references','storage_url':'{{api}}/es/phd/reference/_search?dev=false'}).oap_figs().oap_headers().oap_toc({'prependto':'#contents', depth:5});

  /*if ( $(window).width() > 1200 ) {
    $('#maincontents').addClass('visible-print');
    $('#content').css({'margin': '100px 10% 200px auto'});
    $('#content').before('<div id="csidecontents" class="hidden-print" style="position:fixed;top:0;left:0;bottom:0;right:0;width:400px;border-right:2px solid #ccc;padding:5px;background-color:#eee;z-index:1000000;"><div id="sidecontents" style="margin-left:-45px;font-size:0.7em;overflow-y:auto;height:100%;"></div></div>');
    $('#content').toc({'prependto':'#sidecontents', depth:4, url:'{{api}}/es/phd/reference/_search?dev=false'});
  }*/

  var getstatement = function() {
    var qry = {
      "query":{
        "match_all":{}
      },
      "facets": {
        "scholarship": {"terms": {"field":"scholarship"}},
        "research":  {"terms": {"field":"research"}},
        "output":  {"terms": {"field":"output"}},
        "methods":  {"terms": {"field":"methods"}},
        "contribute":  {"terms": {"field":"contribute"}},
        "access":  {"terms": {"field":"access"}},
        "review":  {"terms": {"field":"review"}},
        "attribution":  {"terms": {"field":"attribution"}}
      }
    };
    $.ajax({
      "method":"GET",
      "datatype":"JSON",
      "url": "{{api}}/es/phd/scholarship/_search?dev=false&source=" + JSON.stringify(qry),
      "success": function(data) {
        var scholarship = '<b>' + data.facets.scholarship.terms[0].term + '</b>';
        var research = '<b>' + data.facets.research.terms[0].term + '</b>';
        var output = '<b>' + data.facets.output.terms[0].term + '</b>';
        var methods = '<b>' + data.facets.methods.terms[0].term + '</b>';
        var contribute = '<b>' + data.facets.contribute.terms[0].term + '</b>';
        var access = '<b>' + data.facets.access.terms[0].term + '</b>';
        var review = '<b>' + data.facets.review.terms[0].term + '</b>';
        var attribution = '<b>' + data.facets.attribution.terms[0].term + '</b>';
        var stmt = '<p>Scholarship is ' + scholarship + ", " + research + " education and research, the materials and output of which are " + output + " via " + methods + " distribution methods that are ";
        stmt += contribute + " to contribute to and " + access + " to access, and where quality is maintained via " + review + " review, and where the primary purpose of attribution is " + attribution + ".</p>";
        $('#statementhero').prepend(stmt);
      }
    });
  }
  getstatement();


  var first = true;
  $('#listsgraph').graphview({
    "target": '{{api}}/es/phd/lists/_search?dev=false',
    "pushstate": false,
    "titlefield": "subject",
    "defaultquery": {
      "query": {
        "bool": {
          "must":[]
        }
      },
      "from":0,
      "size":100,
      "fields":[
        "from.exact",
        "organisation.exact",
        "subjected_lists.exact",
        "references.exact",
        "in-reply-to.exact",
        "tags.exact",
        "subject"
      ],
      "facets":{
        "subjects": {"term":{"field":"subject.exact","suggest":true}},
        "from": {"term":{"field":"from.exact","suggest":true, "node": true}},
        "organisation": {"term":{"field":"organisation.exact","suggest":true, "node": true}},
        "lists": {"term":{"field":"subjected_lists.exact","suggest": true, "node": true}},
        "terms": {"term":{"field":"tags.exact","suggest": true, "node": true}}
      }
    },
    "afterresults": function() {
      if ( first ) {
        $('#listsgraph .graphview_nodetype').each(function() {
          if ( $(this).attr('data-field') == "from.exact" || $(this).attr('data-field') == "subjected_lists.exact" ) {
            $(this).attr('checked','checked');
          }
        });
        $('#listsgraph .graphview_to').val('1000').trigger('change');
        first = false;
      }
    },
    "nodesize": 100,
    "focusdepth": 1
  });

});
</script>



<div class="container" id="content">

<h1 id="titleheader" style="text-align:center;" class="ignore"><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Technosocial Development <br>in an Open Society</span></h1>
<p style="text-align:center;">Augmenting our practice with tools to acknowledge the technosocial nature of development. Demonstrated by studies of the values of scholarly practice, and how they are reconstructed by attempts to open scholarly society.</p>

<p style="text-align:center;"><span xmlns:cc="http://creativecommons.org/ns#" href="http://phd.cottagelabs.com" property="cc:attributionName" rel="cc:attributionURL">Mark MacGillivray</span><br>
<a target="_blank" class="hidden-print" href="http://orcid.org/0000-0002-9288-8160">http://orcid.org/0000-0002-9288-8160</a><span class="printref">http://orcid.org/0000-0002-9288-8160</span></p>

<p style="text-align:center"><img src="/static/phd/eushield-clear-200.png" alt="University of Edinburgh crest" title="University of Edinburgh crest" class="aligncenter" style="margin:50px 0 50px 0;" /></p>

<p style="text-align:center;">Thesis submitted on 30th April 2015 to the University of Edinburgh<br>
Laboratory for Foundations of Computer Science<br>
<a class="hidden-print" href="http://www.inf.ed.ac.uk">School of Informatics</a><span class="printref">School of Informatics</span>, University of Edinburgh, UK<br>
for the degree of Doctor of Philosophy<br>
Supervisors: Prof. Stuart Anderson and Dr. Mark Hartswood</p>

<p style="text-align:center;color:#888;font-size:1em;padding-top:20px;">
This work is licensed under a <a class="hidden-print" rel="license" target="_blank" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>
<span class="printref">Creative Commons Attribution 4.0 International License</span></p>

<!--
<div class="hidden-print">

<p>CORRECTIONS NOTES</p>

<p>Provide a glossary of key terms particularly where they are novel or used in a novel way in the thesis. The glossary should be sufficient to allow members of the four audiences to access the content of the thesis. (DONE)</p>

<p>Chapter 1 add short section to clarify the intended target audiences for this thesis. (DONE new section added - and also augmented final paragraph of the section to relate back to it, and the final paragraph of the abstract to relate to it)</p>

<p>Move section on ANT 2.2.3 to start of Chapter 2 (DONE). Add a brief description of Latour's Pasteur example (DONE - it is in the now moved ANT section, three paragraphs above the beginning of the software development as value pracrtice section)</p>

<p>Add a diagram to illustrate the main observations of chapters 2 to 4 respectively at the start of these chapters (DONE)</p>

<p>In section 3.2.2 supply brief illustrative extracts (around 6 lines) from the interview transcripts for at least three of the interviews (DONE)</p>

<p>Add an actor-network diagram for each case study in chapter 3 (DONE for those in s3.1 whereas those in s3.2 are supported by video, survey data, and conclude in s3.3 with automated visualisation anyway)</p>

<p>Reformulate the conclusions in chapter 5 as separate take home messages for each of the four target audiences (DONE)</p>

<p>Redo all diagrams for legibility in print form DONE, and check for clear narratives in captions DONE</p>

<p>Check body font is at least 10pt when printed and typeset the page numbering (DONE for FF print instead of chrome, just fix references and make figs full width DONE, ff will provide suitable page numbering) (also should be not more than 12pt according to printing guidelines. Also removed hr before and after abstract, and before studies, where also added a padding paragraph to fix page break for print - but put this back for online version neatness once pdf copy is taken)</p>

<p>Ensure that sufficient metadata is present for all bibliographic references DONE: ref 3 and 40 (in original doc) are the same - differentiate them. Ref 66 - 67 re. Pironti in wikipedia may be incorrect, is text on the claimed doc? - changed the URL. unquoted the phrase, it does not appear in the doc but is an overview of the doc on the pre-referred wikipedia page, as now acknowledged in this text. Removed the ref (3 in original doc) to co-realisation from third para of intro, as it was not detailed there any more and is now detailed later at the second ref point anyway. Also added last accessed date to all URLs in references, of end September 2015, when each link was last checked.</p>

<p>Include the README file from the CD digital submission as Appendix A (DONE)</p>

<p>ALSO incorporated a number of fixes as highlighted by Ewan in his viva copy. Typing errors mostly, some structural, and some re-wording of the outcomes that seemed appropriate particularly given the changes to the conclusions. To share access to the diff of the edits so that Ewan can view.</p>

</div>
-->


<hr class="embossed"></hr>

<h2 class="ignore">Abstract</h2>

<p>Social and technical advancements have enabled a greater degree of communication and collaboration than ever before. Amongst other things, this has allowed for a particularly interesting triumvirate intersection:</p>

<ul>
<li>software development practices have become agile, able to value individuals and interactions over tools and processes, and to localise developments with user communities</li>
<li>it has driven the evolution of digital scholarship which has vastly increased the quantity of research material potentially available to scholars, and greatly simplified the task of sharing research data</li>
<li>and the unprecedented accessibility to and potential scale of collaborative research - on everything from writing software to cake recipes to extremist propaganda to space exploration surveys - has driven a number of social movements to open up the societal and institutional boundaries that traditionally characterise what it means to be a scholar, a researcher, and even a member of a given community or society.</li>
</ul>

<p>These three are each made possible by, and contribute to, new scholarly practices; for example software developers find that extending their development practice may lead to the creation of even better tools, whilst scholars realise that new tools enable collaborations that lead to new discoveries in their community. However, the specialist nature of advanced work in research and/or development also require a degree of localisation that may present new problems - the developer may find that their close interaction with scholars leads to inheritance of scholarly traditions as requirements, because the scholar finds that their community practice still mandates such traditions. Individuals and teams facing these problems may then find themselves at odds with their professional community (their locality), prompting some form and degree of <i>delocalisation</i>.</p>

<p>This thesis investigates the concern that localisation affects the development process itself, and that this results in friction (and possibly failure) when delocalisation follows. Contrasting examples of this are found when developments are limited in either their <i>technical</i> or <i>social</i> scope, resulting in two types of instantiation of the problem of delocalisation, characterised as follows:</p>

<ul>
<li>A locally successful technical solution may be deemed exportable to another community without consideration of differences in social practice</li>
<li>A new practice (such as a form of digital scholarship) may be considered an advancement of community values, but only a subset of members import or accept the technical capability to perform it</li>
</ul>

<p>Whilst such localisations and their accompanying problems of delocalisation may be quite natural and perhaps even necessary for a development project to get under way, the proliferation of movements to push boundaries and to <i>open</i> societies and communities suggests that the problem is common, and in need of further consideration. This thesis proposes that development of (at least a significant subset of) technology is <i>one and the same thing</i> as development of society, and that advancing a practice of <i>technosocial</i> development may therefore be of value.</p>

<p>To investigate this, two sets of studies were performed in parallel throughout 2010 to 2014, each set characterised by their locality in either traditionally technical or traditionally social developments. One set studies a selection of open source software development projects and their attempts to export technical solutions, whilst the second set comprises a collection of qualitative interviews and ethnographies of practice of a range of scholars, describing their attempts to import aspects of digital scholarship into their community practices, and how they find themselves questioning and reconstructing (and in some cases advocating against) the values of the community in which they are localised. Throughout each set, contrasting qualities of the delocalisation problem are observed.</p>

<p>The outcomes of the studies suggests that the values of a community are naturally expressed in and traditionalised by the practices of that community, and that development is often a reconstruction of community values as introducing new tools can change practice. Such developments should therefore be considered a technosocial practice, and so tools that bring value reconstruction to the fore may better acknowledge the (probably unavoidable) problems of delocalisation. Prototypes of such tools are demonstrated, using as examples the reconstructions of scholarly practices observed during the studies.</p>

<p>The thesis concludes with a summary of contributions to four target audiences - software developers, stakeholders in developments, social scientists, and digital scholars. Further work is proposed for further studies of the practice of technosocial development utilising the tools developed during this thesis, which have been named Leviathan.</p>

<hr class="embossed"></hr>

<h2 class="ignore">Acknowledgements</h2>

<p>This work represents the efforts of many, and I am only the author in the sense that I put some words next to some other words. Many people helped me to do this, and I'd like to thank Stuart, Mark, Massimo and everyone at Edinburgh that I have learnt something from over the years, and of course all of my friends and family.</p>

<p>I would like to acknowledge the support of the Engineering and Physical Sciences Research Council for funding my work, and to Jisc and the Public Library of Science for their funding of many of the projects I studied and contributed to over the last four years. And of course this work could not have been done without the kind participation of the people that agreed to take part in my studies.</p>

<p>I also acknowledge and thank Ant and Laura and Eva, for their help with videos and early drafts, along with anyone that made or contributed to the open source software that I have used.</p>

<p>Finally, many thanks to Richard and all the Cottage Labs people, and Peter and Rufus at the Open Knowledge Foundation.</p>

<!--<hr class="embossed"></hr>-->

<h2 class="ignore">Declaration</h2>

<p>I declare that I have personally composed this thesis and that, except where indicated, it is all my own work. The material herein, except where indicated, has not been published anywhere else and has not been submitted for any other degree or professional qualification. Where sections of this work contain materials produced with co-operation of others or submitted in relation to the projects they describe, such sections are prefixed with an explanation of the capacity in which such co-operations and submissions took place. In every such case, permission has been obtained for reproduction / summary and presentation within this thesis.</p>

<div class="visible-print">
<p><br>SIGNED</p>

<p><br><br>DATED</p>
</div>

<div id="maincontents">
<hr class="embossed"></hr>
<h2 class="ignore">Contents</h2>
<div id="contents"></div>
</div>

<hr class="embossed"></hr>

<h2 class="ignore">Glossary</h2>

<p>This thesis presents interdisciplinary research drawing on backgrounds in software engineering, social anthropology, philosophy of science, the open access movement. There are terms in use in each field that can infer meaning in both positive and negative ways, and understanding those terms cannot necessarily rely on knowledge of just one of those disciplines. This glossary provides an overview of such key terms, to help readers from the various disciplines approach this work and the use of such terms in a broader manner, and also clarifies more precise meaning for some terms and for some with unique application in this work.</p>

<p><b>Technosocial</b> is used uniquely in this work as an alternative to sociotechnical - that is, the combination of social and technical matters in acknowledgement of their dependence and effect upon each other. However, it is desirable in this thesis to distinguish between traditional research of the sociotechnical, which can imply that one may shape the other, and instead to propose to proceed on the basis that they are one and the same thing.</p>

<p><b>Open</b> is often and widely used to prefix other terms to indicate what is usually a relatively new and/or alternative approach to a traditional problem. This thesis will refer to open access, open source, open society, and other open movements in general.</p>

<p><b>Agile</b> is the name of a particular approach to software development. For readers who are not software developers, be assured that to develop software in an agile manner simply means to do so following a (sub)set of agile methodologies. Where specific knowledge of these is required it is referred within this thesis.</p>

<p><b>Actor</b> means an individual entity involved in some sort of interaction. The entity need not be human, but simply anything identifiable by observed interaction, even if nothing more than the observance of those interactions. For sociologists this will be familiar as the actor of actor-network theory (detailed in the Backgrounds), and for software developers this may be familiar to the concept of object orientation, in that an actor is an instantiaion of observable interactions, much as a software object can be instantiated to provide certain interactions on a class of software.</p>

<p><b>Network</b> is used in this thesis mainly as in actor-network theory. However, it is fitting that for software developers it is likely to encourage thinking of collections of actors (software entities, human beings, or otherwise) as networks, although it should not be taken to mean a network that can be configured but more as a network that emerges. It is not the cables of the internet, but the emergent relations between entities observable by their communications.</p>

<p><b>Localisation</b> can be interpreted as in natural usage. There are elements of localisation that relate to software development such as in localising a product, and there are elements of localisation in any form of research where a target for study is identified. Also, any community is a localisation of sorts - a group of people identifiable by geographical localisation, or perhaps by membership of an organisation, or by belief in some ideal. In this thesis a development <i>is</i> a localisation in that it is something (a process and eventually an output) around which an identifiable group localises. The main purpose of this sense of localisation is to support studies of delocalisation.</p>

<p><b>Delocalisation</b> is used in a novel manner in this thesis, as a name for the events that occur when development of a given localisation is attempted. There is prior use of the term but it is used to mean situations such as where administrative tasks of a work process are delocalised from their initial localisation for example by moving certain personal banking support roles out of local branches and to a national call centre (perhaps in a different nation). However in this thesis delocalisation is considered to be intrinsic to localisation and development, rather than an outcome of a particular sort of development - delocalisation is not the act of <i>moving</i> support roles out of a localisation but is an unavoidable part of the development process itself; to develop anything, from any local perspective, is to delocalise from that local perspective by augmenting the ability of local and remote actors to perceive and interact with the functions that define the locality.</p>

<p><b>Value</b> is a very common term but also has a specific meaning in the field of valuation studies. The term is used in this thesis in relation to valuation studies, which is explained further in the Backgrounds. A short example is that software development itself could be considered a localisation of developers around a given set of practices which can therefore be said to be valued by those developers. Those practices may include methods of evaluation, such as software quality testing, but the methods are <i>valued</i> simply because the community formed around them finds those evaluative methods useful to apply and share. Valuation studies as a research discipline is not about studying evaluative methods of any given community, but about studying the imbuing of value by members of a community onto certain entities (which could be methods, rituals, concepts, theories, and so on).</p>

<p><b>Reconstruction</b> in this thesis is what occurs with any sort of technosocial development; the natural necessity of localisation in defining the starting point of a development ensures situation in a certain set of value practices, and as development progresses it is those value practicecs that define the locality of the development that will be changed by the development. Hence delocalisation is inevitable, which in turn will give rise to questioning of values. In order to perform the development a reconstruction of values is inevitably necessary - and the development will only be accepted if the practices augmented by development continue to be valued by (or become valued by) the community or communities to which they pertain.</p>

<p><b>Co-realisation</b> is a proposed collaborative approach to IT systems design and development that relates to the work of this thesis. The term is not novel to this thesis although the practice it describes does have similarlities to what is proposed in this thesis, and could be considered as inspiration to this work. Co-realisation is described further in the Backgrounds.</p>

<p><b>Co-design</b> is mentioned by one of the people that were interviewed as research subjects during the part of this thesis in which different scholarly actors were followed. It is similar to co-realisation in that it encourages greater collaboration between different actors involved in designing something. In the study presented in this thesis, it was about designing research funding strategies by including representatives of funders, developers, and users on a funding decision panel. It has little other impact on this thesis, other than being similar to co-realisation as an inspiration to this work.</p>

<p><hr class="embossed"></hr></p>
<h2>Introduction</h2>

<p>It seems easy to naively distinguish between societies and technologies... people can think, machines cannot; technologies are manufactured, societies are not. However it is also common to anthropomorphise technologies. The reality is that societies and their technologies are indistinguishable - language, for example, is perhaps the basis of a society and yet is also a technology. The things that make up a society are technological, and the things that make technologies are societal <a href="">#whnbm</a>.</p>

<p>When performing research and development, a less naive approach is required, and it is usually necessary to prioritise or specialise. A software developer may build a tool that meets a clear set of requirements, a sociologist may study well defined groups of people, a computer scientist may theorise on how to develop artificial intelligence. In many cases such specialisation is reasonable and highly valuable, and specialists have practices of working together to ensure they can deliver useful outcomes. For example, it is a common scientific practice to consider a problem in isolation and to propose a solution given certain assumptions (or simplifications) about the surrounding environment. Localising a problem is therefore a recognised and valuable technique, so it is not surprising that there are such localisations in the ways things are done, and in particular in how research is performed - there are schools, departments, subjects, research areas, and so on. These specialisations in research and development have driven social and technical advancements that now enable a greater degree of communication and collaboration than ever before.</p>

<p>This has allowed for the practice of <i>software development</i> to become more agile, to "value individuals and interactions over tools and processes" <a href="http://agilemanifesto.org/">#agileman</a>, and to narrow the gap between developers and the community into which the tool they are developing is to be deployed. Software developers may now find that by localising their development practice to a target community, better and more advanced tools can be specified and developed. Developers can also take advantage of new tools to interact and collaborate to greater degrees, and it is now common to find the code created during development projects shared publicly online in "social coding" sites such as github <a href="">#GITHUB</a>, and for developers to share problems and solutions on community sites such as stackoverflow <a href="">#stackoverflow</a>. It may be that, as a relatively new discipline, uptake of new tools that augment practice is easy for developers.</p>

<p>These same tools that are used by developers are also of value to other similarly academic pursuits. Upon the web a new <i>digital scholarship</i> has emerged, vastly increasing the quantity of research material potentially available to scholars, and greatly simplifying the task of sharing research output. Many scholars have realised that these new tools not only allow sharing of new discoveries but enable collaborations at a scale previously impossible, leading to new discoveries for their research community that simply could not have been achieved just ten years ago. But, perhaps because scholarly practices have formed far older traditions than those of software developers, uptake across the wider scholarly community has not been so easy.</p>

<p>It is a feature of the specialist nature of advanced work in research or development that a degree of <i>localisation</i> should occur - that is, the practices of a given community become part of the tradition of that community, and the community is in some sense defined by what it <i>believes</i> and <i>can do</i>. This is why, even in the relatively new community of software development, the concept of valuing individuals and interactions over tools and practices formed part of an agile <i>manifesto</i>, a very clear statement of departure from tradition. Localisation, then, is a natural occurrence of an emerging community, and even a necessary feature of any possible statement on what it is to <i>be</i> a community. And, unless communities are to remain static and unchanging after some period of formative development, neither admissive or emissive of members and practices, some degree of <I>delocalisation</i> must also be occurring as a community develops into and sustains itself as a mature society. However, because part of the function of specialisation is to localise on a problem, the natural occurrence of delocalisation may be overlooked (or even actively obfuscated) when development of a solution takes place, leading to unforeseen problems of delocalisation. For example the developer may find that their close interaction with scholars leads to inheritance of scholarly traditions as requirements of the tools they develop, because the scholar finds that their community practice still mandates such traditions. Individuals and teams may then find themselves at odds with their professional community (their locality) when they try to use or make new tools and form new practices around them, precipitating a move to delocalise from community tradition.</p>

<p>The unprecedented accessibility to and scale at which collaborative research and development can now be performed has therefore given rise to a number of <i>open movements</i>, seeking to transcend the societal and institutional boundaries that traditionally characterise what it means to be a scholar, a researcher, and even what it means to define and be defined as a member of a given community or society. So, whilst such localisations and their accompanying problems of delocalisation may be quite natural and perhaps even necessary for a development project to get under way, the proliferation of movements to push boundaries and to open societies and communities suggests that the problem is common, and in need of further consideration. This thesis investigates the concern that localisation affects the development process itself, and that this results in friction (and possibly failure) when delocalisation inevitably follows. Contrasting examples of this are found when developments are limited in either their <i>technical</i> or <i>social</i> scope, resulting in two types of instantiation of the problem of delocalisation, characterised as follows:</p>

<ul>
<li>A locally successful technical solution may be deemed exportable to another community without consideration of differences in social practice</li>
<li>A new practice (such as a form of digital scholarship) may be considered an advancement of community values, but only a subset of members import or accept the technical capability to perform it</li>
</ul>

<p>In fact, technical development unavoidably interacts with the community it is being developed for, just as observation unavoidably interacts with the observed. Development of (at least a significant subset of) technology is therefore <i>one and the same thing</i> as development of society. Just as space and time are not independent of one another, even if naively conceptualised as so, development must be thought of as a <i>hybrid</i> technosocial practice.</p>

<p>This thesis proposes that in many development scenarios - as exemplified by those occurring where the triumvirate of software development, digital scholarship, and open movements coincide - are not solely technical or social but are technosocial, and they constitute a reconstruction of the traditional values and practices of the community being developed; that localisation of development within a community can obfuscate the unavoidable delocalisation it will precipitate; and that studies of some software developments and open movements may help to advance a practice of technosocial development. The outcomes of these studies should enable development of tools that help to tackle the problems of delocalisation by bringing value reconstruction to the fore.</p>

<p>Researching delocalisation must begin by considering how localisation occurs, and by understanding how delocalisations can be identified for study.</p>

<h3>Target audience(s)</h3>

<p>This interdisciplinary work will unsurprisingly have more than one target audience. Whilst the scope of audience should not be considered limited, and all those with an interest are welcome to read on, there are four target audiences in particular, for whom the conclusions will be tailored:</p>

<p><b>Software developers</b> are likely to find benefit in this work, as it presents studies of a range of software development projects, and considers them from a novel perspective. Any software development project faced with a complex network of both social and technical issues may find the prototype tools demonstrated herein to be of use, either directly as a tool to be used in such situations or at least as a prompt to an alternative way of thinking about such problems.</p>

<p><b>Stakeholders</b> in developments - particularly those where the development requires negotiation across organisational or academic / professional / disciplinary boundaries (or such as in open movements), will find that there is supporting evidence of the complexities that can arise in such situations, and that experiencing seemingly superficial issues becoming rapidly complicated should not be unexpected! There are some perhaps unintuitive ways to overcome this, which may be easier to present to colleagues when supported by accounts of occurrences elsewhere.</p>

<p><b>Social scientists</b> may find the application of new indexing and full text search technologies to the tasks of analysis of ethnographic and survey studies to be of particular interest, as well as learning of a novel application of social science methods to software development projects.</p>

<p><b>Digital scholars</b> in general may find studies of other scholars to be enlightening, particularly where there are common issues affecting various scholars when it comes to agreeing what sorts of source materials are to be considered suitable to their particular disciplines, or in considering what sorts of processes are good enough to validate such materials, and how such considerations change as new developments occur.</p>

<h3>The emergence of localisation</h3>

<p>I think the best way to communicate this concept of localisation, given the nature of the thing, is to begin from the ultimately local perspective. So this introductory narrative begins with my self, and a story of a local perspective that should sound natural and familiar. It alludes to numerous research concepts that will be detailed in the backgrounds.</p>

<h4>A philosopher strays from the path</h4>

<p>I began my academic studies in philosophy. Of particular interest were the topics of logic, philosophy of science, and philosophy of mind. I learnt the rules of propositional logic and primitive recursive arithmetic, I learnt about how science is performed and about how scientific theories evolve, and I learnt about the technology of language and the various theories of how the mind is situated in the physical world. More detail of relevant philosophical works will be provided in the backgrounds, but what matters for now is that I <i>specialised</i> in philosophy, and especially in logic. Unfortunately being a philosopher is not an easy way to make a living, and so I slightly diversified my specialist skills into computer programming. I strayed from the philosophical path of the search for wisdom and wandered into the creative art of software development.</p>

<p>I enjoyed finding ways to use my skills to develop solutions for the problems that people had in their daily lives - being able to use logic and thought to actually make something that alters the way the world works is very satisfying. Of course it was possible to have similar impact before the invention of software, through literature and politics for example - but software development seemed far more accessible. Perhaps also as a result of starting as a poor philosopher, I began using open source software that I could find easily and for free online, and that I could learn about by collaborating, again easily and freely, with other similar people on the web.</p>

<p>So I became a software developer without any formal training, and given my starting position in quite an academic setting as a philosopher, I became a developer of software tools for other academics. However I soon found that there was a common problem in development. I would typically be hired by one department or one research group and asked to make something that took the usual form of a computer programme - receive a certain data input, process it in some specified way, and output it as and when required. For simple cases this was fine, but complications would often arise - and they were rarely technical complications.</p>

<p>The complication was that the simple technical process of data input and output quite often spanned multiple social domains (as is often the case, the performance of a process is not as straightforward as the theory), and in some cases the people commissioning the work (a particular community of researchers) did not have authority over the wider domain of all of the providers of data input or the consumers of data output. So the neat small localised group of people working on the problem, including myself, that had specified the requirements of the tool found that a solution was hard to deliver - because the desire of other communities to maintain their traditional practices had not been taken into account. We were trying to cross social boundaries that were beyond our software development practices, and we were unaware we had inherited local concepts of department, subject, practice.</p>

<p>This problem occurred frequently, and so I began a search for a general (perhaps even an ideal) solution.</p>

<h4>Creative art becomes profession</h4>

<p>So I undertook postgraduate studies in computer science, and specifically software development in a research setting. I learnt about the software engineering body of knowledge <a href="">#SWEBOK</a>, of various software development practices, and essentially specialised further to the point of professionalism, and with validation from a reputable university. In addition to a greater repertoire of practices to employ when faced with complex development project planning and specification issues, I also gained a certain amount of ability to transcend the aforementioned boundaries, because as a professional my opinion could be taken seriously by others beyond the community in which I had previously been embedded. I was now, instead, somewhat outside and beyond that constraint.</p>

<p>The professionalisation of any activity seeks to make it conformant, recognisable, and respectable, forming traditions around practices that communicate the values of the profession. By becoming such a professional I had therefore developed the power of authority, with the weight of tradition behind me and with a collection of tools (like gantt charts, for example) and practices (such as use case elicitation) to appeal to when asked to justify my responses to difficult questions.</p>

<p>This was a mixed blessing. I was now able to transcend some boundaries and to somewhat align disparate communities toward a common developmental goal, but I had become a witch doctor of sorts - someone to whom a community turns when answers are sought. The "magic of the techie" was no longer my performing an illusion, but had become part of the belief system of groups of people seeking enlightenment in certain situations. This was not my intention, nor the fault of those turning to me, but a common, recognisable, natural human interaction.</p>

<h4>Problems and solutions become localised</h4>

<p>As I had found a way of dealing with certain sorts of problem, I had found a way to extend my locality beyond my initial community - or so it seemed. However, although I could take my professional skills and apply them in multiple settings, I found that in some sense I was exporting solutions that had worked elsewhere, or in some cases importing further communities into my understanding of my local one. This increased the likelihood that the solutions I was capable of practicing would be effective, almost to the point that the locality in which a problem existed became the same as that to which a particular set of practices could be successfully applied. Any community that could be understood by the practical framework became part of the local, whereas any that did not would be deemed unsuitable.</p>

<p>It is part of the meaning of being a professional that one is capable of providing answers, and part of the meaning of a profession that it can be applied to certain domains - a body of knowledge based on previous experience, that is expected to work again in similar contexts. It thus becomes less likely that the tools and practices of the profession be considered as unsuitable than that communities be considered so, and a sense of need to convert unsuitable communities arises. The tools then risk becoming totemic <a href="">#wptotem</a>, as the professional presents their practice as the ideal to convert to.</p>

<h4>Practices and values beget traditions and beliefs</h4>

<p>Already the concept of community has changed; hierarchy has emerged and the professional has become senior, the shaman with power over the lesser others who seek such enlightenment (but cannot see it themselves). The concepts of "problem" and "solution" have become part of the nature of the community, those with problems and those solutions. The community now consists of "us" and "them", and "other" begins to form.</p>

<p>Profession very easily becomes ideology; it may begin as a practice forming within a community, but it <i>must</i> have those that understand and practice it, and those that do not. The practitioners claim access to a greater understanding, beyond that of the norm, and so presented as an ideal or a utopia understood only by those that can perform the practice. Solution (the answer, knowledge) becomes idealised, externalised, not of this (world) community.</p>

<p>Unfortunately, answers do not transcend locality - they depend upon it, they are anecdotal, perhaps just coincidental, at best an acceptable export. Totems are not infallible so they rely on stories of success, and practices and values come dressed in traditions and beliefs. We practice what is preached, what we see others do, because we believe enlightenment will occur. Traditionalised practices become more than a practical apparatus for judging what actions to take, and instead dictate how to behave.</p>

<p>We have become naturally attracted to totemic tools that conceptually compartmentalise the space we exist in, tools like magic quadrants <a href="">#mquad</a>, hype cycles <a href="">#hypecycle</a>, personality tests <a href="">#pestest</a> and online dating algorithms <a href="">#okc</a> that tell us which compartment is ours, as if we were not already there. Such an illusion can seem quite informative, and perhaps even a useful director of behaviour - the simplification of a complex problem to a neat conceptual and often visual representation is a subitization, and it is indeed used in judgement <a href="">#wpsubit</a> <a href="">#subits</a>. But are such judgements worthy of our belief in them? We are in need of a transcendental epiphany.</p>

<h3>Transcending localisation</h3>

<p>So, <i>anecdotally</i>, like a <i>user story</i>, localisation arises naturally. To abandon it therefore seems unnatural, but localisation seems also to naturally lead to efforts to delocalise, to export ideals or to comprehend remote communities through local ideals. Some further studies are necessary to find out if this anecdotal understanding of localisation is borne out in the real world, but in order to formulate suitable studies some idea of what to look for, what delocalisation attempts might look like and where they might occur, will help.</p>

<h4>Communities bound by tradition</h4>

<p>If a community is a localisation identified by a shared practice of values, then what the community <i>can</i> do informs not only what it values but also its boundaries. Practiced values become defining features, and something that community members adhere to as a sense of identity <a href="">#cop</a>.</p>

<p>The tools of a community - which include language, ideas, concepts, institutions, as well as simple things like wheels or slightly more complex things like the internet - also form part of the community. Community <i>is</i> the expression of values by practice, and so the community can only be instantiated as it is by inclusion of the tools it uses. The community that is identified by their possession of swords are as well defined as the community that is identified by the colour of their skin, or by their dwelling in the mountains.</p>

<p>However this communal identifiability comes to rely on the practices that seem to define it, and so maintaining community identity encourages enforcement of practice as tradition, and opposition to change.</p>

<p>For an artist who has learnt the practices of an engineer, who wishes to make new things and who appreciates the beauty of practicality, change is a necessity. So, alternative practices <i>must</i> be sought. Heresy must be committed.</p>

<h4>Proposing an open society</h4>

<p>There are many moral and political philosophies purporting to describe how people should live, ranging from rational imperatives to appeals to the greater good to social contracts to doing what is most fun. Most, if not all, of these share the common feature of an <i>ideal</i>, even in the case of the ideal being terrible or antithetical (to have specifically no ideal is an ideal all the same). But if no value is to be considered intrinsically ideal or virtuous in and of itself, and no progress can be made in the name of upholding such an ideal, then what practice can the artistic engineer developer follow?</p>

<p>All that is left is experimentation, exploring and testing alternative practices and values, rather than seeking to maintain specific traditions. This is not to say that tradition and belief on the whole must be abandoned - they cannot, for they are a natural occurrence of a community, and how a community is sustained across generations. But to the developer, the <i>ideal</i> open society may seem like one that is capable of reflection on its own practices and values, and is open to and positive about reconstructing them.</p>

<h4>An engineer imagines a social machine</h4>

<p>Seduced by this beautiful, practical, <i>practically ideal</i> concept of the open society, an engineer may well imagine that if a society can reflexively know its traditions and beliefs, and know how best to augment them, then perhaps it can transcend traditional rituals, and instead develop <i>reconstructable</i> practices and values, and maintain them only whilst it remains practical to do so.</p>

<p>Our engineer might imagine further that some machine could be made, somehow capable of doing these things for society, yet never having that weakness of individuality. Our engineer may seek ways to build such a machine, follow practices, value things such as efficiency, development, design. The engineer might come to view the machine as not necessarily an explanation or a definition of a society, but an instantiation of the ideal form, a society achieving transcendence.</p>

<p>Unfortunately (for the engineer, at least) the other people in the world are not all engineers, and do not necessarily value the same things that the engineer does. So the attempts of the engineer to build this machine, or small parts of it, are unleashed upon the world and perhaps sometimes claimed as being the solution to all our problems. But, others rarely see them that way - the values of the engineer are delocalised when the tool is sent out into the world, and users fiddle with it in unintended ways. (This can be most depressing for the engineer.)</p>

<h4>But it is beyond reach</h4>

<p>The engineer is imagining the social machinery of an <i>ideal</i> society - some combination of people and tools all networked together and working in harmony. However the engineer that professes to take us to an ideal cannot possibly do so; such power would depend on a specialist practice, and a unique ability to lead towards some universally valued truth. But universal value is <i>not</i> a value that can be acquired or held locally, so any claim that such a thing was ever discovered by an expert that can lead us to it is illusory. Experts may seem to know such universal value (may even believe they do), but in fact they have just been <i>practicing</i> more!</p>

<p>The software developer or engineer as computer scientist has gone so far as to propose how to test for intelligence, as if there is some universal concept of it that could provably be instantiated. But as with all communities the engineers have disputes over everything from how to make good software to how to interpret their tests. The expected problems creep into development projects, when delocalisations are flawed by obfuscated inheritance of traditions as requirements. For example it may seem that to be intelligent is to be human, and that to be otherwise is to be artificial. But perhaps it is the practices of the engineers - those things that they <i>made</i> - that are artificial, and that intelligence is somewhere else altogether.</p>

<p>The humanities scholars as sociologists also face problems, as greater access to information triggers new studies and new understandings of old pronouncements on society, and yet traditional practices of survey cause studies to be limited towards certain outcomes. But it is now, when it has become so easy to be interdisciplinary, that community identity seems to matter more than ever. Who are we, what do we practice, and why? What is our value? Can we change, and continue to have relevance, in this opening world?</p>

<p>We have only just started learning how to be reflexive at a societal scale. We have only just turned on the web and realised that we are a complex social machine. Further studies, of where societies are opening and changing and reconstructing, and of where problems of delocalisation occur, may help advance our understanding.</p>

<h3>Researching delocalisation</h3>

<p>The values and practices of a community are emergent and dynamic properties of networks of actors <a href="">#wpant</a>. Such networks are instantiated in what they do, as their actors localise on shared values and develop practices around them. Yet as they continue in their dynamism, practices must be reconstructed as actors come and go, and as new tools are introduced. The defining features of a community - their locality around certain practices - must be called into question by new developments, and a delocalisation must occur as the community reconstructs itself. The reconstruction of community seeks to realign practices with values, and so the delocalised may be assimilated into practice once it has been understood in terms of community value. Alternatively, permanent separation may occur and the community instead realigns around what remains - either way, the community is not as it once was, and has been reconstructed.</p>

<p>Software development involves localisation around certain practices of value, as different developer groups construct methods most suited for achieving certain development goals. The output of their developments include the tools of digital scholarship, such as the web, which offers new opportunities for development practice, and also for other community practices. One such example is that of scholarly practice, where individuals localise around certain academic values (forming subject areas, for example), but where recent developments of and upon the web that enable digital scholarship have called many traditional scholarly practices into question, resulting in a number of open movements where the practices and values of scholarship are being reconstructed.</p>

<p>Upon a hybridised background in this triumvirate of software development as value practice, digital scholarship and the technosocial, and open movements delocalising the local, an advancement of the practices of research and development may be achieved.</p>

<p>In this thesis two sets of studies are performed, each taking a perspective from either side of the triumvirate, and both searching for examples of delocalisation:</p>

<ul>
<li>The first set seeks delocalisation of practice via the development of tools that transfer local practice to a wider community. The localisation of these studies is in software development projects and how they reconstruct the values of traditional academic communities.</li>
<li>The second set seeks delocalisation of community members by their uptake of digital scholarship tools that transcend local practice. The localisation of these studies is with scholars and the open movements that reconstruct values.</li>
</ul>

<p>The outcomes will show that these sorts of development involve reconstruction of value practices, and thus are technosocial developments. When this goes unacknowledged an obfuscated delocalisation may occur, such as when development of a technical standard in a local context is deemed suitable for use in other locales (delocalising the technology), or when new tools like collaboratively editable web resources start to be utilised by some members of a traditional community (delocalising members from tradition, as they augment their practice).</p>

<p>Reconstruction is a natural activity of a community, but when delocalisation is obfuscated it is that much harder for the community to reconstruct, as it is less obvious how to understand new practice and align with community values. So the overall aim of this thesis is to show that practicing technosocial development could bring delocalisation to the fore, allowing for fuller consideration of reconstruction of values during research and development projects.</p>

<p>During this research some tools for supporting technosocial development are prototyped. These tools compute a sort of social mapping of value reconstruction, and are demonstrated by examples for the values of scholarly practice observed during the studies. Such tools could in future be used to augment software development practices with an alternative form of technosocial requirements gathering, and could also augment ethnographic research practices where surveying the values of a community is required. These tools and augmentations will be contextualised in the conclusions to the four target audiences (software developers, stakeholders in developments, social scientists, and digital scholars).</p>

<p><hr class="embossed"></hr></p>
<h2>Backgrounds</h2>

<p>These backgrounds develop three concepts: practice of value, the technosocial, and delocalisation. Each is important in respectively supporting the discussion of software development, digital scholarship and open movements. A wide array of backgrounds is required to support such interdisciplinary research as that undertaken in this thesis, so it must sacrifice deeper detail where a more localised project would not have done. But of course, a sacrifice of some degree of specialisation and localisation in return for the opportunity of a new perspective is the ideal compromise and/or challenge for a project studying the problem of delocalisation.</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="https://docs.google.com/drawings/d/1LVFjfhF56Iwd6m3A6GOsUjSK9AyJOVEXDwfI8NA950k/pub?w=811&amp;h=651">
<p class="figure">The collection of research disciplines, grouped into software development as value practice, digital scholarship and the technosocial, and open movements delocalising the local, that provide the background of technosocial development.</p>
</div>
</div>
</div>

<p>As the studies of this thesis are to consider a hybrid of social and technical values and practices and to look for problems of delocalisation in development, the constructivist research method developed by Latour, Callon, and Law (amongst others) and known as Actor-Network Theory (ANT) is appealing (and well introduced by Cressman <a href="">#cressman</a>). Latour has used actor-network theory as a research method into scholarly practice, such as in his works "Science in Action" <a href="">#latoursia</a> and "Laboratory life" <a href="">#latourll</a>, which further supports use of ANT in the studies in open scholarship to come. It has also been used in other settings, and of particular revelance to the coming studies in software development are uses by design theorists such as Storni, and software developers working on the semantic web <a href="">#semweb</a> <a href="">#ld4</a> <a href="">#ldsofar</a> such as Carr et al:</p>

<blockquote>Actor network theory ...  has further extended some of these concerns by putting forward a symmetrical approach that tries to overcome both social and technological forms of determinism and introduces a new vocabulary based on the hybridization of the social and the technical<br>
<small>Storni <a href="">#storni</a></small>
</blockquote>

<blockquote>...the complexity of the relationships between human actors and technologies presents us with a problem; we need an analytical method to understand these networks, one which appreciates the complexities of their socio-technical nature and which, enables humans and technologies to be considered within the same analytical platform...<br>
<small>Carr et al <a href="">#webant</a></small>
</blockquote>

<p>Latour himself has also cautiously proposed a philosophy of design <a href="">#prometheus</a>, and Yaneva <a href="">#yaneva</a> (amongst others) examines the applicability of actor-network theory to the practice of design and vice-versa. Consider too the following from Venturini:</p>

<blockquote>As ANT showed, it is through design that the technical components of objects are assembled into a functioning unity. And it is thanks to design that such "internal" assembly mirrors and contributes to the external assembly of collective phenomena. Every time a designer connects separate parts or materials, she is also bounding social actors and groups. Far from being a superficial decoration, design (the art of form) guarantees the organization of modern societies by informing and formatting our sociotechnical existence. Time has now come for design to return the favor and serve ANT. Among others contributions, designers can play a crucial role in developing the controversy mapping approach. Originally conceived as a method to train students in the exploration of sociotechnical debate, controversy mapping is nowadays turning into a full research method. Its aim is to investigate the most entangled scientific issues, deploying the fabric of modern technoscience and modern societies. In such enterprise, ANT needs help from Design. Who better than the designers knows how to simplify collective imbroglios while respecting their richness? Who better than designers knows how to articulate complexity?<br>
<small>Venturini <a href="">#venturini</a></small>
</blockquote>

<p>Further examples of the concept of controversy mapping (a didactic actor-network theory), mentioned here by Venturini, are given on his website <a href="http://www.tommasoventurini.it/wp/?cat=9">#venturiniweb</a>. Visualisation plays an important role for these designers in their practice of controversy mapping, and this will become more relevant as the studies of this thesis unfold.</p>

<p>The concept of hybridisation, and the hybrid "technosocial", serve as a reminder that the studies to be performed must not seek a solution as purely social or purely technical, nor a coincidental socio-technical or techno-social. This thesis must not unintentionally localise itself in one or the other nor be seduced into providing a determinist explanation, or else the problems of delocalisation may slip by unobserved. Latour argues that there is no way in which the observer can be truly separated from the subject, that for example the civilised cannot explain the actions of the uncivilised by appeal to their lack of civility, and that when considering the sociology of science, science itself cannot be relied upon to be an infallible measure of itself nor an impartial method of study for itself. This means that sociology loses some power as an explanatory practice, but this is acceptable as it is not the intention of this thesis to use sociology to explain anything. ANT is a material-semiotic practice that considers the relationships of things and of concepts simultaneously, and is fitting to studies of the technosocial (one material-semiotic thing) rather than the social or the technical. Actors need not be human, so for studying software development this can allow for inclusion of software object entities that are interacted with via their APIs. The interactions between actors are how actors are understood, and those interactions are understood to be performative - they exist because they are performed, and they cease to exist if they cease to be performed.</p>

<p>This thesis seeks to explore traditions that claim authority for one source of information (for example a journal) over another (such as Wikipedia); to avoid defining what it is to be a scholar; and not to expect the technical approach to be true by virtue of being technical; to find out what happens when the researcher is not considered the source of truth, where the developer is not considered the source of truth, and where seemingly logical solutions fail to alter the behaviour of actors. This is possible with ANT, as an amodern method of studying actors and the networks that emerge, as Latour describes in "We have never been modern":</p>

<blockquote>This retrospective attitude, which deploys instead of unveiling, adds instead of subtracting, fraternizes instead of denouncing, sorts out instead of debunking, I characterize as nonmodern (or amodern)...<br><br>
Antimoderns want to defend localities, or spirit, or rationality, or the past, or universality, or liberty, or society, or God, as if these entities really existed...<br><br>
Seen as networks, however, the modern world, like revolutions, permits scarcely anything more than small extensions of practices, slight accelerations in the circulation of knowledge, a tiny extension of societies, minuscule increases in the number of actors, small modifications of old beliefs. When we see them as networks, Western innovations remain recognizable and important, but they no longer suffice as the stuff of saga, a vast saga of radical rupture, fatal destiny, irreversible good or bad fortune.<br>
<small>Latour <a href="">#whnbm</a> </small>
</blockquote>

<p>When Latour here describes Western innovations as no longer a vast saga of radical rupture, it recalls Dijkstra and his thoughts on the lack of radical novelty in software engineering. Perhaps it is just these small incremental changes to everyday practices that amount to radical novelty, and clearly the defence of localities must have relevance to the problems of delocalisation that are to be explored.</p>

<p>An example of this approach to understanding a situation in a non-reductionist way is given by Latour in "The Pasteurization of France". He considers the success of Pasteur (which may be considered a great success of science) in introducting to France the concept of "invisible" microbes as responsible for the spread of disease, and the subsequent presentation of Pasteur as a single cause of the benefits such a concept afforded France in terms of improved hygiene and reduced illness. However, Latour argues that Pasteur was in fact situated in a far larger network of actors from which such changes in French attitude emerged, including the public, public health researchers and policymakers, laboratory scientists, and medical researchers. By examining the discussions in journals of each of these groups at the time, Pasteur can instead be understood as one actor in a network undergoing very small changes in understanding of disease, in cultural values of polite behaviour, and in the application of medical treatment to a general public rather than a controlled group such as an army regiment. This presentation of the work of Pasteur may go against the notion that science solved the problems of disease and poor hygiene, but it also does not mean that a society simply wielded science in finding a solution to these problems either. In promoting an irreductionist and irreligious stance, Latour points out the following:</p>

<blockquote>Of course, it is exciting to believe that one actor may contain the others because we start to believe that we "know" something, that there are equivalences, that there are deductions, that there is a master, that there is law and order. We have two irons in the fire, the real and the possible. In this way we become invincible, since we are able to make an attack "en double"...<br>
If we adopt the opposite principle and try to see how far we can get by denying the distinction, then we have to claim, by contrast, that nothing reduces to anything else. Yet, it will be said, things are linked together; they form lumps, bodies, machines, and groups. Of course this cannot be denied. But what kind of ties hold them together? Since there are no "natural" equivalences, these can be of only one kind: groping, testing, translating. As soon as the principle of irreducibility is accepted, it becomes necessary to admit this first reduction: that there is nothing more than trials of weakness. The distance between actors is never removed; neither is the distance between words. And if there are equivalences, then they have to be seen as problems, miracles, tasks, and costly results. Thus there is no paradox. There are two consistent ways of talking. One permits reduction and builds the world by starting from potency. The other does not allow this initial reduction and thus manifests the work that is needed to dominate. The first approach is reductionist and religious; the second is irreductionist and irreligious.<br>
<small>Latour <a href="">#latourpasteur</a></small>
</blockquote>

<p>An anti-reductionist stance is sometimes considered to be a dualist philosophy, such as an object-oriented philosophy - i.e. that objects exist independently of observation and interaction. Harman considers these issues in section seven of "Prince of Networks" <a href="">#harman</a>, where he proposes that Latour is the closest to a "hero of object-oriented philosophy" as he embraces actors (objects - a concept which will become increasingly relevant in the following section on software development), but there is a worry that Latour creates an infinite regress of black boxes in actors, and that explanatory power is hidden within the concept of actor. Harman does go on to consider ways to avoid such problems in understanding ANT, but what is of greater interest is that in order to make this claim against Latour, one must first require that the concept of explanatory power can be located in an actor (or intellect) in the first place. So it seems to have taken on a traditional understanding of what it is for information to have value, as if explanatory power cannot emerge out of collaborative and undirected effort. Anti-reductionism is not necessarily the same as irreduction, and perhaps the very idea of explanatory power located "within" is a tradition delocalised by technological development, as such developments delocalise effort and become more collaborative. Perhaps it is necessary to consider a more social form of value reconstruction, and to rubbish the tradition of explanatory power (and perhaps even that of human intellect).</p>

<blockquote>Truth and falsehood. Large and small. Agency and structure. Human and non-human. Before and after. Knowledge and power. Context and content. Materiality and sociality. Activity and passivity... all of these divides have been rubbished in work undertaken in the name of actor-network theory<br>
<small>Law <a href="">#law</a></small>
</blockquote>

<p>What Latour describes in "Reassembling the social" <a href="">#latourrts</a> is a form of reconstruction of values - the place where technosocial development may be useful. Callon presents a similar concept of translation, in which actors build and maintain a network via problematisation, interessement, enrolment, and mobilisation <a href="">#callon</a>. A reconstruction of values is a problematisation, about which actors can be interested, through which actors can be enrolled by engaging in analysis of their practice, mobilising reconstruction of those values through the technosocial development project. These are ways in which communities localise, and subsequently delocalise. By making use of these approaches in the studies, it should be possible to bring problems of delocalisation to the fore.</p>

<p>And so with actor-network theory by our side we proceed into the backgrounds.</p>

<h3>Software development as value practice</h3>

<p>Software development is both a subject to which this thesis contributes, and also an example of a key concept of the research. Practice is the collection of observable actions of a community, as in a community of practice <a href="">#wengercop</a>, and the values of a community are those concepts that community members localise around and which are reflected in their practices. For example a software developer may practice certain software development methodologies that reflect their values. Over time, new practices may become possible, and a review of recent changes in software development practice will set the scene for advancement of those practices and also serve as example of reconstruction of value practice in a community.</p>

<p>Software development practice is particularly interesting in that it is a practice that intends to develop and introduce new tools into a community, changing the practice of that community, and so are of greater relevant to this research. Also, recent changes in development practice have promoted the localisation of developers with the community for which they are developing, so there is potential to look for problems of delocalisation within the application of software development practices in particular development projects.</p>

<h4>Object orientation</h4>

<p>Software architecture emerged as a discipline in the 1970s, following advancements in software engineering, which itself had emerged from a need to manage the increasing complexity of software systems development projects <a href="http://en.wikipedia.org/wiki/Software_architecture">#wpswarch</a>. Within the software architecture community numerous design patterns emerged and development practices formed around them <a href="">#intsoftarch</a>. Object oriented design is one particularly successful practice and is also particularly relevant:</p>

<blockquote>Numerous software design methods based on objects have been proposed. The field has evolved from the early object-based design of the mid-1980s (noun = object; verb = method; adjective = attribute) through object oriented design, where inheritance and polymorphism play a key role, to the field of component-based design, where meta-information can be defined and accessed (through reflection, for example).<br>
<small>"Software Engineering Body of Knowledge" <a href="">#swebokoop</a></small>
</blockquote>

<p>As opposed to writing a software program that abstracts the function of one simple process, object orientation allows for further abstraction by considering real world objects and the language used to interact with them as an architectural style. A software object is thus considered to be a well defined and reusable entity which can be interacted with via specific methods, and so it can perhaps serve purposes in situations beyond that for which it was originally written. Libraries of such objects can then provide a large amount of the code required to perform many software tasks, and they can simply be imported into a new software project and called upon to perform their well known functions.</p>

<p>In addition to this orientation of software towards objects, allowing for the conceptualisation of software libraries as discrete entities with which to interact, the increasing ease with which data can be accessed quickly and remotely and from numerous and various locations has led to data-centric software architectures, in which sources of data are conceived as resources with which to interact and from which information can be requested. In this age of the web, patterns for interaction with such data objects can prove extremely powerful and have become ubiquitous in development of software systems that allow both people and machines to interact with each other across application programming interfaces (APIs) <a href="">#api</a>. Perhaps the most well known API design pattern is REST. Proposed by Roy Fielding in his PhD thesis <a href="">#rest</a>, REST describes a method for accessing information resources simply by having a unique identifier - a Uniform Resource Identifier or URI - for the resource and by being able to transfer representations of the resource - a Representational State Transfer (REST).</p>

<p>In the context of the web there are four access methods - GET, PUT, POST and DELETE. GET is most common - for example, retrieving a web page is a GET request. PUT or POST are typically for submitting new representations or updating existing ones - these are less common for typical human web users, although anyone using a web page to book a flight has likely performed a POST, sending a representation of a booking record to the company that manages bookings via their web servers. DELETE is even less common for human web users in that it allows for deletion of the identified content - however, anyone with administrative privileges to an online blog or web application may have deleted a post or a record at some point, and this may have been achieved using a DELETE request. REST is not a standard, but is often considered a good <i>practice</i> when building software, and applications built this way are referred to as RESTful <a href="">#wprest</a>.</p>

<p>Despite not being a standard (and perhaps because of this), REST has become a well known practice because it makes sense to many developers and they find a community of practice is a useful way to achieve a goal; developers are of course involved in development projects with non-developers, and so the concepts of this practice and the values represented (well organised data and management) augment the behaviour of a wider community. So a natural concept of the world around us - objects - has supported software development itself, which as development has become more common and has become more intertwined with the activities of a wider community, the social concept that drove technical developments has become social once again. With this enhanced practice enabling software re-use and simple ways to interact with various data sources as well as applications that follow RESTful concepts, along with additional developments in the practices of software development, it has become possible for radical statements to be made regarding the overall practice of software development itself, and particularly how it must interact with a wider non-technical community.</p>

<h4>The agile manifesto</h4>

<p>The agile manifesto represented a philosophical change in the planning and management of software development projects, and could not have occurred without changes in hardware cost and availability, software language evolution, the desires of developers to prototype new ideas, or the willingness of customers to define requirements; it was because prototyping could be done so quickly and cheaply that fast iteration and localisation on individuals and collaborations could be achieved, resulting in some abandonment of traditional software development practices, yet in the process forming a new one. Software architecture as a discipline fell out of fashion with the advent of agile development <a href="">#agilesoftdev</a>, but some practices remained useful, and in fact the dynamic patterns of agile design are themselves architectures anyway; so software architecture was never truly abandoned, but underwent a renaissance <a href="">#swarchren</a>. The evolution of software architecture has traversed through Big Design Up Front to Agile to Lean <a href="">#leanarch</a> <a href="">#ppfswarch</a> and the balance between early inflexible design and agile <i>localised</i> development has become critical.</p>

<p>The agile manifesto has successfully promoted a localised approach to software development, and it is assumed that in the ideal agile development project close contact with users and collaboration with customers will direct the developers towards the correct requirements, and that this localisation will allow requirements to emerge and to be tackled as the project progresses. But despite the evolution of software development, engineering, architecture, design patterns, waterfall, and agile processes, projects still suffer failures <a href="">#softfail</a>. Recent attempts have sought to combine the flexibility and speed of agile with more traditionally disciplined concepts, with some now recommending a hybrid approach of Disciplined Agile Delivery <a href="http://disciplinedagiledelivery.wordpress.com/2014/02/05/hybrid/">#dadhybrid</a>, introducing the concept of retrospectives and process-goals <a href="http://disciplinedagiledelivery.wordpress.com/2014/03/06/improve-retrospectives-with-process-goals/">#dadretro</a> as alternatives to traditional practices. However this suggestion - and simlar proposals <a href="">#swarchmethods</a> <a href="">#archinag</a> for combining traditional architecture with agile development - acknowledge the tension between the two <a href="">#xgdiv</a>. So whilst there are architectural / agile methods purporting to enable large scale agile development across big distributed organisations <a href="">#earch</a> <a href="">#scagile</a>, it remains interesting that such problems are still commonly acknowledged in the first place.</p>

<p>Software engineering practices initially followed traditional project management processes such as the Waterfall model <a href="">#wpw</a>. This concept of linear development and simple repeatability has since been called into question and has evolved into something more agile. However, Dijkstra has argued against the entire concept of software engineering as a profession, claiming it as too dismissive of the "radical novelty" of computer science <a href="">#wpse</a>. Consider his opinion of the practice of software engineering:</p>

<blockquote>A number of these phenomena have been bundled under the name "Software Engineering". As economics is known as "The Miserable Science", software engineering should be known as "The Doomed Discipline", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter "How to program if you cannot".<br>
<small>Dijkstra <a href="">#dijkstra</a></small> </blockquote>

<p>However, even a developer that is radically novel will form some sort of practice (even if it is an anti-practice); and Dijkstra in fact advocated formal approaches to writing software based on mathematics, and desired a world where programmers were more like him:</p>

<blockquote>Dijkstra's crisis was rather more idiosyncratic than has generally been acknowledged. He took up the language of crisis to convey the need to replace "Chinese armies" of badly trained and mediocre programmers with an elite corps of "mathematical engineers" modeled on himself<br>
<small>Dijkstra's Crisis: The End of Algol and Beginning of Software Engineering <a href="">#crisis</a></small>
</blockquote>

<p>Perhaps what Dijkstra wanted is suitable in some cases, where software is being created for a very specific technical purpose. But it seems more radically novel to break with such simple technical developments of software, and instead to experiment with the interactions that do in fact exist between tools and their users. Perhaps as software development practice becomes more able to engage with users and their practices, it will help it avoid such doom. Perhaps a better understanding of reconstruction of value, particularly now when development practices are becoming so localised, will reconstruct the doomed tradition of software development.</p>

<h4>Co-realisation</h4>

<p>Sometimes new tools offer a way to advance practice, and it is understood to be advantageous both socially and technically, but cannot easily be achieved. This can occur because those involved in the development - perhaps a single department or company - envisage a change but, whilst they have technical capability, they do not have the authority to alter the practices of those beyond their immediate community. For example, the way they share data with other companies may already be well defined, and changing that practice may have significant cost for an external entity. Thus the community of impacted stakeholders exceeds the community of engaged stakeholders.</p>

<p>Some large organisations intentionally place development teams outside of standard business process teams, and have them assess the interactions between other teams from an external perspective <a href="">#wiegel</a>. Also, companies and researchers building tools to advance current practice start by analysing that current practice then trying to augment it. Sometimes this plays out well, and progress is made. But when such great control over the impacted community is not possible, for example in a far more distributed community of relatively autonomous scholars and scholarly institutions, this approach may not be of much help.</p>

<p>In "Co-realisation: Towards a principled synthesis of ethnomethodology and participatory design", Hartswood et al call for a re-specification of IT design via co-realisation as follows:</p>

<blockquote>Co-realisation calls for a re-specification of IT design and development as a principled synthesis of ethnomethodology and participatory design. A system which embodies workplace specific knowledge and which has IT professionals responding to the practical exigencies of living with the system is likely to produce a more elegant solution to the problems of living with IT. Put most simply, co-realisation advocates taking engagement with users seriously, asking IT professionals to capitalise on the mundane and to 'stick around' and see what happens.<br><br>
...it is in and through such an enhanced, long-term engagement that IT professionals become accountable. Co-realisation asks that IT professionals become more committed to the moral order of the workplace. This is the ethnomethodological character of co-realisation and it turns on the notion of membership. That is to say, the IT professional has to capitalise on what people know and use, not in the manner of 'professional' sociology, but in the way that members come to be vulgarly competent and thereby to know 'what goes on around here' <br><br>
... The culture of design as a relatively isolated process (inter alia organisationally, temporally and spatially) must be replaced by accountable design, which for co-realisation means enabling the unfolding implications of technology for the workplace in which it is located. <br>
<small>Hartswood et al <a href="">#coreal</a></small>
</blockquote>

<p>This argument for moving the practice of development closer to the community for which development is being undertaken is similar to that advocated in the agile manifesto. Although it is stated above as something the IT professional must do, co-realisation requires engagement of <i>all</i> actors involved in a development. The user community must partake in the practice of development, and engage in a concerted effort to augment their practice - the non-IT professional is just as likely to have to capitalise on what the IT professionals know and use.</p>

<p>People in various sorts of local community (developers, users) are finding themselves having to interact with members of other communities, and each must consider reconstructing their practices as they find themselves delocalised from their usual community. Hartswood et al have further considered the delocalisation of local meaning in what they call universalised locals, as concluded in "Building Information Systems as Universalised Locals":</p>

<blockquote>We have presented local information management practices in a hospital ward. The picture that emerges from this account is one of locally meaningful arrangements that are closely tailored to the specific needs of actors in the ward. While the information management practices are very locally meaningful, they also relate to the overall practice of medical care. We thus see them as universalised locals. Introducing centralised information systems such as EPRs is a difficult undertaking as they have a natural tendency to strip away local meaning, replacing it with universalities. In order for such a system to work, local interests have to be enrolled to support the universalities and keep them functional through day-to-day interpretation.
<br>
<small>Hartswood et al <a href="">#unilocal</a></small>
</blockquote>

<p>This concept of universalised local is indicative of the direction that a practice of technosocial development might take, but rather than exporting locals as universals (resulting in problems of delocalisation) it is precisely these sorts of export that may be more easily acknowledged and perhaps made available for a more collaborative and reflexive development activity.</p>

<h3>Digital scholarship and the technosocial</h3>

<p>Development necessarily implies the changing of a thing, so research into technosocial development must therefore take place where something is being done, and where change is occurring.</p>

<p>The scholarly community is made up of individuals - people, machines, institutions, research groups, groups of groups, and hybrids of all of these. Communities also have boundaries - limits to their capability to practice - and this is where change can be found, where minds are changed, where scholarship is performed, and where development of new tools and practices occurs. Development is <i>necessary</i> at boundaries, in order to advance them. This is where values are reconstructed and where the local gives way to something beyond. In the case of the scholarly community, digital scholarship is an example of this; new technologies present new opportunities for performing scholarly research in different ways, causing individuals to try new approaches and develop new practices, and to advocate changes to scholarly tradition.</p>

<p>Digital scholarship is described on Wikipedia <a href="http://en.wikipedia.org/wiki/Wikipedia:About">#WIKIPEDIA</a> as:</p>

<blockquote>... the use of digital evidence, methods of inquiry, research, publication and preservation to achieve scholarly and research goals. Digital scholarship can encompass both scholarly communication using digital media and research on digital media. An important aspect of digital scholarship is the effort to establish digital media and social media as credible, professional and legitimate means of research and communication.<br>
<small>Wikipedia <a href="">#wpds</a></small>
</blockquote>

<p>That such a quote is included here from Wikipedia perhaps holds the potential to delocalise a scholar (or those accepting his work) from traditional scholarly practice, as Wikipedia is not yet considered a valid source of information. Harvard University has a page on their website entitled "What's wrong with Wikipedia", upon which it states that:</p>

<blockquote>... when you're doing academic research, you should be extremely cautious about using Wikipedia. As its own disclaimer states, information on Wikipedia is contributed by anyone who wants to post material, and the expertise of the posters is not taken into consideration. Users may be reading information that is outdated or that has been posted by someone who is not an expert in the field or by someone who wishes to provide misinformation.<br>
<small>Harvard Library website <a href="">#harvardwp</a></small>
</blockquote>

<p>However, the Harvard page contains no evidence of who wrote it (not in the visual content, nor in the underlying code that generates the page), nor when it was last updated, or on what basis the assertion that taking the expertise of posters into consideration would ensure the information presented on a collaboratively editable platform is better or equal to that in a traditional publication. It seems this statement on the Harvard website could itself be misinformative, but it is hard to contribute to it and offer improvements, and it seems that the fact that this is hard should be treated as authority in itself - an implicit application of the traditional practice of scholarly publication. On the other hand, whilst Wikipedia can be edited by anyone, it is also possible to track when edits were made and by whom, and to see other edits the same users have made, and to form an opinion about whether or not the authors of a given page have demonstrated a suitable level of expertise. This is certainly a <i>different</i> practice of assigning value to materials when considering them suitable for inclusion in a PhD thesis, but is the <i>same</i> value - the desire to find and use relevant supporting material. So this thesis does explore such use, and uses simple tools to advance this new practice further.</p>

<p>Wikipedia is an excellent example of digital scholarship offering up a new tool which has far more than technical impact on how scholarship is practiced, and how such radical change can make it hard to understand how the shared values of scholarship continue to be upheld by such altered practices. This clearly has the power to divide a community, and to delocalise some scholars from their tradition. Some further examples of new digital scholarship tools on the web will be useful, along with an introduction to the backgrounds of research in social informatics and social computing that will help to tackle this question of value in radically new collaborative forms of digital scholarship, and a couple of relatively new research practices in actor-network theory and valuation studies that are appropriate to the studies to be performed in this thesis and the presentation of outcomes to be made.</p>

<h4>Getting into the web</h4>

<p>The majority of software development for the web (and in general) develops tools that manage information in ways that are meaningful to an audience community. It is to that community that the output of the software has value, and for new tools there is the added complexity that part of meaning is provenance - trusted sources are considered valuable. For example, it is part of how information about the world is encountered, organised and shared that gives meanings to certain classifications of diseases <a href="">#IIHISTORY</a> - the communities involved and the tools used in cataloguing and sharing this information have an impact on the meaning of terms.</p>

<p>The web was originally invented for the purpose of easily making information available to scholars across different sites, and was placed in the public domain shortly after its creation <a href="http://tenyears-www.web.cern.ch/tenyears-www/Declaration/Page2.html">#wwwpd</a>. With this, publication to a global audience moved from the preserve of the few to the choice of the many. As a result the amount of accessible information in the world has vastly increased for most people, and there is now far more information on the web, on all sorts of topics, than in all other information stores combined <a href="">#worldinfo</a>. Freedom of expression is perhaps a great feature, although it also questions the value of information and relevance to particular community - such freedom does not always mesh well with traditional practice, so efforts are often made to restrict what can be accessed <a href="http://www.theguardian.com/technology/2014/jan/08/david-cameron-great-firewall">#doctorowukfirewall</a>.</p>

<p>There are scholarly communities where the web has been embraced to a great degree for the purpose of publishing new findings - arXiv <a href="">#ARXIV</a> has become very popular with physicists and mathematicians amongst others, and similarly repec <a href="">#REPEC</a> with economists, BioMed Central (BMC) for biology <a href="">#BMC</a>, Public Library of Science (PLoS) <a href="">#plos</a> and the Internet archive <a href="">#internetarchive</a> more generally, as well as numerous other online journals, institutional archives, and blogs. Many of these provide simple, easy, and freely available access to some research, however these sources are considered <i>secondary</i> and none of these have changed the traditional practice of journal publication to a great degree - it is still commonly expected that official publications appear in journals and go through a pre-publication peer review process, whilst services like arXiv are considered pre-print and not strictly official sources. So although the technology has changed who can publish and the ease with which people can do so, it has not yet radically altered the practice by which scholarly work is published and deemed to be of high quality. This can be problematic, as a scholar can now easily publish and have access to more publications, but may find that some of them are not acceptable to the traditions of their scholarly community.</p>

<p>Tools such as the web - built for the purpose of sharing scholarly work - present new opportunities so radical that it seems at times that the scholarly community must reject that new potential for fear of loss of value through erosion of tradition. Despite the great advances in digital scholarship, there continue to be vast quantities of scholarly material rendered inaccessible due to social issues such as that of copyright law. Technology alone does not drive societal change as technological determinism claimed <a href="">#techdet</a>, and in fact once communities embrace technologies to change, they must do more work to overcome their traditional rituals <a href="">#careyritual</a> <a href="">#wpritual</a>.</p>

<p>For example, "Who Needs Access" is a website that collects stories about people who need access to research, in order to advocate for wider access by describing specific instances where lack of access was a particular problem, on the basis that "public access to scientific research makes <i>all</i> our lives better: it makes us healthier, better governed and better educated; it lets us live in a cleaner environment, a more civilised society and a healthier economy" <a href="http://whoneedsaccess.org/">#whoneedsaccess</a>. The stories are edited and published by a small team using an open source online blogging and content management platform <a href="http://wordpress.com/">#wordpress</a>. This technology, based on the web, makes it easy to publish such articles and to enable feedback and discussion to occur around them; this in turn promotes the content of the site, and provides a mechanism for wider engagement, for more people to engage as editors or to submit or be the subject of new content. It is, therefore, very similar to a publishing journal. So a community forms around the desire to solve a problem, doing so using the very technology that could easily solve such a problem, and yet there is far more to do than simply embracing the technology.</p>

<p>These hybrids of technical and social change - these technosocial developments - allow communities to localise around values by developing practices, whilst also prompting delocalisation by bringing traditional values into question. Researching these issues is more than a social or a technical study - it is a hybrid: social informatics.</p>

<h4>Social informatics</h4>

<blockquote>Social Informatics (SI) refers to the body of research and study that examines social aspects of computerization - including the roles of information technology in social and organizational change, the uses of information technologies in social contexts, and the ways that the social organization of information technologies is influenced by social forces and social practices.<br>
<small>Kling <a href="">#klingsi</a> <a href="">#wpsi</a></small>
</blockquote>

<p>Social Informatics is a relatively young research discipline, related to science and technology studies, described by Kling as an "interdisciplinary study of the design, uses and consequences of information technologies that takes into account their interaction with institutional and cultural contexts" <a href="">#kling</a>. Social informatics is described as having three orientations <a href="">#sawyerrosen</a>, as described below:</p>

<blockquote>Normative research focuses on the development of theories based on empirical analysis that may be used to develop organizational policies and work practices. The heart of such analyses lies in socio-technical interaction networks, a framework built around the idea that humans and the technologies they build are "co-constitutive", bound together, and that any examination of one must necessarily consider the other. Studies of the analytical orientation develop theory or define methodologies to contribute to theorizing in institutional settings. Critical analysis, like Lucy Suchman's examination of articulation work, examine technological solutions from non-traditional perspectives in order to influence design and implementation.<br>
<small>"Wikipedia: Social Informatics" (original included reference to Kling as above and to Suchman <a href="">#suchman</a>)</small>
</blockquote>

<p>The "indispensable analytical foundation" <a href="">#klingiaf</a> of social informatics is therefore clearly suitable for this work. However, situation in just one of these three orientations is not appropriate, as it is precisely the co-constitutive practices of software development in localised (organisational, geographical, conceptual, communal) settings that is where the studies of this work will start, and seek to move beyond. And whilst a social constructivist may find it surprising upon opening the black box of technology that there is no power embedded within it to repair society <a href="">#winner</a>, this is not surprising to a developer. So this thesis takes the non-traditional perspective from which the critical analysis of this work will be performed, as that of a developer embedded in - rather than external observer of - the constitutive/constructive design and implementation process.</p>

<p>An information infrastructure <a href="">#WPII</a> is summarised on Wikipedia as "all of the people, processes, procedures, tools, facilities, and technology which supports the creation, use, transport, storage, and destruction of information" (as described by Pironti et al <a href="">#PIRONTI</a>). Hanseth further describes information infrastructure as "a shared, evolving, open, standardized, and heterogeneous installed base" <a href="">#HANSETH</a>. Further studies go on to elaborate on this point, and some to claim that it is in standards development where the social (in the form of community values) and the technical (in the form of the developed tools) intersect:</p>

<blockquote>... community systems are of necessity social and political systems as well as technical infrastructures. We have developed the position that where the rubber hits the road in terms of the enfolding of community values into community information systems is at the point of the development of classifications and standards. This incursion of the political and social into computer science is not a negative feature to be eschewed; it is a necessary feature of moving between multiple social worlds - which we must do if our systems are ever to scale up outside of particular interest groups.<br>
<small>Bowker & Star <a href="">#IIMEDICINE</a></small>
</blockquote>

<p>There is a certain appeal to the neat idea that there is a social world of technologists and a social world of politicists, and that the writing of standards and classification is a neat conduit for communication between the two. But this idealisation of standards and classification, described as an incursion of social and political into computer science, perhaps does not fully consider the hybrid nature of society and technology. Whilst the social construction or social shaping of technology <a href="http://en.wikipedia.org/wiki/Social_construction_of_technology">#scot</a> <a href="http://en.wikipedia.org/wiki/Social_shaping_of_technology">#sst</a> is closely related to the concept of the hybrid of the technosocial, it is not the aim of this work to show that technology is socially constructed nor that society is technologically constructed. Instead, in order to take the technosocial as given and to study the problems of delocalisation occurring in technosocial development, this work must be done without bias to social or technical perspective and without bias to one orientation of social informatics (highlighting the suitability of actor-network theory).</p>

<h4>Social computing</h4>

<p>The premise of social computing is that just as it is possible to configure a network of many distributed computers to perform small parts of large computations, so too can a network of people. Just as human beings used to be computers in the sense that they were employed to perform parts of large calculations now easily performed by machines, they remain capable of being organised to perform other tasks. Where these tasks are something that computers in the usual sense are not good at, social computing may be of benefit. Success has already been achieved using this approach for some scholarly disciplines, such as in astronomy with Galaxyzoo <a href="">#galaxyzoo</a>, which enables people to classify galaxies discovered in images from the Sloan Digital Sky Survey <a href="">#sloan</a>. There are now also services that offer configurable access to this sort of human-computational resource <a href="">#crowdflower</a> <a href="">#amt</a>, so that it can be put to use on virtually any suitable project.</p>

<p>There is a further advancement to this concept, which is that a group of suitably organised individuals may be capable of providing better answers than any one individual - no matter how relevantly skilled - could provide. This is an alternative understanding of explanatory power, known as the wisdom of crowds, quoted on Wikipedia <a href="">#wpwoc</a> from Surowiecki <a href="">#woc</a> as having four main criteria for achieving crowd wisdom:</p>

<table class="table table-bordered">
<tr><th>Criteria</th><th>Description</th></tr>
<tr><td>Diversity of opinion</td><td>Each person should have private information even if it's just an eccentric interpretation of the known facts.</td></tr>
<tr><td>Independence</td><td>People's opinions aren't determined by the opinions of those around them.</td></tr>
<tr><td>Decentralization</td><td>People are able to specialize and draw on local knowledge.</td></tr>
<tr><td>Aggregation</td><td>Some mechanism exists for turning private judgments into a collective decision.</td></tr>
</table>

<p>This idea of a wise crowd is further elaborated by the eight conjectures of Oinas-Kukkonen:</p>

<blockquote><ol>
<li>It is possible to describe how people in a group think as a whole.</li>
<li>In some cases, groups are remarkably intelligent and are often smarter than the smartest people in them.</li>
<li>The three conditions for a group to be intelligent are diversity, independence, and decentralization.</li>
<li>The best decisions are a product of disagreement and contest.</li>
<li>Too much communication can make the group as a whole less intelligent.</li>
<li>Information aggregation functionality is needed.</li>
<li>The right information needs to be delivered to the right people in the right place, at the right time, and in the right way.</li>
<li>There is no need to chase the expert.</li>
</ol>
<small>Oinas-Kukkonen <a href="">#oinas</a></small>
</blockquote>

<p>There are criticisms of crowd wisdom, and examples of failures are provided by Surowiecki (and can be found via the aforementioned Wikipedia page). Although crowd wisdom is described as a group of people making a judgement, it is also considered necessary to evaluate that judgement externally. The simple examples of groups guessing the weight of an object are certainly easily open to and worthy of such evaluation, and it is easy to test if the crowd is suitably configured to achieve an accurate result, and also easy to look for flaws in this method. But if it is so easy to check the answer that a crowd comes up with, then this form of computation has not provided anything beyond what is already possible; it may at best improve efficiency or accuracy in some way. Instead, what if crowd wisdom could provide answers that cannot otherwise be provided?</p>

<p>In "Social Computing: from social informatics to social intelligence", the authors talk of how social computing may become a new computing paradigm based on modelling social reasoning, and they conclude with the following:</p>

<blockquote>Social computing represents a new computing paradigm and an interdisciplinary research and application field. Undoubtedly, it will strongly influence system and software development in the years to come. We expect that social computing's scope will continue to expand and its applications to multiply. From both theoretical and technological perspectives, social computing technologies will move beyond social information processing toward emphasizing social intelligence. As we've discussed, the move from social informatics to social intelligence can be achieved by modeling and analyzing social behavior, by capturing human social dynamics, and by creating artificial social agents and and generating and managing actionable social knowledge<br>
<small>Wang et al <a href="">#wang</a></small>
</blockquote>

<p>Whilst the development of the web and the social media applications that now thrive upon it suggests social computing is already influencing system and software development, Wang et al make the claim that the move from social informatics to social intelligence can be achieved by modeling and analysis, assuming that some new form of intelligence can arise simply from the study of current forms. Similarly, the claim that this wil be facilitated by creating "artificial" social agents implicitly assumes some sort of "natural" social agent already exists, somehow independent of artifice. This seems to assume some traditional concepts of social agency and intellect.</p>

<p>Pitting natural versus artificial intelligence suggests a conceptualisation of intellect as somehow independent of communication and communication tools - language being the first. Therefore, a modeling or analysis of social behavior feels too restrictive in that it seems to assume that it is a process by which some <i>other</i> artificial social agent can be constructed. Instead, if the problems of delocalisation can be tackled, perhaps we will discover that social intelligence emerges from that very <I>practice</i> of treating the social and technical as the hybrid they are, and that it is not an intellect that produces the sorts of answer that we can already externally verify.</p>

<p>This technosocial intellect may sound like an online collective, of which there have been criticisms, and doubts as to whether they can have wisdom at all, and even claims for example by Lanier that they are in fact extremely dangerous:</p>

<blockquote> ...new online collectivism ... is nothing less than a resurgence of the idea that the collective is all
-wise, that it is desirable to have influence concentrated in a bottleneck that can channel the collective with the most verity and force. This is different from representative democracy, or meritocracy. This idea has had dreadful consequences when thrust upon us from the extreme Right or the extreme Left in various historical periods.<br>
<small>Lanier <a href="">#lanier</a></small>
</blockquote>

<p>However, such criticism seems also misplaced: it depends upon the notion that the "online collective" or "social intellect" that is brought about by social computing is in some way either ideal or fundamentally evil. The argument that such intellects are dangerous relies upon defining them as dangerous intellectual concepts. This sounds simply like an argument against technosocial development of the concept of intellect - an attempt to uphold the traditions of intellect as being human, to localise intelligence within humanity, a false correlation of human intellect with good thought and the source of ideas, and a continued belief in the localisation of explanatory power.</p>

<p>It is not the purpose of this thesis to argue that online collectivism is good, nor that social computing results in good decisions. Reconstructing social computing will not create an artifical or social intellect capable of stating what can already be deemed right or wrong, but it will make small changes to how social computing is configured, so that actor-networks can be understood to make new (novel, radical) value judgements. With such an appropriate tool, a sort of social translucence <a href="">#soctrans</a> <a href="">#erickson</a> may be achieved, in which delocalisations will become obvious and easier to integrate into reconstructive technosocial developments.</p>

<h4>Valuation studies</h4>

<p>Studying reconstruction of value first requires an understanding of the "value of values" beyond facts. This "fact/value dichotomy" was described by Putnam as follows:</p>

<blockquote>The idea that "value judgments are subjective" is a piece of philosophy that has gradually come to be accepted by many people as if it were common sense. In the hands of sophisticated thinkers this idea can be and has been developed in different ways. The ones I shall be concerned with hold that "statements of fact" are capable of being "objectively true" and capable, as well, of being "objectively warranted", while value judgments, according to these thinkers, are incapable of object truth and objective warrant. Value judgments, according to the most extreme proponents of a sharp "fact/value" dichotomy, are completely outside the sphere of reason.<br>
<small>Putnam, <a href="">#putnam</a></small>
</blockquote>

<p>Putnam goes on to describe how such views rest on exaggerations of such dichotomy examples, and that the notion that "facts" simply rely on "senses" whilst values do not is over-simplified at best, and that ethical reasoning and description of the world are in fact interwoven. By freeing the concept of social computing from the idea that "fact" or "technology" is valid, and by treating social justifications of values as measurable through observation of practice, then social computing could be freed from the constraints of external verifiability and initial configuration. Rather than ensuring social groups are well defined and not overly interacting and that they are given "suitable" questions, instead a self-configuring actor-network can emerge as actors define themselves, their interactions, and the values and practices of relevance to them. Rather than being concerned by over-interaction, interactions between actors can be accounted for when utilising results retrieved from the computation - making interaction explicit and accountable rather than avoided and therefore potentially unknown. This sort of computation will not be capable of being either technologically or sociologically deterministic - such a reading would be an over-simplification, incorrectly assuming that the power to analyse is the power to explain, as Ackermann pointed out:</p>

<blockquote>The relationship between "cultural values" and "technology" is not uncommonly represented as one of direct interaction.... This is particularly true of the analysis of the relationship between values and technology in traditional societies. In this type of analysis it is often assumed that: (a) values determine social behaviour (assumption of causality); (b) values form a coherent system, shared by the whole of a given society (assumption of homogeneity); and (c) values constitute the core of culture and lend it creativity and capacity of resistance.<br>
<small>Ackermann <a href="">#ackermann</a></small>
</blockquote>

<p>So this thesis proposes only to perform social computing as an extension of practice - a form of feedback or guidance, to support technosocial development not as a deterministic activity but as a constructive practice. Beyond the methods of various practices - such as investment, accountancy, art critique, carbon offsetting, scholarly peer review, and many more - to quantify value in some way, there is an act of valuation. Like the ritual of communication, this is not an evaluative act that conveys indisputable information about a given thing or situation, but a ritual in itself - a practice of coming to a shared understanding of the value of something to a community, and of forming a community around that understanding.</p>

<p>Consider this description of valuation studies as given in the first article published in the new Valuation Studies journal:</p>

<blockquote>Stating that "valuation as a social practice" is a specific and interesting topic to study brings on several challenges. Valuation has many objects as well as many subjects, and is a process that takes many forms. Sometimes it is about assessing value, sometimes about producing it, and sometimes about both at the same time. Finally, valuation might be appreciated and analysed in many ways. This is both a blessing and a curse when claiming that "valuation studies" is an emerging field that is possibly identifiable. State it broadly, and valuation becomes everything and its study meets the entire field of the social sciences and humanities. State it narrowly, and the study of valuation as a social practice becomes the business of a handful of contributions locked up inside a closed and abstruse field of inquiry.<br>
<small>Helgesson and Muniesa <a href="">#fwiw</a></small>
</blockquote>

<p>There are numerous examples given in this article, and in the others of the journal, of valuation practices ranging from online film review and rating to a film that is about a form of valuation practice (scouting for baseball players) and how to change it in order to gain a competitive edge <a href="">#moneyball</a>. But what of the relevance to software development and open scholarship? Where is the ideal balance of "broadly narrow" focus?</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="/static/phd/issuetracker.png">
<p class="figure">An example of a software development project issue tracker, showing evidence of valuations in the form of tags, priorities, and milestones. See https://github.com/CottageLabs/OpenArticleGauge/.</p>
</div>
</div>
</div>

<p>The practice of developing software that enables communities to go beyond what they can do, and to progress towards some changed (ideally improved) version of what they can do, is a process that involves such valuative practices. The process does not simply include an identification and quantification of requirements, but also an implicit assertion that whatever the list of requirements defines is "worth doing". That certain requirements are prioritised, or that certain arising issues are deemed more critical than others, are examples of quantifiable value judgements - although they are not quite as clear-cut as the ideal social computing configuration may call for. And yet they clearly are very much an example of a <i>social</i> computation - of a decision made by a group of actors to prioritise certain issues over others. It is here, where practices that currently express a value may need to be reconstructed in order to allow for a change of practice brought on by the introduction of a new tool or a new actor, that values must be valued. Perhaps, here, what is required is a tool that mediates - that acts to overcome traditions in practice.</p>

<p>The four subsection headings given under "Reasons for studying valuation" in the second article published in the <i>Valuation Studies</i> journal relate well to the aims of this thesis:</p>

<blockquote>Because It Is Empirically Relevant/Important<br>
Because It Is Theoretically Challenging/Interesting<br>
Because It Allows Us to Do Interesting or Fun Stuff<br>
Because We Can Improve the World<br>
<small>Kjellberg, Mallard et al <a href="">#twocents</a></small>
</blockquote>

<p>There are observational statements to be made about open scholarship; there are interesting challenges to be put to the theory of software development; there is certainly fun and interesting stuff to be done with social computing; and perhaps with hindsight the world will be considered improved. By studying reconstructions of the practices of value, seeking a hybrid advancement, a technosocial approach for reconstructing the practice of values may be supported. The outcomes of this thesis are therefore contributions to valuation studies via software development and social informatics.</p>

<h3>Open movements delocalising the local</h3>

<p>Localisation is a common and natural occurrence; language is localised - and localises - in a group of users of that language, software designs are localised to versions that run on various operating systems and those versions can be localised further to customise their interfaces depending on display parameters or on the spoken language of the user. Concepts are localised and shared in a community - the different versions of the concept of a god, for example, depended on and are dependent upon those that believe in it. Values are localised, and a community that upholds certain values can express them in their own local practices. Localisation of a community comes to depend upon their practices, and evidence of such value practices may identify something or someone as belonging (being local) to that community.</p>

<p>Successful communities also fluctuate and grow, push their boundaries, encounter new externalities, sometimes export their local values and/or import new ones, as their changing nature changes what they can <i>do</i> and thus what they can <i>practice</i>. So communities, societies, networks of actors defined by their locality on a set of value practices may undergo delocalisation when new opportunities arise - when new actors are introduced and actors experiment with new tools to achieve goals in new ways. Open movements such as those of open source software, open access to research, of open knowledge, are examples of such occurrences. Scholarship itself, characterised by the grand mission statements of academic institutions and the high regard in which scholarly pursuits are held, is also subject to reconstruction of values as the traditions of scholarship, and what it means to be a scholar, are called into question by the new capabilities brought about by the web and digital scholarship.</p>

<p>For the purposes of this research, open movements are considered a signifier of the localities in which problems of delocalisation will be found and can be studied. So a study of software development and open scholarship is not a study of people advocating for or against the ideals of open source software, free software, open data, open knowledge; and open scholarship is not considered to be for or against any particular ideology, as Beall recently described the open access movement <a href="http://triplec.at/index.php/tripleC/article/view/525/514">#beall</a>; instead it is a study of a network of actors and how they practice scholarship, and how those practices are reconstructed as the technosocial network topology evolves. So an overview of the relevant open movements will help to situate the studies to come.</p>

<h4>Philosophy of the open society</h4>

<p>Open movements themselves represent the edge cases of communities, where traditional values and practices are questioned and possibly found wanting, and it has been claimed that freedoms could be eroded if the ability to share information is not identified and advocated for <a href="">#klang</a> <a href="">#poynder_plos</a>. This ability to test tradition and freedom to engineer new practices are what has been claimed by Karl Popper as crucial to the success of an open society. The open movement therefore is that place where an open society attempts to reconstruct values, and where the locality of a community is undergoing delocalisation. This philosophy therefore helps to define the scope of this research.</p>

<p>Although "open" seems representative of modern movements, it is not just a modern mode of language to prefix a more traditional term with "open". In 1945 Popper proposed in "The Open Society and its Enemies" <a href="">#popperosenemies</a> that much of the historical understanding of modern societies turns on a misinterpretation of Plato's Republic <a href="">#REPUBLIC</a> and other works. He argues that democracy is not in itself virtuous, but that democracy (or any political doctrine) is not something capable of being used to guide a society to an ideal - which is how Plato presented it and, according to Popper, is how Plato and modern understanding of his work is in fact a totalitarian application of the concept of democracy as an ideal method for leading a society to a utopia. He claims instead that an open society is one that allows for piecemeal social engineering, rather than one which seeks to maintain traditions and thereby ideologies. This does not mean that traditions are without value, but that a society must be open to testing small practical changes in order to progress. Otherwise, the society can only be totalitarian, regardless of its flavour of government or knowledge generation.</p>

<p>Strong criticism has been put to the ideas Popper presented in this book, and it is particularly interesting to note that many of them are of the form that Popper himself was making an ideological claim, that he was "motivated by partisan ideological considerations" <a href="">#bhargava</a>. This recalls the claim by Beall that the open access movement is fundamentally ideological and not open at all. However these concerns do not impact the use of the concept of the open society within this thesis, because such criticisms serve only to strengthen the suspicion that open movements are the place where the practices and values of a community are being called into question, and where problems of delocalisation will be found. That these criticisms exist at all is only more evidence to this hypothesis.</p>

<p>Popper proposed also in "The Logic of Scientific Discovery" <a href="">#popperlogic</a> that the scientific method is similarly incremental - that it specifically requires change of practice when false accounts are discovered. Scientific progress is therefore not an attempt to adhere to a tradition, but an attempt to falsify traditions and to replace them with better practices. However Kuhn has since argued against Popper that science in fact progresses in large scale paradigmatic shifts that revolutionise the otherwise normal (and unchanging) scientific practice <a href="">#KUHNSSR</a>. But, it only seems possible to understand such a paradigm shift with the benefit of hindsight, and unless such shifts are somehow able to take place instantaneously and without any prior warning, they must to some extent still rely on smaller attempts to change a community practice. Understanding societal change as a paradigm shift thus appears to be a result of having taken a particular perspective on the concept of societal change in the first place - namely a historical perspective, and one that is far removed from any small changes that may have been occurring around the time.</p>

<p>Popper produced a full criticism of historicism in "The Poverty of Historicism" <a href="">#povhist</a>, which denounces the practice of historicism in the social sciences as a method for understanding how societies evolve and thus to predict how they may evolve in future. This relates also to his proposal that Plato was similarly a historicist, and that modern societies following Plato implicitly accept the historicist practice, when they think they are following an ideal example of democratic practice. It seems then that Popper was proposing that those who are modern in fact never were, just as Latour has done.</p>

<p>In 1985 Steven Shapin and Simon Schaffer published "Leviathan and the Air-Pump" <a href="">#latap</a>, a historical study of how the experimental method of Robert Boyle prevailed over the natural philosophical method of Thomas Hobbes. This book is therefore a historical study of the emergence of the scientific practice that Popper himself developed in The Logic of Scientific Discovery, and it appears to prove that such scientific practice did indeed win out. It may seem then that Boyle was the champion of the open society, whilst Hobbes wished to uphold tradition - but Hobbes was also the author of "LEVIATHAN" <a href="">#leviathan</a> in which the liberal theory of the social contract (that individuals form societies by adhering to mutually agreed terms, thus improving the lives of all) was first established. Such a liberal idea does not seem in keeping with those purported to value tradition over advancing practice. However, Hobbes also proposed that the way such a social contract should be maintained is by subjugation of all under a ruling sovereign authority - although it is at least an authority by agreement, in which the sovereign holds the same values as those that have surrendered rights to him, for the good of society. Somehow the sovereign of Leviathan must know what is best for everyone and act only on that basis, and everyone must agree that this is the case, thus taking all to utopia. This seems fundamentalist once again.</p>

<p>Such differing perspectives and historical accounts seem capable of infinite regress on whether or not open is good, whether or not the ideas of Boyle win over those of Hobbes, whether or not Popper was an idealist in championing a liberal democracy of an open society, whether or not open access is ideological, whether or not rational people would submit to a social contract. Fortunately, these are just the sort of "X vs Y" arguments that this thesis seeks to avoid, in preference to study of changing values and practices. For this, open movements need only to <i>be</i>. There may of course be places that are not open movements where similar events occur, but open movements have the benefit of being easy to find and study; whether or not there are <i>other</i> locales does not invalidate this localisation of these studies.</p>

<p>(Whilst it would be wonderful to delve further into the philosophy of the open society and the issues it raises, it is beyond the scope of this thesis, and all that is crucially required has been presented already. Further information is available in the works mentioned above, as well as via Wikipedia <a href="">#wpopsoc</a> <a href="">#wppovhist</a> <a href="">#wplevpump</a> <a href="">#socialcontract</a> and the links that can be found there. Progress must now be made on specific examples of open movements.)</p>

<h4>Open source software</h4>

<p>Instead of building a machine that performs a particular function on the basis of its physical properties, instruction sets allow a machine to perform a particular function on the basis of reconfiguration of its constituent parts over time. By abstracting the reconfiguration process, it is possible to build a machine that is generally reconfigurable and which can therefore be programmed to perform particular reconfigurations by provision of an instruction set. So, software is nothing more than the abstraction of instruction sets, which are themselves just representations of configurations of machines. The function that once was perhaps discernible by careful analysis of the form of a machine can therefore instead be achieved by careful analysis of the instruction set.</p>

<p><p>As demand for software increased so too did the opportunity to profit from the sale of software. A simple way of achieving this was to make it closed source, hiding how the software did what it did and making it illegal to reverse engineer it. Licenses can then be sold that permit certain use, essentially extending the concept of ownership from things to actions. As ownership is generally well understood and software is not, this became the common understanding of software - as a licensed, closed, black box that could be purchased and that somehow performed a desired function.</p></p>
<p>However this enclosure of software was viewed by some to be antisocial, and a form of propaganda <a href="http://www.gnu.org/philosophy/why-free.html">#STALLMANGNU</a>, giving rise to the free software movement in 1983 <a href="http://en.wikipedia.org/wiki/Free_software_movement">#WPFS</a>. From this came the Free Software Foundation in 1985 <a href="http://en.wikipedia.org/wiki/Free_Software_Foundation">#WPFSF</a>, which sought to promote the concept and use of free software on the basis that the freedom to use software as one sees fit is a moral rather than financial issue.</p>

<blockquote><i>Free software</i> means that the software's users have freedom. (The issue is not about price.)<br>
<small>"Philosophy of the GNU Project" <a href="http://www.gnu.org/philosophy/philosophy.html">#gnuphil</a></small>
</blockquote>

<p>There was also an Open Software Foundation founded in 1988 and becoming the Open Group after a 1996 merger <a href="http://opengroup.org">#opengroup</a>. However this foundation was not formed as a matter of ideology but for creating open standards, initially for the UNIX operating system <a href="http://en.wikipedia.org/wiki/Open_Software_Foundation">#wposf</a>. But the term "open standard" itself is subject to debate over exact meaning <a href="http://en.wikipedia.org/wiki/Open_standard">#openstandard</a>. Despite this, it is evidence that the term "open" originates not only from the social ideology of a community of users, but also from opportunities to develop (and the social ideologies of) business practices. 1998 saw the formation of the Open Source Initiative, during a strategy session exploring the following issues:</p>

<blockquote>The conferees believed the pragmatic, business-case grounds that had motivated Netscape to release their code illustrated a valuable way to engage with potential software users and developers, and convince them to create and improve source code by participating in an engaged community. The conferees also believed that it would be useful to have a single label that identified this approach and distinguished it from the philosophically- and politically-focused label "free software." Brainstorming for this new label eventually converged on the term "open source", originally suggested by Christine Peterson.<br>
<small>"History of the OSI" <a href="http://opensource.org/history">#osihistory</a></small>
</blockquote>

<p>This is notably different from the ideology of the Free Software Foundation, and demonstrates a deliberate choice to talk about open rather than free. However, despite being an attempt to avoid philosophical and political considerations, the supporting open source definition includes a number of points that are beyond the concern of software licensing in itself, and are rather more relevant to the rights of individuals and groups involved with open source software:</p>

<blockquote>...<br>
5. No Discrimination Against Persons or Groups. The license must not discriminate against any person or group of persons.<br>
6. No Discrimination Against Fields of Endeavor. The license must not restrict anyone from making use of the program in a specific field of endeavor. For example, it may not restrict the program from being used in a business, or from being used for genetic research.<br>
...<br>
<small>excerpt of "The Open Source Definition" <a href="http://opensource.org/docs/osd">#osiosd</a></small>
</blockquote>

<p>These principles are intended to be instantiated in the form of an open source license. However, different forms of open source license support slightly different implementations. Of particular interest is the matter of virality in what is termed "copyleft" licensing. Licenses such as the GPL <a href="">#gpl</a>, for example, do not limit types of use but do state that derivative works must carry the same form of license - thus the license infects derivatives with open source ideals. Alternative licenses such as MIT <a href="">#mitlicense</a> do not include such clauses. This is a good example of how similar values can have differences in practice. But the underlying value that the work should be available for others to access, use, and alter as they see fit for their own purposes is clear and has since spread further, delocalising from the software development community; it resonated particularly with the practices of the scholarly community of accessing, verifying, and building upon the work of others.</p>

<h4>Open scholarship</h4>

<p>The Open Access Initiative has a clearly similar ethos to that of open source software, but it is about open access to information <a href="">#oa4lib</a> <a href="">#introoa</a>, generally supported by an open access journal and/or an institutional repository <a href="">#harnad</a> built on open source software such as DSpace <a href="">#dspace</a> and Eprints <a href="">#eprints</a>. Much effort has gone into integrating the repositories with the typical academic workflow <a href="">#repoicl</a> but the initiative is still affected by various issues, such as software interoperability <a href="">#jones</a> <a href="">#tapir</a> and engagement with library communities <a href="">#vrana</a>. Copyright law is often of great concern as there is a great deal of perceived risk in wrongly ascribing or misunderstanding copyright, and this in itself can be a deterrent <a href="">#repoucd</a>. This is complicated by the fact that it is not always clear what open access should mean - although there is at least one clearly stated definition from the Budapest Open Access Initiative (BOAI) declaration <a href="">#BOAI</a>, there are different ways of achieving it. In 2011 the Intellectual Property Office review of intellectual property and growth made recommendations that have resulted in a number of implementations <a href="http://www.ipo.gov.uk/hargreaves">#hargimp</a>. Of particular relevance is the following statement in the Foreword of the report itself:</p>

<blockquote>Could it be true that laws designed more than three centuries ago with the express purpose of creating economic incentives for innovation by protecting creators' rights are today obstructing innovation and economic growth?<br><br> The short answer is: yes. We have found that the UK's intellectual property framework, especially with regard to copyright, is falling behind what is needed. Copyright, once the exclusive concern of authors and their publishers, is today preventing medical researchers studying data and text in pursuit of new treatments. Copying has become basic to numerous industrial processes, as well as to a burgeoning service economy based upon the internet. The UK cannot afford to let a legal framework designed around artists impede vigorous participation in these emerging business sectors.<br>
<small>Hargreaves <a href="http://www.ipo.gov.uk/ipreview-finalreport.pdf">#hargreaves</a></small>
</blockquote>

<p>Subsequent to this review, in 2012 the Finch Working Group on Expanding Access to Published Research Findings published Accessibility, sustainability, excellence: how to expand access to research publications <a href="http://www.researchinfonet.org/wp-content/uploads/2012/06/Finch-Group-report-FINAL-VERSION.pdf">#FINCH</a>, in which the case was made for what has become known as gold open access. Gold differed from the previous model of publication in institutional repositories (now known as Green open access) in that gold maintains reliance on a more traditional publishing model but shifts the payment requirement from consumers to producers - upholding a similar value, but a subtle change in practice.</p>

<p>Following this the UK government responded positively <a href="https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/32493/12-975-letter-government-response-to-finch-report-research-publications.pdf">#willetts</a> and organisations such as Research Councils UK (RCUK) followed up with their own announcements <a href="http://www.rcuk.ac.uk/media/news/120716/">#rcukfinch</a> mandating open access on the basis laid out in the Finch report. However, because the RCUK announcement states that funding for the costs associated with producers (authors) paying for publication should come from research funding, it implicitly supports gold over green open access, contrary to the years of effort that has gone into building up the green infrastructure, leading to further debate and in some cases removal of support for the RCUK mandates <a href="http://openaccess.eprints.org/index.php?/archives/924-Suber,-Neylon-Harnad-on-Finch,-RCUK-Hybrid-Gold-OA.html">#snhfinch</a>.</p>

<p>This is an example of reconstructing practices of a particular value (open access to publicly funded research) resulting in perhaps wider engagement but also forming a boundary between communities that share the value, evidenced in a further prefixing of the concept of access beyond open - now it is (green vs gold) (open vs closed) access. It is not straightforward to identify the differing interests of those involved in open access, as it is clear that all have similar intent and yet quite different practice - indeed, Richard Poynder has published an entire series of articles on the state of open access <a href="http://richardpoynder.co.uk/the-state-of-open-access.html">#poynder_state</a>.</p>

<p>This begins to highlight the localisation around values, and problems of delocalisation as subtly different practices emerge, that could affect a technosocial development. Consider for example the traditional technology of scholarly dissemination - the journal. As journals became the most efficient way of distributing content to multiple subscribers, quality control by peer review became part of the journal publication practice <a href="http://expertedge.journalexperts.com/2013/01/26/scholarly-publishing-a-brief-history/">#pubhistory</a>. This led certain journals to develop a good reputation so publishing in certain journals became desirable. Scholars now find themselves ranked by the impact factor <a href="">#GARFIELD2</a> <a href="">#GARFIELD</a> and journal publication has become so ubiquitous that rankings such as the Research Excellence Framework <a href="http://www.ref.ac.uk/">#REF</a> now directly affect academic career progress.</p>

<p>It is therefore easy to assume that the journal tradition is one and the same thing as the scholarly tradition - that the value placed on quality of research is realised by the traditional practice of journal publication. This is evident in debates where peer review is often misconstrued as a property of journal publication, and changing journal publication is often seen as threatening peer review <a href="http://poynder.blogspot.co.uk/2006/10/open-access-death-knell-for-peer.html">#poynderpeerreview</a>. But the journal tradition was not designed - it emerged as an efficient (and profitable) practice that represented value, and it was the best option available despite being fallible <a href="http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble">#selfcorrectscience</a> <a href="">#retractionwatch</a>. However, as the introduction of new technology changes this balance of efficiency, profitability, and fallibility, further open movements arise, and the quest for open access becomes a movement for open knowledge.</p>

<blockquote>The <b>Open Definition</b> states a piece of data or content is open if anyone is free to use, reuse, and redistribute it &mdash; subject only, at most, to the requirement to attribute and/or share-alike.<br>
<small>"The Open Definition" <a href="http://opendefinition.org/">#OPEND</a></small>
</blockquote>

<p>The Open Knowledge Foundation has worked since 2004 to promote open knowledge - "any kind of data and content - sonnets to statistics, genes to geodata - that can be freely used, reused, and redistributed in order to deliver far-reaching societal benefits" <a href="">#OKF</a>. The OKF has worked on exposing data sources that may not otherwise have become available and building knowledge communities around them, and it has done this through a combination of advocacy and technical development. For example the OKF produces CKAN <a href="">#CKAN</a>, which is "the world's leading open source data portal platform ... used by numerous national, regional and local portals from Austria to Brazil" <a href="http://okfn.org/about/">#aboutckan</a>, and it also has other projects that are a mixture of technical community development such as the OpenSpending project <a href="https://openspending.org/">#openspending</a>, and others that are non-technical such as the Public Domain Review <a href="http://publicdomainreview.org/">#PDR</a> and the School of Data <a href="http://schoolofdata.org/">#schoolofdata</a>.</p>

<p>OKF have also been involved in licensing data for use in the public domain via their Open Data Commons project <a href="http://opendatacommons.org/licenses/">#odclic</a>, and similar work in this area has been done by Creative Commons (CC), who provide a suite of licenses for a variety of purposes in line with their stated mission:</p>

<blockquote>Creative Commons helps you share your knowledge and creativity with the world.<br><br>
Creative Commons develops, supports, and stewards legal and technical infrastructure that maximizes digital creativity, sharing, and innovation.<br>
<small>"Creative Commons" <a href="https://creativecommons.org/">#cc</a></small>
</blockquote>

<p>The OKF Open Science working group <a href="">#OKFOS</a> works to promote open knowledge in the field scientific research. An outcome of collaboration within the OKF between software developers, scientists, and activists, led in 2010 to the formulation of the Panton Principles, a hybrid of open source, access, and knowledge:</p>

<blockquote>Science is based on building on, reusing and openly criticising the published body of scientific knowledge. For science to effectively function, and for society to reap the full benefits from scientific endeavours, it is crucial that science data be made open.<br>
<small>"The Panton Principles" <a href="">#panton</a></small>
</blockquote>

<p>Similar expressions of value can be found in the Science Code Manifesto <a href="http://sciencecodemanifesto.org/">#SCM</a>, and in 2012 the Royal Society published a call for "Science as an open enterprise", the summary of which begins and ends as follows:</p>

<blockquote>Open inquiry is at the heart of the scientific enterprise. Publication of scientific theories - and of the experimental and observational data on which they are based - permits others to identify errors, to support, reject or refine theories and to reuse data for further understanding and knowledge. Science's powerful capacity for self-correction comes from this openness to scrutiny and challenge.<br><br>
Careful scrutiny of the boundaries of openness is important where research could in principle be misused to threaten security, public safety or health. In such cases this report recommends a balanced and proportionate approach rather than a blanket prohibition.<br>
<small>The Royal Society <a href="http://royalsociety.org/uploadedFiles/Royal_Society_Content/policy/projects/sape/2012-06-20-SAOE.pdf">#SAOE</a></small>
</blockquote>

<p>These points present science as being an enterprise of "open inquiry" whilst recognising certain boundaries must be considered and also cautioning against prohibition. However, the metadata of the Royal Society publication includes the statement "The text of this work is licensed under Creative Commons Attribution-NonCommercial-ShareAlike CC BY-NC-SA", which is a particular form of Creative Commons license <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/">#ccbyncsa</a>. This viral license attaches non-commercial clauses to re-use, and requires share-alike; so even in a document about science as an open enterprise, there are assumptions as to what the content of the document could or should be used for, which may be disagreeable to others. There are also disagreements about the intentions and effects of open publishing and publishers, for example Kent Anderson published a lengthy list of misgivings about the behaviour of Pubmed Central and claimed that they had no right to "lecture anyone about standards, fairness, openness, or propriety" <a href="http://scholarlykitchen.sspnet.org/2013/12/24/pubmed-central-revealed-reviewing-and-interpreting-the-findings-of-a-surprising-2013/">#SKKAPMC</a>. However, these descriptions of potential failings of open publishing are not compared to known failings of traditional publishing, so they serve only to show disagreement in practice rather than proof of either practice to achieve greatest value.</p>

<p>There are also groups focusing on similar issues in the humanities - for example the OKF has an open humanities working group <a href="http://humanities.okfn.org/">#OKFOH</a>, and development of an Open Library of Humanities <a href="">#olh</a> based on the PLoS model is under way. It is interesting that although the values espoused by the Free Software Foundation and Open Source Initiatives were desirable to humanities scholars, the fact that free software definitions exclude works that are not software <a href="">#fsdef</a> excluded those in the humanities more so than scientists (who are more likely to create software outputs). As the various open definitions did not necessarily suit less technical communities, a reconstruction was required and provided by Nina Paley in her 2011 "Rantifesto" <a href="">#paleyfsrant</a>, questioning why the freedoms of free software were not also guaranteed for free culture. She points out that the idea that it is possible to know what purpose a cultural object may be put to is an oversimplified view of cultural objects such as works of art, that works of art are always derivative, and that therefore such works are equally deserving of the full freedoms that are claimed for software.</p>

<p>Paley continued to develop this argument in her Minute Memes for the Question Copyright organisation <a href="">#qc</a>, in which she emphasises the idea that, because all creative works are derivative, copying is not theft and that it is solely by attribution that the work of an artist is recognised. In order to enable this recognition whilst attempting to avoid the complexities of licensing, she went on to propose the copyheart license on the basis that "copying is an act of love" and that "love is not subject to law", therefore the copyheart statement indicates the intention of artists to create works for the purpose of sharing <a href="">#copyheart</a>. This questioning of copyright highlights the important point that copyright is something to be asserted, which must be enforced by the copyright holder, as opposed to a law which should not be broken <a href="http://en.wikipedia.org/wiki/Copyright">#wpcr</a>. Copyright is just another <i>practice</i> of a similar placing of value on creative works, but attempts to delocalise one value from another (monetary vs intellectual, for example) can prove enough to cause yet more friction.</p>

<p>If the scholarly community relies on social artefacts such as publishing standards and copyright law, and technical systems such as databases and web servers, then the work of innovative developers of scholarly practice is to overcome practical differences. Innovations continue both in technical aspects such as digital formats <a href="http://scholarlykitchen.sspnet.org/2013/12/06/the-evolution-of-digital-publishing-and-its-formats/">#digitalformats</a> and in social aspects such as harmonising international copyright laws <a href="http://ec.europa.eu/internal_market/consultations/2013/copyright-rules/index_en.htm">#europacopyright</a>, but as the myriad differences of opinion presented in this short background to open scholarship demonstrate, a technosocial practice that brings these differences to the fore may be useful.</p>

<p>Open scholarship <a href="">#oaprinciple</a> <a href="">#osgetz</a> <a href="">#openscholarship</a> has recently emerged as an umbrella term for open scholarly movements, and so this is used as the overarching open momvement in which the studies are to be framed:</p>

<blockquote><b>Open Scholarship</b> is an umbrella term used to describe developments such as open access, open science, open education and other open initiatives.<br>
<small>Council of Australian University Librarians <a href="http://www.caul.edu.au/caul-programs/open-scholarship">#CAUL</a>  </small>
</blockquote>

<p><br></p>

<h2>Studies</h2>

<p>The triumvirate of software development, digital scholarship, and open movements, has been proposed as an interesting locale in which to perform this research of the problems of delocalisation. It is expected particularly that open movements signify attempts to reconstruct community practices, clashing with tradition and bringing community values into question. The practices that a community is capable of become part of community tradition and part of the definition - the localisation - of that community. The introduction of new actors (whether software tools, different people, or mandates and standards) cannot help but cause a change in the community - after all, the actor-network that instantiates the community must change to incorporate new actors. So the community <i>delocalises</i> as actors move in and out of it, as they import and export values and attempt to reconstruct practices in the dynamically emerging actor-network.</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="/static/phd/sios_quadrants.jpg">
<p class="figure">The many types of actor, that may be manifested as people, organisations, software systems, communication infrastructures (and more) that contribute to the network of scholarship, and that define and re-define what scholarship means and how it is performed.</p>
</div>
</div>
</div>

<p>These studies seek to observe delocalisation problems such as when technologies are naively exported or when societies naively import tools that change practice. The studies are split into two sets - of four software development projects and of three concepts of scholarly actor. Each set progresses in order of some concept of size - for the development studies this is in terms of their scale, ranging from small local developments up to national infrastructure projects; the scholarship studies are ordered by actor, starting with individual interviews up to institutions as hybrid scholarly actors and scholarly communities as actor-networks. These studies will show how attempts at delocalisation are currently practiced and problematised, providing evidence for outcomes that will bring these problems to the fore in advancing a practice of technosocial development.</p>

<h3>Studies in software development</h3>

<p>These studies of four software development projects describe how problems of delocalisation occur when successful developments of tools to support the practices of a given community are then exported to a wider community, either as new instantiations of the same tool, or as interaction routes with an external community, or as attempts to standardise practices across communities. These projects are presented in order of scale - the extent to which the projects require interactions beyond the relatively small community driving the project, into a wider community with similar values but separate (although possibly substantially the same) practices. The first project study is presented in a style best suited to demonstrate localisation of a community, whereas the remaining projects describe the emergence of new problems of delocalisation, and how those problems are tackled.</p>

<p>(Whilst I explore these development projects as an ethnographer of practice, I was also directly involved in all to greater or lesser degrees and in varying capacities. I make these involvements clear as they become relevant to each of the studies.)</p>

<h4>Lothians Equal Access Programme</h4>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="https://docs.google.com/drawings/d/1DzLlAwTWVEgxvABCNopz0ONusRFvK48nJV-Vx30QiRU/pub?w=728&amp;h=237">
<p class="figure">The actors involved in the LEAPS software development project, representing as funders a group of universities and Scottish government agencies, with further universities and schools as stakeholders in the LEAPS mission and the development of it, students as stakeholders in the functionality of the mission and its outcomes, and of course LEAPS themselves, and the developer involved in the project (me).</p>
</div>
</div>
</div>

<p>The Lothians Equal Access Programme for Schools is a community support project based at the University of Edinburgh with a simple and widely shared goal - to help widen access to higher education. LEAPS is managed by a small team of people who have developed practices for working toward this goal, and who want to take advantage of technological improvements in order to improve their efficiency by decreasing the amount of time they must spend on data collection and administration, and instead spending more time doing what they are good at - namely interacting with other people and identifying those that require further assistance, and achieving and communicating the progress of their project.</p>

<p>The LEAPS project is delivered on behalf of a consortium consisting of about twenty other universities, and representing the interests of the Scottish Government, to whom they must report. Therefore, despite being a small project with a focus on people from particular geographical regions, they have numerous interactions beyond the bounds of their own organisation.</p>

<p>There is nothing technically complex about the software that LEAPS required - all it needed was some time committed by a skilled developer, and careful consideration of the practices already performed and how they could be improved by the application of new technology. Whilst these discussions were easy to take forward locally, it was not so easy to take the discussions beyond the organisational boundary to the wider community. There are a number of reasons for this:</p>

<ul>
<li>LEAPS have no development skills of their own - development is outwith their remit, they are funded to perform a function</li>
<li>they had no authority over the organisation to whom they report, and they were tasked with and budgeted for the ability only to provide such reports - <i>not</i> to change practice</li>
<li>whilst LEAPS provided vital information to the universities they supported, and in turn received information critical for reporting, they held no authority over the universities they reported to</li>
<li>although LEAPS collected data from students, they had no direct access to or control of student data - they interacted via schools and colleges</li>
</ul>

<p>Thus LEAPS found themselves in the complex situation of having been set up and funded externally to any one parent entity with the intention of serving a community, and with a requirement to engage with other institutions in order to contact the subjects of their studies, and whilst this separation was ideal for the purpose of focusing on the task of delivering that critical service to the community requiring it, it also left them without direct control of the full range of tasks regulalry practiced.</p>

<p>Whilst it was possible for LEAPS to control their budget and allocate funding to development once it became clear that there was value in doing so (improving efficiency by moving to online systems) they remained stuck with the issue of how to alter the practice they took part in. The development project itself therefore became more than a software development and instead had to reconstruct practices around the core value of increasing access to higher education. This led to the technical development itself having a delocalising effect on the actor-network involved in the practice, because developing a new way to do things raised questions as to how the new ways maintained old traditions. For example, the traditional practice of storing old paper copies of student interviews in a cupboard ritualised the value of data backup, which had to be reconstructed by a more modern but somewhat more opaque (to the non-technical staff) practice of off-site database backups.</p>

<h5>Engaging a software developer</h5>

<p>LEAPS began by engaging a software developer - this was how I became involved with the project, as I developed the software they required and received permission to present the development as a case study in this thesis.</p>

<p>The first step of this engagement, as a developer external to the LEAPS team, was to analyse the LEAPS team practices and to write user stories and use cases to represent these. Then followed a requirements analysis to form the basis of the development agreement, and an introduction and explanation of the principles of agile development to be used during the course of the project. Therefore the initial requirements were used as a scoping exercise and a budgeting tool rather than as strict deliverables. The key to this was focusing on the user stories and use cases as the priorities, and using them to drive iterative developments of the software.</p>

<p>Whilst the typical software development, engineering, architecture, and project management methodologies were sufficient for capturing the interactions within LEAPS and between the developer and LEAPS staff, they were not so useful for capturing the involvements of more remote actors, such as those in other universities and organisations. So it was hard to model user stories of users beyond the local group that could make a decision about developing the practices.</p>

<p>By engaging with a software developer, LEAPS overcame their lack of expertise in what was technically achievable. This may seem like a straightforward step in any technical development project, but it highlights the fact that an act of transcendence had to be performed, that local practices had to be called into question, and that the wider community that LEAPS interacted with were made aware of a shift in the orientation of LEAPS towards their traditional methods of interaction - the practices by which they shared information about students and their progress. A reconstruction had begun, and LEAPS had taken a delocalising step from the actors in their local community.</p>

<h5>Working with schools</h5>

<p>LEAPS collect data from students in order to identify those eligible for assistance in their applications to university. This was performed by providing paper questionnaire forms to schools to be distributed to students, completed, and returned via the schools. So this practice included schools and their staff, and divested some power to them. So the technically simple process of reconstructing the questionnaire form as an online form in fact represented a significant value reconstruction whereby the power of schools to oversee the provision of data from school students to LEAPS was weakened, and their role in such a practice was called into question - a delocalising act, requiring negotiation and alternative understanding of traditional responsibilities and capabilities.</p>

<p>The technical solution to data protection rules was straightforward - by providing a secure web interface where students could connect directly to an online version of the questionnaire and provide their own answers. This was demonstrated to school staff, and was familiar to them as being similar to how they would interact with their online banking interfaces. This resulted somewhat surprisingly in the schools embracing the new online questionnaire, as it represented a recent and familiar change in practice of personal security for the staff of the school as they had moved to use online banking themselves, and so the schools took the opportunity to disentangle themselves from the responsibilities of the process of managing the collection and safe delivery of the student data, whilst enabling them to focus on and to increase the value of their most valued practice - supporting their students with teacher time, access to equipment (school computers), and guidance where the students required it, with the new practice of completing the online questionnaires.</p>

<p>In this case, then, the technological and social aspects merged well, and the initial delocalisation quickly became positive, and LEAPS and their school partners quickly localised around a newly reconstructed practice of value. This was facilitated by their having already been introduced to such new technological concepts in their own personal banking interactions.</p>

<h5>Working with universities</h5>

<p>At the point of delivering their service to the universities to which students would apply, the LEAPS development project encountered a more significant delocalisation problem. At that point, the practice extended beyond LEAPS control and involved handing responsibility (and supporting information) to continue the application process over to actors not directly related to the LEAPS organisation or the development project that LEAPS had begun. Although these actors could perhaps have been modeled as consumers of the output of the tool being developed, they could not be treated so simply as their role was a continuance of an ongoing process culminating in further communication with students and feedback of information to LEAPS.</p>

<p>Ideally, all the actors involved in the full process would have been considered stakeholders in the development, and of course there are ways to represent such stakeholders in software development practices. But this was simply beyond scope in terms of time, budget, and <i>authority</i>. The LEAPS biennial conference, along with regular meetings with key groups such as the University of Edinburgh, were the only regular direct contact times between the actors of the various organisations, and apart from that their contact was <i>via</i> the practiced process of sharing information about the students. In moving to reconstruct this practice, LEAPS found another delocalisation occur.</p>

<p>The project did successfully deliver to these stakeholders as well, but the cost of doing so was considerable delay - not in the sense that the project was delivered later than planned, but that the project could not be delivered earlier than when it became accepted by external groups. Thus the critical path of the development project itself became something that was under the control of external stakeholders - and, regardless of their representation in the development project, this is what put the project beyond that of simply a technical development; instead, it became a reconstruction of practice that called values into question, such as what it meant for a university to sign a form offering alternative criteria to a student applying to enter their institution. The act of accepting a student is one of great importance to a university, and the difficulty with which one can get in to a given university can be considered directly related to the <i>value</i> of access to that university, so once again the simple technical task of changing a paper practice to a digital one - something that makes it <i>easier</i> to apply for alternative criteria for acceptance, calls into question the values a community holds and realises in their practices.</p>

<h5>Reconstructing the LEAPS actor-network</h5>

<p>LEAPS have no remit to change the practice of the community they serve, and no authority to mandate the values of that community. Therefore, the only way to achieve such reconstruction of practice was to instigate a delocalisation that called values into question by beginning to develop a new tool. LEAPS did not have the power (or desire) to force a community to change or to devalue any actor either directly or by deconstruction of their practices, so the outcome of the development they took part in had to be one that would <i>change</i> their community, taking them to a new localisation that incorporated a new tool.</p>

<p>For the complicated problem of the involvement of the universities and how reconstructing the practice of providing acceptance criteria potentially delocalised them, this was achieved by purposefully developing a technically sub-optimal demonstration and presenting it to the wider community at one of the LEAPS biennial conferences. This enabled LEAPS to avoid making direct mandates to external actors to change their practices - for example, attempting to tell a university admissions officer not to bother performing their traditional practice of sending a printed copy of a form to three different members of staff to sign off, when no such signature was in fact required - and instead to ask them to imagine how their practices could be made simpler, less time consuming, less demanding, and to report back with suggestions. This incentivised disinterested and skeptical actors to reconstruct their own practices in ways that might save them effort, whilst never directly questioning their values. The functionality of the system became a topic around which everyone could argue, and by <i>engaging</i> in this the actors became <i>mobilised</i> in the reconstruction - they sought to <i>translate</i> themselves. It was generally viewed as being better to be involved in the reconstruction - to help form new practices - than to argue to maintain tradition, once it became apparent that there was no intention by LEAPS to force different values upon any actor or community. LEAPS essentially abdicated the position of expert or shaman, and the void was filled by communication.</p>

<p>By engineering conditions in which reconstruction could take place, the problem of delocalisation was transcended by localisation to the context of shared reconstruction. This is what occurred - in an undirected and unsupported way - at the LEAPS biennial conferences. The unique values of various community members, traditionalised in particular workflow practices, became separated from those practices and were made available for reconstruction into new practices in a more technologically advanced (by the introduction of the new actor that was the LEAPS online system) actor-network.</p>

<p>The LEAPS study therefore demonstrates that aspects of the technosocial development practice already exist, in that it is tacitly performed by actors involved in development by methods such as augmenting technical development schedules to coincide with social events. Perhaps, acknowledging this, there is potential for a tool specific to managing that context of the process - not a requirements analysis or a user story or an issue tracker, but an actor-network reconfiguration tool. A tool for making the reconstruction of values an engaging practice in itself rather than a delocalising one. But what sort of tool this should be - a design tool or an implementation tool, or something else - remains to be seen.</p>

<h4>Jisc Open Biblio</h4>

<p>Whilst performing my study of this project I also acted as technical developer during the first year, and project manager and technical developer during the second year, as part of my responsibilities at the time with the Open Knowledge Foundation. I was the lead author of the project completion reports that were submitted to Jisc at the end of each project, and which are published on the project website <a href="http://openbiblio.net/2011/06/30/final-product-post-open-bibliography">#JISCOBR</a> <a href="http://openbiblio.net/2012/08/23/final-report-jisc-open-bibliography-2/">#JISCOBR2</a>. The content of this section includes material from those original reports, and such re-use is permitted under the terms of the original publications themselves and of the terms of funding imposed by Jisc upon the projects.</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="https://docs.google.com/drawings/d/1lGjIesuFdHOiQllj_f6PyjV25JROBFZsa2pDCS-ppPw/pub?w=803&amp;h=434">
<p class="figure">The actors involved in the Jisc Open Biblio project, linked by their interaction, via bibliographic metadata. Researchers and funders create and consume such metadata about their outputs, whilst libraries and publishers aggregate and disseminate it, to a public audience (that could include funders, researchers, and other subsets); the project developers in this case interacted directly with all but the public - except as they were already represented in the other subsets.</p>
</div>
</div>
</div>

<p>The Open Biblio project, funded by Jisc, was an exploratory development project with the aim of demonstrating what could be achieved technically if a particular social value were reconstructed in a particular way - namely that of treating the bibliographic metadata of published books and articles as something that is independently useful, re-usable and re-distributable beyond the traditional value of metadata as a tool for identifying and referring to individual works. The study of this development project focuses on the concerted effort made by the project team to use ambitious technical developments as tools to encourage others to consider delocalising themselves from the traditional understanding of bibliographic metadata, and to advocate for reconstructing practices of how bibliographic metadata is created, used, and shared.</p>

<h5>Open Biblio project review - year 1</h5>

<p>Bibliographic data has long been understood to contain important information about the large scale structure of scientific disciplines, such as the influence and impact of various authors and journals. Instead of a relatively small number of privileged data owners being able to manage and control large bibliographic data stores, this project aimed to make it possible for an individual researcher to browse millions of records, view collaboration graphs, make complex queries, make selections and analyses of data - all on their laptop whilst commuting to work. Although typical laptop hardware had become powerful enough to support this, the software tools for such easy processing were not adequately developed at the time this project started in 2010, so the project aimed particularly to demonstrate that such capabilities could be possible. Achieving this aim relied primarily on acquiring datasets upon which such software could operate, and secondarily on demonstrating that such capability would be valuable should further datasets be made openly available.</p>

<p>The elements of bibliographic data are facts, which in most jurisdictions cannot be copyrighted; there are few technical and legal obstacles to widespread replication of bibliographic records on a massive scale, but there are social limitations: because bibliographic data is subject to a process of continual creation and replication, it relies on individuals and organisations to be sufficiently motivated and able to create and maintain open bibliographic resources. So it was important for this project to make such practices easier. The output of this project - Open Bibliography - is a combination of open source tools, open specifications and open bibliographic data (full details of the outputs are in the original project report). In working towards acquiring these open bibliographic datasets, the key principles of open bibliographic data were clarified and set out for others to reference and endorse <a href="http://openbiblio.net/principles">#OBP</a>. Over 180 endorsements have since been received, and the principles continue to be promoted. Anyone seeking access to bibliographic data, which was commonly complicated by both technical barriers to accessing millions of records on remote computers and also social barriers such as the perception that bibliographic databases were valuable private collections belonging to organisations, can now therefore use these principles and the endorsements supporting them to leverage arguments in favour of open access to such metadata.</p>

<p>In this project the concept of actor-networks became apparent, despite being unknown to the project team. The project team were aware that reaching their goal required engaging people with tools as impressive demonstrations, rather than simply building tools to meet specific requirements. As bibliographic metadata collections are inherently datasets about the interactions of many actors within a scholarly subject network, visualisation tools were created to demonstrate the value of the datasets that had already been made available to the project. Examples included a geo-temporal map of the bibliography of open access articles of the International Union of Crystallography and also a world map of the articles in the Medline dataset <a href="">#medline</a>, used to great effect in publicising the outputs of the project:</p>

<div class="row">
<div class="col-md-6">
<div  class="thumbnail">
<img src="/static/phd/globe.png">
<p class="figure">An example interactive data visualisation developed during the Jisc Open Biblio project, demonstrating the ease with which complex data can be represented and explored by emphasising particular localising contexts - in this case, geographical location and change over time <a href="">#globe</a>.</p>
</div>
</div>
<div class="col-md-6">
<div  class="thumbnail">
<img src="/static/phd/mmrspider.jpg" style="margin-top:20px;margin-bottom:20px;">
<p class="figure">Another example visualisation developed during the Jisc Open Biblio project, showing relation by citation of scholarly artefacts as a network graph, enabling easy discovery of articles that cite a controversial and subsequently retracted paper.</p>
</div>
</div>
</div>

<p>Another example used network visualisation techniques to show a citation map of papers referencing Wakefield's controversial paper on the adverse effects of MMR vaccination <a href="">#WAKEFIELD</a>. A full analysis would require not just a record of the act of citation but also the sentiment, and initial inspection showed that the immediate papers had a negative sentiment i.e. were critical of the paper. Wakefield's paper was eventually withdrawn but the other papers in the map still existed at the time of the Jisc Open Biblio project. Because citation sentiment is often unknown, and as papers that cite retracted papers are not necessarily themselves retracted, recursive citation can often build a false sense of value for a distantly-cited object. Further examples such as the annulment of plagiarised theses <a href="">#germanmp</a> <a href="">#germanmp2</a> or claimed inconsistencies in global warming research data <a href="">#climategate</a> show that traditional quality control methods are of course fallible. So whilst quality control may be an important value of any research publication community, changing current practices is not a move from rigorous to less rigorous but from traditional and trusted to different and untrusted.</p>

<p>This project demonstrated that although it is difficult to change the practice of bibliography and citation, the difficulty does not come about by lack of technical capability nor by lack of support for new approaches. However it is complicated by the implicit assumption that traditional practice is trustworthy, so the reconstructive attempt must delocalise from community practice by demonstrating their fallibility, and seeking to show that community values may be upheld just as well by alternative methods.</p>

<h5>Open Biblio project review - year 2</h5>

<p>The stated aims at the beginning of the second year of development were to demonstrate to anyone with a reliance upon or capability to make use of bibliographic data - including developers, academics, Galleries, Libraries, Archives and Museums (GLAM) staff, and so on - what would be lost if Open Bibliography was not practically valued by a community committed to discovery and dissemination of ideas. This was to be demonstrated by proving the value of carefully managed metadata collections of particular interest to individuals and small groups, thus realising the potential of open access to large collections of metadata that was achieved during the first year of the project.</p>

<p>The project was successful in achieving these aims, and in doing so provided a range of outputs including software <a href="">#bibserver</a> <a href="">#facetview</a>, a selection of national bibliography open datasets and a community supporting them <a href="">#datahubbiblio</a>, and a bibliographic metadata format named bibJSON <a href="">#bibjsonspec</a> <a href="">#obstm</a>. These outputs have all been sustained beyond the end of the project and saw uptake in other projects, however no services were sustained as their purpose as demonstrations had been outgrown.</p>

<p>During the development of bibJSON a dichotomy arose between whether a specification is centrally controlled, or more of a communal agreement on use. The community of practice approach was followed in the end, as in the early stages there was more value in demonstrating practicality as widely and openly as possible than in maintaining close control, as this acted as a form of advertising and advocacy in itself as well as engaging others to contribute. Close control could have been possible with specific and ongoing commitment to maintaining a standard specification, however this would have been a focus on the specification itself rather than use by and growth of a community of practice, which can drive ongoing development as needs arise.</p>

<p>Community building and maintenance was found to be somewhat serendipitous, and when it comes to maintaining services a great deal of community engagement and sustainability effort is needed. Just as word of mouth can enhance reputation, community breakdown can detrimentally impact the project. The project found that by ensuring everything is as open as possible from the outset, the impact of any one particular project failure could be reduced as others in the wider community could step in. Where certain aspects of community discussion or particular development were not made widely available, such as via mailing lists or public code repositories, failings were either overlooked to later detriment or simply not available for further improvement and integration into the project output.</p>

<p>Over the course of this project it became clear that open source development practices provided great flexibility and power, and open access in general negated many difficult constraints, such as the problem of attribution stacking <a href="">#attstack</a> in large datasets. Whilst licensing remains an issue, it is also clear that the issue becomes more complicated as more attempts are made to use licensing to control reuse, and there is an increasing lack of clarity as different forms of licensing and different jurisdictions compete to provide similar functionality. The best approach to this problem was therefore also to remain as open as possible, reducing complexity to the minimum.</p>

<p>Over two years the concept of open bibliography has gone from requiring justification to being an expectation; the value of making this metadata openly available to the public is now obvious, and such access is no longer so difficult; where access is not yet available, many groups are now moving toward making it available.</p>

<h5>Reconstructing principle community values</h5>

<p>These Jisc Open Biblio projects demonstrate a concerted effort to reconstruct the practice of a value - accessibility of prior research for the purpose of ensuring quality. This value is part of a traditional practice of managing bibliographic metadata that has been successful for many years, but which now can be done differently because of technical advancement. But reconstructing these practices must contend with the fact that reconstruction may seem threatening to traditional values, or to the practices of particular individuals, and so the odd situation arises in which the social aspects are harder to tackle than the technical ones, and so reconstruction attempts are performed in the guise of software development projects.</p>

<p>These projects did include the development of a bibJSON specification, and so this may seem to suggest that it is by definition of standards that such reconstructions are achieved. However, the specification in this case was developed based on community need, and was itself a reconstruction of an earlier attempt which failed to gain traction because it specified how it <i>should</i> be used rather than <i>use</i> driving specification. So although these projects appeared to be technical in that they were ostensibly software development projects, their success depended on the socialisation of development and the technical demonstration of the practice of community values in new and potentially better ways.</p>

<p>By presenting new potential to a community of practice, these projects reconstructed values and overcame their delocalising aspects by using social statements of community principles and technical visualisations of community networks to advocate a new localisation of values around practices augmented with greater technological capability - incentivising reconstruction of practice by offering new potential. This is a good example of where practices that encompass these technosocial aspects of development may make such projects easier in future, rather than risking failure in attempting to perform technosocial developments as seemingly simplified technical demonstrations. The network visualisation tools demonstrated by these projects may even be appropriate to such a practice.</p>

<h4>Jisc Open Citations</h4>

<p>This section includes content from a Cottage Labs project report <a href="http://cottagelabs.com/news/opencitations-wrap-up">#clocwrap</a> that I co-authored. I am a founding partner of Cottage Labs, and this content is used with permission of lead author Richard Jones and co-author Martyn Whitwell.</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="https://docs.google.com/drawings/d/1lGjIesuFdHOiQllj_f6PyjV25JROBFZsa2pDCS-ppPw/pub?w=803&amp;h=434">
<p class="figure">The actors involved in the Jisc Open Citations project, linked by their interaction, via bibliographic metadata. Researchers and funders create and consume such metadata about their outputs, whilst libraries and publishers aggregate and disseminate it, to a public audience (that could include funders, researchers, and other subsets); the project developers in this case interacted directly with all but the public - except as they were already represented in the other subsets. (This is the same network as for the Jisc Open Biblio project, although it was instantiated in different actors.)</p>
</div>
</div>
</div>

<h5>Open Citations project review</h5>

<p>The Jisc Open Citations project <a href="http://opencitations.net">#ocnet</a> developed a new and enhanced Open Citations Corpus (originally developed in an earlier project) and the technical infrastructure to support the expansion of that corpus. This involved developing the architecture of the Corpus and the software tools that support it, such that it is extensible with additional data sources beyond the original PubMed Central (PMC) Open Access (OA) subset <a href="">#pmcoa</a>, and such that an increasing number of bibliographic records and their relations can be scalably stored and disseminated.</p>

<p>This work resulted in an open source software framework for importing bibliographic data sources <a href="https://github.com/opencitations/OpenCitationsCorpus">#occ</a>, demonstrated by implementation of support for arXiv and re-implementation for PMC OA, and development of visualisations to demonstrate the insights that could be gained by access to and use of such open citation data.</p>

<p>The import process includes an OAI-PMH <a href="">#oaipmh</a> implementation which can be used to regularly harvest new content from the data sources concerned, and other OAI-PMH compliant sources. Records are extracted from the OAI-PMH responses, converted into bibJSON, and stored locally. These data sources offer different ways to access their citation data, so each source has its own custom process to obtain that information. A periodical matching process across the local data looks for bibliographic records which also appear in the reference lists of other records, and asserts links between them. This produces an open corpus with identified relationships between bibliographic records across multiple external datasets, providing tremendous value as a map of relationships between open access literature and also references from open into closed datasets.</p>

<p>The objective of the visualisation work on this project was to answer certain example questions that could not be tackled with the original individual datasets prior to their integration and the addition of relation assertions. The questions range from the relatively simple to the relatively hard, both in terms of calculation but more importantly in terms of presentation.</p>

<ul>
<li>How many times has our 2009 paper on graphene been cited?</li>
<li>Who, among the academics we fund, has published the most papers on graphene?</li>
<li>Show me the time-line of publication dates of papers citating our 2009 graphene paper, by month.</li> <li>How many papers on graphene have been published by Oxford University academics over the past four years?</li>
<li>What has generated more impact, in terms of the sum of all citations to all the papers that we have funded on that topic - graphene research or fuel cell research?</li>
</ul>

<h5>Visualising the research citation network</h5>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="/static/phd/shotton.png">
<p class="figure"> A simple network graph of the relations between papers, journals, and authors, demonstrating how key nodes can be easily represented even without provision of numeric values or additional understanding of the underlying data.</p>
</div>
</div>
</div>

<p>Whilst it is possible to analyse the underlying data for answers to the questions the project wished to solve, such analysis is not necessarily straightforward and even representation of it in a familiar form such as a spreadsheet would leave it difficult to gain meaningful insight without a comprehensive understanding of the information involved. However, the answers to the questions were found to be easily representable in visual form, so the project developed timeline visualisations of citation events as well as network graph visualisations of relations between papers, journals, and authors. These visualisations allow people to quickly understand the answers without having to go into detail about the underlying dataset; for example, the clustering of papers represented as coloured circles around a central larger and differently coloured circle can easily indicate the importance of that central entity in the relation of the others, even without any specific information being present as to the numeric values of such relations. Although such representation obviously loses some detail, it can still be retrieved as necessary from the underlying data. For this reason, in this and other projects visualisation is becoming a very popular tool for simple representation of very complex datasets, as it is now increasingly easy to manipulate such complex datasets in a real time and highly interactive manner, further adding to the appeal of illustration as a form of communicating complicated concepts.</p>

<p>The ability to produce such visualisations was not newly developed by the project, and in fact the interactive network visualisation tool <a href="https://github.com/CottageLabs/graphview">#graphview</a> was built around an already popular visualisation library <a href="">#d3</a>. So the concepts and technologies - although relatively recent - are not new. However the use in this project of such network visualisations to answer the questions posed is further example of the increasing use of the concept of network relationships to discuss and solve problems - in this case not just to present information but to enable users to explore for themselves and to make new assertions by learning something about the relationships between entities represented in the data.</p>

<h5>The emerging importance of networks</h5>

<p>Combining disparate datasets and visualising the relations between entities offers new ways to explore and understand data, and also offers an additional reason for doing so - it introduces the potential to learn more than just the values presented in the raw data. Further personal work on this opportunity led to the development of citation mappings for comparing the practices of particular research communities, enabling inferences about the performed practices within those communities.</p>

<div class="row">
<div class="col-md-6">
<div class="thumbnail">
<img style="height:380px;" src="/static/phd/complementary_medicine_citations.png">
<p class="figure">One thousand open access articles related to complementary medicine, clustered by citation. Note the large outer ring of poorly cited articles, and the lack of large (highly cited) nodes, and overall loose clustering indicating a low degree of interrelation.</p>
</div>
</div>
<div class="col-md-6">
<div class="thumbnail">
<img style="height:380px;" src="/static/phd/malaria_citations.png">
<p class="figure">One thousand open access articles related to malaria research, clustered by citation. Note the narrow outer ring of poorly cited articles, and the common occurrence of large (highly cited) nodes, and overall dense clustering indicating a high degree of interrelation.</p>
</div>
</div>
</div>

<p>The above pair of figures compare the clustering of citations from articles within two distinct research communities - complementary medicine and malaria. One thousand articles from each are queried from the Open Citations Corpus, and the top thousand citations are used to group the articles. The extent to which the clusters pack tightly, and the visibly larger nodes in the tighter cluster, show that there are a greater number of uses of each citation in malaria than there are in complementary medicine. It is therefore possible that a greater degree of related work is occurring, and more use of the available material via citation may demonstrate a greater degree of attention to and scrutiny of published materials. However, it could also indicate other things. The aim of this thesis is not to make claims about what it does or does not indicate, nor to make claims about either field of research, but to show the emerging usefulness of actor-network relationships in exploring such claims and the additional benefit that may be gained by representing such actor-networks in visualisations (similar to the aforementioned controversy mapping as didactic actor-network theory).</p>

<h5>Visualisation of value reconstruction</h5>

<p>Relation by citation is an extremely important practice of scholarship, imbuing cited works with value by their relation to famous works or authors, demonstrating the extent to which the work has been researched, or simply by showing the extent to which a work is exposed to its community. Visualising that relationship not only illustrates the practice of that value within the scholarly community, but also contributes to the problem of reconstructing those values.</p>

<p>Although it is possible with these approaches to understand more about how scholarly works relate, and to visualise the impact of authors and works upon others, it is also a departure from tradition - this is not how citation metadata was expected to be used, and so it is using a dataset for other than its intended purpose. So the use of such tools is yet another problem of delocalisation, as questions arise as to how such use should be understood, and to what extent it should be accepted as valid academic practice. Some will accept such use whilst others will not, and so a gap forms between traditional and new practice. However, these very same tools begin to show signs of being applicable to the delocalisation problem itself - after all, the problems of delocalisation are changes in the relations of actors in a network with respect to their practice of shared values, and so it seems that there are dynamically changing values assignable to those relationships that could perhaps be somehow better understood through visualisation. Development of such tools therefore does not just enable visualisation of relationships and simplify the representation of answers to a range of questions, but it becomes part of the setting and solving of new questions and new challenges to the academic community itself, and can become part of the mechanism by which the community reflects upon itself.</p>

<h4>Jisc Gateway for Higher Education</h4>

<p>The Jisc Gateway for Higher Education project (G4HE) was a national project to make use of openly available research metadata to connect project funding to project outputs, to show the value of research performed in the UK and to report on the value of research collaborations between UK research institutions, private organisations, and beyond. The study of this project provides a perspective on software developments at a national scale, where the local is not the starting point that it was in the earlier studies. This study instead considers a project where large institutions (hybrid actors) seek to state and enforce the community values that empower them.</p>

<p>The project review includes material from project documentation that I co-authored, and I was involved in this project as technical developer. The original project materials are available on the project documentation website <a href="http://g4he.wordpress.com">#g4he</a> and on the Cottage Labs website project page <a href="http://cottagelabs.com/projects/g4he">#clg4he</a>. Use of this material has been authorised by Neil Jacobs of Jisc in his role as project manager. Neil has also approved the content of this section for publication in this thesis, and my thanks go to him and the rest of the project team for their support.</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="https://docs.google.com/drawings/d/1RuadSKHFoHYW5kPY-d6i7R8kQq0Ure10bF8oPKqjvEM/pub?w=887&amp;h=683">
<p class="figure">The actors involved in the Jisc Gateway for Higher Education project. This project had a clear separation between project team and development team, although some members of each were on both. There was also a separate development team building the GtR API and managed by the funding councils. The development team became more highly networked in this case than the project team itself, due to differences in approach to communication with wider audiences.</p>
</div>
</div>
</div>

<h5>G4HE project review</h5>

<p>G4HE was a national infrastructure project led by Jisc with the aim of improving access for Higher Education Institutions (HEIs) to information held by Research Councils UK (RCUK), by giving something useful in return for all the effort that goes into creating, maintaining, and collecting this research information. G4HE uses the RCUK Gateway to Research API (GtR) <a href="">#gtr</a> to retrieve data collected from institutional research management systems such as Researchfish <a href="">#rf</a> and ROS <a href="">#ros</a>. The project also provides recommendations for how to improve the administrative and technical aspects of the task of collecting and managing research information.</p>

<p>The purpose was to find ways for people within the HEI sector to benefit from the aggregated research project metadata, and to prove the value of tools based on (and such as) the GtR API. After a period of stakeholder engagement the key use cases that emerged were:</p>

<ul>
<li>Report to senior management about key collaborations within our grant portfolio, so we can identify institutions with whom to build partnerships</li>
<li>See who we are collaborating with in particular fields, and find other institutions in the same field that we are not already collaborating with</li>
<li>See number and value of collaborations with commercial partners to get an idea of the value and extent of our knowledge transfer activities - which is useful for internal and external (e.g. HE-BCI) reporting</li>
<li>Benchmark funding awarded between groups of researchers at my institution with groups at other institutions to ensure that we are performing at the expected level</li>
<li>See how much a department has brought in because it helps me show the success of our research groups</li>
</ul>

<p>After stakeholder analysis the development team created prototype reports to share with a test group of research managers as part of the agile development practice, performing regular sprint meetings and prototype iterations. Parallel work continued with the research councils to track development of the data collected by GtR from seven research councils. Aggregating data presents the opportunity for enrichment with content from third party services, and also allows for the development of tools that would not be so useful within a local context - for example, it becomes possible to represent current and/or potential academic relationships beyond local organisational boundaries.</p>

<p>As a result of successful demonstration in G4HE of these sorts of benefit, GtR and the research councils are considering opportunities for enrichment via services such as the Open Researcher and Collaborator ID Initiative (ORCID) <a href="">#orcid</a>. This represents a growing trend across data providers to use commonly accepted definitions and identifiers wherever possible, so that sharing data beyond organisational localities can be beneficial with less effort.</p>

<p>The software development outputs of the G4HE project are publicly available in the open source project repository <a href="https://github.com/g4he/g4he">#g4hecode</a> and a demonstration of the service that the software implements is available for use <a href="http://g4he.cottagelabs.com">#g4hetest</a>.</p>

<h5>Visualising the research funding network</h5>

<p>As with the Jisc Open Citations project, network graph visualisations emerged as a useful tool for exploring the G4HE data. Whilst standard bar and pie charts were ideal for displaying the specific values of particular entities such as reearch collaborators or funding sources, these types of graph could not so easily illustrate the relationships between such entities. This is what a network graph is ideal for, and although it may be less precise it can still convey enough of the concept of size or value to empower users to form opinions and make decisions based on the graphs.</p>

<div class="row">
<div class="col-md-6">
<div class="thumbnail">
<img src="/static/phd/brunelbars.png" style="height:300px;">
<p class="figure">A traditional bar chart generated by the G4HE service, showing quite precisely the value of various organisations in terms of research funding.</p>
</div>
</div>
<div class="col-md-6">
<div class="thumbnail">
<img src="/static/phd/brunelbubbles.png" style="height:300px;">
<p class="figure">A less precise network graph with no relation to external scale such as monetary value, but more capable of highlighting the value of various organisations to particular research contexts.</p>
</div>
</div>
</div>

<p>This network graph example from G4HE shows the top ten best matches for a given entity, in this case Brunel University. The matches are new potential matches - other entities that Brunel does not yet have a relationship with, but who may be of relevance to the work that Brunel wishes to do. Having a relationship is quantified as being involved in any other project with Brunel, and relevance is defined by provision of keywords or sources from which keywords can be extracted, such as other project descriptions or organisational web pages.</p>

<p>Whilst relevance is controllable by the user in providing keywords or keyword sources, the relationship harbours a hidden constraint - it relies somewhat on the definition encoded into the software as to what sorts of collaborative role instantiate involvement with a project. However, creating this definition turned out to be the source of a number of problems for the project.</p>

<h5>A problem of delocalisation in context</h5>

<p>The data in GtR was collected from different sources (various research councils using various research management software systems) and was originally collected and processed - before the GtR and G4HE projects existed - for different localised purposes. Some of the data available via GtR had been made widely available prior to the GtR project, but it was originally collected mainly to enable councils to monitor and report on the research they fund, and to inform their own planning decisions. Removing such data from local context and putting it to use in a delocalised aggregation, beyond the scope of a particular organisation or small collection of organisations, unsurprisingly caused problems when the data was interpreted by users translating it back to their local understanding in their use of the interface.</p>

<p>Unfortunately, in order to tackle this problem, the team determined to define the meaning of specific terms so as to minimise what they experienced as misinterpretations by consumers of the service.</p>

<p>The data in GtR is reliable in the sense that it accurately reflects the data recorded by research councils during the process of awarding grants and recording the outcomes from those grants, however during the project testers had raised concerns that the alpha release of the G4HE user interface showed collaborations between organisations that had no such collaborations, or included organisations that did not actually exist. None of the G4HE data processing was capable of causing this to happen, and further tests confirmed that the data in G4HE was up to date, uncorrupted, and only altered as intended for use in the G4HE service. However, the project team had decided that the meaning of collaborator should be defined as follows:</p>

<blockquote>a collaborator is any organisation affiliated with a PI (principal investigator) or CI (co-investigator), and any organisation with the role "Lead Research Organisation" or "Fellow"</blockquote>

<p>This meant that a test user in a specific organisation was able to see a relationship between their organisation and another because a collaborator (a person) in one of their projects had perhaps previously worked at a different organisation, or had multiple organisational affiliations. These relationships had perhaps never been recorded in the data stored in the local systems of the organisation of the test user, and so could not be confirmed as accurate relationships. This caused the test users to report such collaborations as inaccurate.</p>

<p>That such relationships could only be identified at the point of aggregation of the delocalised datasets was actually a win for the development team, in that the project was proving the value of performing such aggregations and sharing such data - it <i>was</i> making it possible for new discoveries to be made, because it is quite often true that a collaborator may have multiple affiliations and they are often relevant to research collaboration, as people tend to work in organisations that support their personal research goals. So the project was succeeding in identifying relationships of value, and yet at the same time was seen to be making errors by the users it sought to provide a service to.</p>

<p>The local understanding, within the project team, of their success criteria was not the same as that of the test users. And the meaning of collaboration had been lost when the data driving the service had become delocalised from original context by its localisation into the GtR service. When it was shared back to the test users, it underwent delocalisation once again, as it became subject to interpretation by users beyond the project team, and the reconstructed meaning of collaboration was not successfully communicated to or accepted by the test user communities.</p>

<h5>The shortcomings of delocalisation by standardisation</h5>

<p>In the local practices of particular consumers, value had already been placed on the original data, and the terms used to interpret it already <i>had</i> meaning, but those meanings had been lost when the data was aggregated into GtR. Although this was apparent to the project team members, the problem was misconstrued as a <i>local</i> lack of clarity that could be solved by reconstructing a meaning, and that by <i>exporting</i> a standardised meaning the practice of the user community could be dictated as they interacted with the user interface. In the context of what appeared to be a technical issue, there did not seem to be anything wrong with instructing a consumer of data as to what the correct interpretation of such data should be - in fact, proper documentation and usage instructions seems like a good idea.</p>

<p>But this was not a technical issue, and treating it as such inadvertently led to yet another problem of delocalisation, when attempts were made to export not only a collection of data but a local understanding of meaning - a local value - that sought to justify itself by appeal to that higher power of technical standardised definition. The user community did not accept this totemisation, and contemporary project management and technical development practices (implemented with the best of intentions) proved insufficient for the problems of delocalisation, thus supporting the claim of this thesis that technosocial development approaches must be considered.</p>

<p>This project lacked a practice for collaborating on the meaning of collaboration, and tools for supporting the reconstruction of the values of a user community. A different sort of technosocial approach in place of standardisation may instead enable a producer or consumer to share their own value judgement as to the meaning of the information to be communicated, at the point of consumption; the producer and consumer may then evaluate data and infer meanings appropriate to their community, because they will naturally <i>choose</i> what to <i>believe</i>. If the practice of technosocial development is to support reconstruction of values, then studies of actors performing such value reconstructions are now necessary.</p>

<h3>Studies in open scholarship</h3>

<p>These studies in open scholarship present ethnographies of actors in scholarly networks, the performance of which required both a method of identifying scholarly actors, and a methodological approach.</p>

<blockquote>The <b>scholarly method</b> is the body of principles and practices used by scholars to make their claims about the world as valid and trustworthy as possible, and to make them known to the scholarly public.<br>
<small>Wikipedia <a href="http://en.wikipedia.org/wiki/Scholarly_method">#wpscholarship</a></small>
</blockquote>

<p>To identify scholars by tradition such as by qualification or professional affiliation risked stepping directly past the problems of interest in this thesis. So instead a simpler approach was applied, of studying any actor that self-identifies as being scholarly or practicing scholarship. Most actors were additionally involved in an open movement of some sort, increasing their relevance to these studies. Open scholarship in this thesis means the activity of any self-identifying practitioner of a scholarly method in creating, accessing, sharing or archiving a scholarly work - scholarly works being any work deemed valuable in a scholarly context to any scholar. Open scholarship is not taken to mean any particular politic or ideology, although of course the various scholars involved in these studies exhibit their own politics and ideologies - scholars expose their values whilst performing their scholarly practice.</p>

<p>The first methodological issue was the prior and direct involvement of the researcher (myself) with the target community. Rather than accepting this as a limitation, it is instead utilised as a rare hybrid perspective of developer/ethnographer, accepting a natural localisation to a field of expertise whilst practicing a reflexive delocalisation. The risk of personal bias is limited in that these studies are only of actors and statements and events that can be shared in the public domain. Anonymity is thus not guaranteed nor even attempted, and no outcomes that rely on anonymity or random sampling shall be produced. Instead, the practice of these studies will reflect the nature of the subjects that is of most interest - their openness, what they are willing to say and do publicly.</p>

<p>Whilst ethnographic methodology seeks to limit bias in surveys and interventions, the setting of <i>any</i> questions proved impossible without some inference of direction, and a suitable survey length was hard to gauge for such a loosely defined target community. So this was again taken as a feature rather than limitation, leading to a very directed and restricted first survey specifically studying the problem that statements communicate values, and a second study of video ethnography where no questions at all were set and interviewees start only from a knowledge of the title of the study they are joining, and acknowledgement that the study is in the public domain. The studies conclude with two attempts to consider hybrid actors at the community scale and to begin to search for tools to support this technosocial practice of ethnorgraphic study. Similarities will be drawn with controversy mapping, and with the use of visualisation tools demonstrated during the first set of studies.</p>

<p>These studies seek evidence of the problem of delocalisation, instantiated as attempts by actors to make use of new tools and practices, delocalising them from the traditions of their community. From the studies as a whole these problems of delocalisation will be brought to the fore, and whilst it may not be possible to explain <i>why</i> they come about (or to stop them from happening, if localisation and delocalisation are natural occurrences) it should be possible to form a how-to rather than a why. This would be in keeping with how Latour may describe the findings of such studies, and a how-to would be an apt outcome for advancing a practice of technosocial development.</p>

<h4>Surveying scholars</h4>

<p>The immediate problem of these studies was that it was not possible to simply define what open scholarship is or what a scholar is, without having committed to a reconstruction of value before even beginning the research. So it was difficult to formulate questions to put to the target community without making implicit assumptions of the values to be explored. Instead, the making of such assumptions was made explicit, and the concept of a survey was simplified down to just one purposefully contentious statement on open scholarship.</p>

<p>Similar to the aforementioned Open Definition, Panton Principles, and Open Biblio Principles - statements formulated by a small group representing a community and then promoted for endorsement - a statement was presented as an experiment in collaboratively computing the values of open scholarship. Eight polarising terms were included, and offered as simple binary choices in an online survey consisting of just the one statement <a href="">#siossurvey</a>.</p>

<div class="jumbotron">
<p>Scholarship is {OPEN|CLOSED}, {COLLABORATIVE|COMPETITIVE} education and research, the materials and output of which are {SHARED|RESTRICTED} via {INCLUSIVE|EXCLUSIVE} distribution methods that are {CHEAP|PROFITABLE} to contribute to and {FREE|COSTLY} to access, and where quality is maintained via {PUBLIC|ANONYMOUS} review, and where the primary purpose of attribution is {PROVENANCE|ACKNOWLEDGEMENT}.</p>
</div>

<p>The simple binary questions are a rudimentary social computation game as a survey, with the interesting difference that the answers to the simple questions could not be independently validated as usually expected in a good social computing configuration. However, it remained possible to yield a statement of open scholarship based on aggregation of survey feedback and to analyse the choices for contention:</p>

<div class="jumbotron" id="statementhero"></div>

<p>Contention is represented by the difference between counts of votes for option x and option y divided by the total count of votes for x and y. A value of 1 represents no contention and values approaching 0 indicate increasing contention.</p>
<p>( &#916; x y ) / ( x + y )</p>
<ul style="font-size:1em;">
<li>open | closed = 1</li>
<li>collaborative | competitive = 1</li>
<li>shared | restricted = 0.8888</li>
<li>inclusive | exclusive = 1</li>
<li>cheap | profitable = 1</li>
<li>free | costly = 0.8888</li>
<li>public | anonymous = <b>0.0588</b></li>
<li>provenance | acknowledgement = <b>0.1765</b></li>
</ul>


<h5>Contentious statements of value</h5>

<p>Once the simple binary options represented as a single statement had succeeded in drawing the attention of respondents, they were invited to provide further feedback on any of the eight options. The contentious statements turned out to be last two - whether or not peer review should be public or anonymous, and whether or not the main value of attribution was provenance or acknowledgement.</p>

<p>In the responses to the peer review statement it is obvious that peer review is considered both beneficial and necessary, no matter what, by all respondents. So peer review is clearly a very important value to the scholarly community. However there are very divided opinions as to how it should be practiced. Some respondents consider anonymity to be crucial, whereas others make it clear that anonymity is usually overcome by secondary information anyway. Some respondents believe there should be a choice, whereas others are concerned that hierarchies may emerge without anonymity, and still more are concerned that without anonymity reviewers may be scared to be honest.</p>

<p>Responses to the second highly contentiious issue indicated again that the issue itself - attribution - is an important value for scholars. Further feedback indicated that whether attribution is used for provenance or acknowledgement, what is important is that attribution is seen as a mechanism of quality control, just as peer review is. The value of referring back to earlier work is that a sense of quality can be established. Most respondents saw value in attribution as both provenance and acknowledgement, so this point is contentious in a different way, in that it highlights a value about which most people agree, but for which there are multiple accepted purposes.</p>

<p>It is around contentious points such as this that communities localise and delocalise - for example most believe that peer review is valuable to the community and so this is a value around which the community localises, and yet there is disagreement on whether it should be practiced publicly or anonymously; peer review may historically have been performed anonymously and that may become a traditional practice, so those that believe it should be public may distance themselves from those that do not, and the problems of delocalisation may arise. If a software development project took place in this community it may be that one value guides the development whilst the other is overlooked, leading to creation of a new tool of digital scholarship that is acceptable to those leading an open movement for public peer review, but that further delocalises the community for which it is developed.</p>

<h5>The scholarly practice</h5>

<p>Having overcome the problem of directing the feedback of respondents by purposefully polarising their introduction to the survey, five further optional questions were set, in the hope of learning more about the scholarly practice with respect to discovery, production, quality, community, and accessibility. Responses to these further questions indicated the occurrence of shared values expressed with differing opinions on practice, in particular as respondents made use of new tools of digital scholarship and the output of open access movements to perform their research.</p>

<p>Respondents were asked what sorts of <b>discovery</b> methods they employed when searching for information. The most common were Google / Google Scholar, Web of Science / Web of Knowledge, colleague recommendations and mailing lists, the web pages / blogs of other scholars, Wikipedia, and the library. It was pointed out that Web of Science only provides journal information whereas Google Scholar provides more, and can assist in finding alternative (free) sources for articles in subscription journals. At least one respondent stated they would never admit to reading Wikipedia whereas others listed it as a source without concern. A common practice was to follow chains of references from one article to another, regardless of starting point, and Wikipedia includes reference lists just as traditional journal articles do. At least one respondent admitted never physically going to a library, although another believes books still hold more credibility than web pages (but does not indicate how).</p>

<p>Respondents were then asked if they had ever authored or <b>created</b> their own work, and if they placed any access restrictions on it. Those with peer reviewed publications have either published openly (i.e. in an open access journal), or have done so where affordable, or would have preferred to publish openly if it were not for impact factor considerations (indicating that in their field open access journals have a lower impact factor). Some indicated that restriction can be justified if contributors to data expect it. Some also publish on blogs or publish other creative works such as photos, in which case it was done under a creative commons attribution license. All respondents thought that attribution is important, for numerous reasons including that they like to be attributed or think they have a right to be attributed, although at least one pointed out it is not the attribution so much as that users prove where they got what they used, so it can be validated. At least one person would restrict access on grounds of stopping predatory re-use, although it was not made clear what sort of re-use would be considered predatory.</p>

<p>Respondents were also asked how they personally judge <b>quality</b> when searching for relevant material. Some consider the journal of publication to be important, as previous experience with that journal may indicate quality peer review. Almost all consider reputation of the author and either already know or look up previous works and perhaps affiliations. None appear to be willing to accept work outright based on journal or author - even if in a particular journal considered of high quality, the particular work in question is reviewed on merit. At least one respondent treats everything with skepticism and behaves as if reviewing the work anyway, regardless of whether or not it has already gone through a trusted peer review process. Some would accept works of apparent high quality on blogs or sources such as Wikipedia, and particularly if the reputation of the author is good. Most, particularly when studying a field in which they are less experienced, would consider the opinion of colleagues and may allow that to guide them until they gained sufficient personal experience in the field.</p>

<p>As respondents self-defined as scholars there was no pre-conception as to what sort of scholarly <b>community</b> a respondent may be local to, however they were asked to describe any practices of their community that had an impact on what sort of scholarly materials they could access or make use of. One respondent would cite non-peer-reviewed material carefully, if the material is sufficiently well understood and is trusted (in this case meaning if in arXiv). At least one person sees cases where they cannot use something as a problem to be solved rather than a reason not to use it, in which case searching for alternative acceptable sources for the same material would take place (but the same person also believes knowledge has to be paid for somehow). One person would read anything relevant to their field regardles of where it is, and consider it with an open mind. Whether particular journals are paid for by their institution does not have an effect on whether the content is considered valuable. A few use inter-library loans, and some are happy with this service whilst others find it time consuming and costly. In the case of data, one person states data that would be useful is often not available or is insufficiently annotated. One person found no barriers to access but instead encountered too much poorly managed information. At least one PhD student believes their work suffered as a result of lack of access or lack of usability, and claims to have found materials such as some PhD theses more relevant than published articles but yet not considered a valued source by their institution. One person specifically lists paywalls for journals as getting in the way, one would like to use a dataset but cannot afford to, and also found papers that are too expensive and not paid for by their institution, but otherwise would be used.</p>

<p>Finally respondents were asked to describe any encounters with specific barriers to <b>accessing</b> material. This for most respondents was journal paywalls, although most also said they have ways round them. At least one circumvents paywall barriers by searching for alternative sources, but would pay access costs if they could be shown to be reasonable. Some respondents ask colleagues for copies, and at least one believes that following the law to the letter would be unworkable and prohibitively expensive. One person caused their entire university to be cut off because they "downloaded too much", despite having paid as an institution for the right to access. Emailing authors or members of other institutions with access and asking for copies to be sent was common, and is indicated as necessary regardless of legality, as it is critical to ongoing work. One respondent mentioned that the source code of a simulation was unavailable, and in some cases even the authors were unable to provide it, making experiments difficult or impossible to replicate. At least a few respondents believe journals are increasingly too expensive, and at least one would abandon an expensive relevant article in favour of an open access one. Most respondents indicate they would ask colleagues or authors, or search online before asking or going to their library, or asking for inter-library loan.</p>

<h4>Following scholars</h4>

<p>Having made an initial study into the actor-network of the scholarly community via the survey, and having encountered evidence of shared values expressed with differing opinions of practice, closer ethnographic studies were performed with the aim of gaining more insight into the values and practices of actors in their everyday network of interaction. As previously noted, scholars were recruited for these studies solely on the basis of their self-identifying as scholars, and being willing to discuss issues that would be shared publicly, so as to minimise inadvertent introduction of delocalisation problems in this work itself. Permission has been received (with gratitude) to publish these interview materials openly, from the following scholars:</p>

<table class="table table-bordered">
<thead>
<tr> <th>Name</th> <th>Role</th> <th>Description</th> </tr>
</thead>
<tbody>
<tr> <td>Misti Cooper</td> <td>Undergraduate physics student</td> <td>Misti works for a university in a finance/administrator capacity and has recently completed a BSc in Physics at the Open University.</td> </tr>
<tr> <td>Tim Reid</td> <td>Physics research scientist</td> <td>Recently Tim has been working on a project looking at arctic snow cover in the north of Scandinavia, in particular in areas where there are forests that are snow covered for at least half of the year, studying reflectance - an important parameter for climate and weather prediction models. He is currently at the University of Edinburgh.</td> </tr>
<tr> <td>Eva Giraud</td> <td>Lecturer and researcher in Critical Theory</td> <td>Eva is a lecturer in Culture and the media at the University of Nottingham</td> </tr>
<tr> <td>Josh Bowsher</td> <td>PhD student in Critical Theory</td> <td>Josh is doing a PhD in Critical Theory and his work focuses on a critique of transitional justice. He is currently at the University of Nottingham.</td> </tr>
<tr> <td>David Flanders</td> <td>Research community manager</td> <td>David is a research community manager and has worked for numerous funding agencies, and open research mandates play a key role in the decisions he regularly makes. He is currently at the University of Melbourne.</td> </tr>
<tr> <td>Jenny Molloy</td> <td>PhD student in malaria research</td> <td>Jenny does pathogen research on mosquito immunity, studying disease transmission, at Oxford University.</td> </tr>
<tr> <td>Andy McGregor</td> <td>Jisc Innovation Officer</td> <td>Andy is an Innovation Officer, which is a new role holding responsibility for defining new approaches to innovation.</td> </tr>
<tr> <td>Brian Hole</td> <td>CEO of Ubiquity Press</td> <td>Brian works at Ubiquity Press, a new publisher that publishes datasets rather than journal articles.</td> </tr> <tr> <td>Tom Olijhoek</td> <td>Freelance scientist</td> <td>Tom is a freelance scientist and also an open access advocate</td> </tr>
<tr> <td>Richard Jones</td> <td>Co-founder of Cottage Labs</td> <td>Richard is an open source software developer and senior partner of Cottage Labs</td> </tr>
<tr> <td>Emanuil Tolev</td> <td>Developer at Cottage Labs</td> <td>Emanuil is a recent university graduate and open source software developer</td> </tr>
<tr> <td>Theo Andrew</td> <td>Scholarly Communications Officer</td> <td>Theo is a Scholarly Communications Officer at the University of Edinburgh library</td> </tr> </tbody>
</table>

<p>This set of studies were performed as visits to the scholars place of practice, following them in their daily routine, and closing with a video interview of an open discussion guided only by an introduction to the concepts explored in this thesis, and with a request to discuss openly any issue that came to mind. No questions were set in advance, and no direction was set for the interviews, although some idea of areas of interest had of course been formed from analysis of the survey results.</p>

<p>The goal of these interviews was two-fold: firstly, to understand more about the scholarly practice and to find more evidence of delocalisations from local community as a result of uptake of new digital scholarship tools or other causes; and secondly to further explore the concepts emerging from the studies in software development of representing values and practices and orientation towards them in some way that might begin to support the advancement of a practice of technosocial development. These were jointly achieved by a combination of traditional analysis of the videos and copying out the transcripts, followed by processing of the textual data into an elasticsearch index queried by software enabling interactive network graph visualisation.</p>

<p>The video recordings and full transcripts are available along with the deposit of this thesis, however the following section describes their content and some brief excerpts demonstrate particular relevance to three of the four target audiences.</p>

<div class="hidden-print">
<p>In the case of viewing this thesis in original form as a web page, video interviews are also embedded below for easy reference.</p>

<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/86373559" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/86421849" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/87591913" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/87142795" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/87142794" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/87815209" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/88302046" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/89094370" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/90531346" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
<iframe class="thumbnail" style="display:inline;width:375px;height:210px;" src="//player.vimeo.com/video/90653755" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>

<h5>Ethnography of practice</h5>

<p>The practice of scholarship at first is an expression of the value of learning. Actors seek out new information perhaps as surely as they will seek out food and shelter - after all, learning more about an environment is a sure way to discover any food and shelter present within it. For every actor, any source is initially considered as potentially valuable, and efforts will be made to gain futher access in order to personally assess the relevance of the found material.</p>

<p>If a potential source of information cannot be accessed for any reason - from the library being shut to the web site being down to the local institution not having paid the subscription fees - this is considered anomalous, inconvenient at best and dangerously risky at worst. No actor indicated simple acceptance at such situations, and the first response was a form of "why". Of course, in simple cases where the lack of access is obvious and reasonable in local context (such as the library being closed at night), then not much more comes of it. But when there is not an immediate local explanation, the actors respond with suspicion that only subsides once they believe they have been able to make their own judgement about the relevance of any given artefact to their interests.</p>

<p>Where some artefact of information cannot be accessed, the personal value of it cannot be judged, and actors are uncomfortable with relinquishing this valuation to a third party. Such relinquishing is only accepted when it seems a worthy payoff for convenience, or is an obvious necessity of technical limitation. For example, access to an abstract used to be considered reasonable enough, when access to the full resource was harder to come by - it was cheaper to deliver small documents than large ones; however now that vast quantities of textual information are easily available online, scholars no longer feel that access to an abstract is sufficient nor reasonably balanced against the cost of delivering the full text. Similarly, the developers that were studied expect full access to source code and to be able to discuss any aspect of a piece of software on forums such as stackoverflow, and even to get full demos of if not completely free access to useful software and data.</p>

<p>Although any particular artefact is generally deemed valuable by personal judgement, there are certain signifiers that act as short circuits to this practice. For example, particular journals or authors or web sites are considered as authoritative sources by some actors, based usually on a combination of personal experience and reputation of the source within a community of practice. This relates to the issue of provenance brought up in the survey studies, where provenance was shown to be valued by respondents but as having mutliple acceptable practical applications. These quick signifiers of value act as filters, narrowing the search space of potentially relevant materials, and so they are applied by actors in relation to their perceived convenience - where they help to reduce a vast search space they are a worthy trade-off for accuracy, but where the search space is already small then they are less likely to be used.</p>

<p>Judging value clearly requires access, and so access is expected for valuation. Where access is more difficult - which could be due to restriction of access to full content <i>or</i> due to full access to so much material that judgement becomes difficult - then local practices form which delocalise (groups of) individuals from a collective value. For exampmle, a certain group may place value on a particular author name or journal whereas another group do not, thus leading to these two groups that share the same value having different local practices for identifying value, and delocalising each from the larger collective. Localisation and delocalisation go hand in hand, to some extent, in the reconstruction of value practices. For all of the actors seeking information, however, there is a shared practice of seeking ways around barriers to acess; paywalls are circumvented by sharing access permissions, copies are shared between actors. Such barriers as cost or copyright are not seriously considered as valuable qualities of the works in question, and whether a work is expensive or copyrighted does not make it more valuable. Works are not filtered on the basis of their accessibility or on their cost in the way that they may be by author or journal, and where unwanted cost or access restriction is discovered it is removed as quickly as possible. Such qualities of a work are clearly in stark contrast - authorship capable of conveying an optional sense of value, access control conveying a sense of unavailability for valuation.</p>

<p>Beyond discovery of relevant material, actors may however find that the works they have chosen to value are not acceptable by their wider community - a further problem of delocalisation. It is possible for an actor to discover a highly relevant work that they personally judge valuable after gaining access, but that their institution cannot accept. For example the scholar that finds Arxiv to be the best source of recent expert discussion on the particular branch of physics they wish to study may also find that Arxiv is not considered a valid source when preparing a literature review for submission to their own institution. This appears to emerge specifically because of that feature of a community of practice being one that shares filters or short circuits - a quick way to profess value to the many members of a community is to imbue a particular quality of works, such as their place of publication, with value. So again the processes of localisation in community and delocalisation from community can be seen to naturally arise.</p>

<blockquote>...getting access to the paper wasn't always easy. I did all my searches with arxiv which maybe i should specify as you told me that was an ambiguous word. Arxiv means the arxiv website. But the project that I had to submit, every reference had to be from a peer review jounal. That includes preliminary publishings and white papers. So once i found papers that were useful on arxiv I then had to go and find the actual published version of it through publisher or university libraries and a lot of the time that meant I had to discount papers as they didn't make the published status, but we'd know who the authors were and go looking for similar work they had done in the area through the publication.<br>
<small>Interview excerpt from Misti Cooper, of particular relevance to digital scholars</small>
</blockquote>

<p>For those actors that had published their own works, peer review was again a very important and contentious value. For all such actors, peer review was considered indispensible to the practice of scholarly production, and in fact was stated as one of the most rewarding aspects of being part of a scholarly community. However this was not because peer review was a useful filter or short circuit to finding quality work (although in some cases it is used as such) but that it is a creative social activity that helps a scholar to develop their skills and to form relationships. Being able to communicate with peers and receive feedback is part of what it is to learn, and part of the scholarly method the actors self-identified with. What clearly matters much more than the question of whether something has been peer reviewed is whether some<i>one</i> can experience the collaborative benefits of <i>doing</i> peer review.</p>

<p>The software developers of the study were also scholars of sorts, having gone through university education and having learnt more about their profession by further study. However they had also been involved in software developments of digital scholarship tools for the benefit of other researchers. A particularly relevant example was raised, in which the developers had encountered a requirement of a software system for a university library, stating that whenever a copy of a manuscript was issued, there had to be a page on the user interface where the user confirmed receipt of a copy of the manuscript in question - to replicate the traditional practice of signing for a book when removing it from the library collection. This was clearly a practice on the value of preserving important works, and it was believed to be necessary to port that same practice over to the new tool under development. However, such confirmation was no longer necessary, because the new tool was providing access to digitised copies of the works, which are essentially free copies of the content. So, whilst the originals continued to have value as artefacts, it was no longer necessary to take such care in safeguarding access to copies of their <i>content</i>. But this seemingly simple change of practice - although technically easy to implement - was not easy to negotiate with the community of librarians. Over time, their practice had developed a tradition from which they had gained additional capability - not only safeguarding rare manuscripts, but <i>knowing who had accessed them</i>. This need to know may well have also been a value of the community in question, but had not been listed as such in the requirements of the development project, and so introducing a tool that did not conform to this practice not only changed the traditional practice of a value but had an unforeseen effect on a value acquired by librarians due to their traditional role as gatekeepers to knowledge.</p>

<blockquote>...we were told early on in the project that it was a requirement that people sign some kind of license agreeing to some terms of use before they read any thesis and the people who were responsible for the print thesis were adamant that this was a requirement. So we put it on our list but we were slightly suspicious of it and as we went through the development it became clear that replicating this requirement wasn't really possible in the web environment and after a bit of to-ing and fro-ing we discovered that actually the reason they wanted people to click through a licence was the current process was that people would sign a piece of paper before they received the print thesis but the reason they signed the piece of paper was not so they had signed the piece of paper but so the librarian responsible for the thesis knew where the one copy of that print thesis was and when we understood that we understood that the requirement was not - you must click through a licence, the requirement was - we mustn't lose the copy of this thesis; and so that changes the way that we approach the problem. The problem was no longer one of licensing, the problem was preservation and storage. So there is a learning process to be had by running through the development.<br>
<small>Interview excerpt from Richard Jones, of particular relevance to software developers</small>
</blockquote>

<p>Some of the actors were related to scholarly activity both by their own studies and also by their roles as employees of funding councils. These actors expressed interest in helping scholars access material relevant to their work, and also a desire for information about how works their organisations had funded had subsequently been disseminated. They were further interested in learning more about how best to direct future funding and projects in ways that would help the scholarly community achieve greater success. The problems of doing this were seen as significantly social ones, to do with how to organise people around making decisions on where next to fund research and development, and then how to ensure that funded projects were well supported to deliver successful outcomes. Of course, tools and technology assisted in these tasks, and a great deal of project management activities were being supplanted onto web based tools such as collaborative document editors and online video and audio communication tools. One actor specifically mentioned their newly developed approach to these problems that was being called co-design, in which they were seeking to orient tool developers, funding providers, and consumers of created tools in such a way as to maximise the chance of decisions representing all relevant parties and for funded projects to deliver output that met the requirements of each type of stakeholder. This was clearly an effort to perform a sort of co-realisation, and an ideal example of a space in which a practice of technosocial development could prove useful.</p>

<blockquote>...Jisc is interested in Co-design because of the Wilson review. So in 2010 amid the financial crisis the government had a look at all the various organisations that were funded by the government. Jisc was reviewed by Wilson. That's why it's called the Wilson review and he had a number of things to say about all aspects of Jisc but innovation he had a few particular things to say about, one - that we did too many things, two - that they didn't always have the impact they should have because we were probably doing too many things and three - the process by which some of those things were agreed and started was often opaque. So Co-design as an approach to innovation was a response to that to try and address some of those concerns. It does that by ensuring that key stakeholders for the sector, the people who are going to use the things that innovation produces are involved in the process right from the very start and right throughout. So I don't mean that they are just consulted or they are used as a steering group. They are actually the people who we work with in a collaborative way to define what we are going to do, how are we going to do it and if we do it what success looks like for them.<br>
<small>Interview excerpt from Andy McGregor, of particular relevance to stakeholders in developments</small>
</blockquote>

<p>It is clear from these ethnographies of practice that shared values are expressed in different practices, and this becomes part of the definition of a certain localisation - a group of actors that share certain practices being a locality. It also follows that, because the localisations are formed by movements of individuals towards practical methods of doing what they need to do, that actors will continue to move in and out of such local settings as they come upon new tools such as those of digital scholarship. The ease with which new tools can be introduced into local practice is dependent on understanding the values expressed by communities of practice, and supporting reflexive reconstruction of practice.</p>

<h5>Practice of ethnography</h5>

<p>Given the growing appreciation of visualisation tools as solutions to the problems faced in the studied software developments, and given the prior art in design theory and controversy mapping, applying similar visualisation tools to the output of the ethnographic studies following scholars was the next step in attempting an extension the practice of ethnography towards the goal of this thesis of supporting technosocial development.</p>

<p>The transcript text was processed and loaded into an index, then made available via a visualisation interface similar to that used during the software development projects of the first set of studies <a href="">#transvis</a>. Taking the contentious issues identified by the open scholarship survey study as a guide for potential areas of interest to explore, the following figures were produced:</p>

<div class="row">
<div class="col-md-6">
<div class="thumbnail">
<img style="height:380px;" src="/static/phd/transcript_pr.png">
<p class="figure">A visualisation of transcript data showing actors (orange) where the terms "peer" and "review" are present in the data. A simple and clear indication of which actors are relevant to (or local to) this contentious issue, with actors of higher relevance appearing larger.</p>
</div>
</div>
<div class="col-md-6">
<div class="thumbnail">
<img style="height:380px;" src="/static/phd/transcript_pr_actors.png">
<p class="figure">A visualisation of transcript data showing actors (orange) where the terms "peer" and "review" are present in the data, also showing the key terms in the related questions (pink) and answers (grey) and how the actors cluster around particular sets of terms.</p>
</div>
</div>
</div>

<p>These figures show that network graph visualisations can easily indicate which subset of a large and complex set of data may be worthy of further investigation, and they show the relation of actors to certain concepts via their shared use of terms in the data. The actual information as to which person or question or answer is represented by which node on the graph is purposefully not shown, to highlight how easy it becomes to identify which actors cluster on various concepts that may be relevant to them.</p>

<p>Whilst this is a useful visualisation in its own right, it is not sufficient to advance a practice of technosocial development as it is just a way to visualise and explore data, and to make decisions based on that data that could have been made anyway by alternative forms of analysis. However, if the statements that drive the visualisations could be directly related to expressions of the practice of values by actors and their communities, then it may become a more useful tool. By capturing data on how actors localise on values and reconstruct their practices as they delocalise and relocalise, it could be possible to interact by the making of statements with the reconstructive practice, bringing the problems of delocalisation to the fore.</p>

<h4>Scholarly actor-networks</h4>

<p>As actors can be more than just individual human beings, these studies must go further than following scholars. And as it would be interesting to find examples of statements expressing practice of values, scholarly institutions and their public statements of value are suitable for further study as hybrid actors. In addition to these, some study of the communications occurring between actors of scholarly open movement networks may provide an understanding of their configuration suitable to that required for a social computing of value statements.</p>

<h5>Scholarly institutions as hybrid actors</h5>

<p>A scholarly institution is a collection of people; each person is an actor, and they form an actor-network. But also, each institution has an identity and makes statements, ostensibly on behalf of staff, but any such statement is also mediated by the process involved in producing that statement, and all such institutions have brands and media personas. it is not uncommon to find statements such as "the University of Edinburgh claims X", and also to find individuals making statements that they identify as being their own or on behalf of their institution. So there is evidence of a distinction between what is presented as the values of an institution as opposed to any individual actors. Institutions are therefore hybrid actors, and take part in the scholarly actor-network in their own right.</p>

<p>Some sort of public statement of scholarly value would be highly relevant to these studies, and would build upon the concepts presented so far, of looking for a way to visualise the data of statements of value. But as it is not so easy to ask a hybrid actor formed of an entire academic institution to take part in a study, an alternative approach was required.</p>

<p>Most universities in the UK have a mission statement, and they are very much statements about the values of scholarship. At the University of Edinburgh, our stated mission is "the creation, dissemination and curation of knowledge" <a href="http://www.ed.ac.uk/schools-departments/governance-strategic-planning/strategic-planning/vision-and-mission">#uoemission</a>. Most universities also have a web site, and on their web sites they commonly and publicly present their mission statements - presenting a perfect opportunity for further study of the values of scholarship.</p>

<p>The top ten terms in the mission statements of the 201 UK universities for which such a statement could be found are:</p>

<blockquote class="visible-print">research (85)<br>
student (66)<br>
education (59)<br>
community (52)<br>
learning (46)<br>
teaching (44)<br>
quality (44)<br>
international (39)<br>
excellence (39)<br>
knowledge (38)<br>
</blockquote>

<div class="row hidden-print">
<div class="row" style="margin-bottom:20px;">
<div class="col-md-6">
<ul>
<li>research (85)</li>
<li>student (66)</li>
<li>education (59)</li>
<li>community (52)</li>
<li>learning (46)</li>
</ul>
</div>
<div class="col-md-6">
<ul>
<li>teaching (44)</li>
<li>quality (44)</li>
<li>international (39)</li>
<li>excellence (39)</li>
<li>knowledge (38)</li>
</ul>
</div>
</div>
</div>

<p>Considering that these studies are on open scholarship, it is notable that the term "open" is only mentioned seven times across all these statements. These mission statements are products of a network of actors and are a mediated statement about the purpose of those involved in that network - about what they value. However, the statements do not change as often as the membership of the institution changes, and certainly not as often as the opinions of the members changes. So when individual actors discover new tools of digital scholarship and begin to augment their practice, their expression of value becomes out of sync with the mission statement of their community. As such statements fall further out of sync, their ongoing use becomes traditionalisation - they become statements of what the institution once stood for, rather than what it actually practices. Delocalisation naturally occurs once more.</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="/static/phd/missions.png">
<p class="figure">A network visualisation of UK universities (blue) related by the terms of their mission statements (red), and grouped by their membership of Russell Group, LERU, or neither (green). The largest grouping is of non-members, related most strongly by the terms student, education, and community. The members relate most strongly by international, research, excellence, teaching, learning, quality, and knowledge.</p>
</div>
</div>
</div>

<p>By analysing and visualising the relationships between universities and the terms of their mission statements it is possible to explore their stated shared values <a href="">#missions</a>, and to represent how those values relate to other aspects of community such as membership of the Russell Group <a href="">#russell</a> or the League of European Research Universities <a href="">#leru</a>. This demonstrates (hybrid) actors self-identifying with certain values.</p>

<p>These statements are collaboratively produced public expressions of value. If similar statements could be made in a more reflexive manner, then there is an opportunity for supporting technosocial development by modeling the relation of actors towards community values, and monitoring localisation and delocalisation in the actor-network. But to understand this activity as social computation, there remains a need for some way to understand the reliance of the actors in the network upon experts and/or key opinion makers.</p>

<h5>Value reconstruction in actor-networks</h5>

<p>Many discussions take place in and around open movements as already demonstrated in the backgrounds. These include disagreements such as the "correct" definition of open access, or which form of open license is most appropriate for academics, whether attribution is sufficient or if further non-commercial or share-alike clauses are beneficial or detrimental, whether the ability to profit from academic work (either from personal work or the work of others) is acceptable, and whether the quality of academic work is improved or degraded by changes to publication format and by differing forms of peer review. These discussions are value reconstructions occurring in actor-networks, and a few examples observed during these studies will demonstrate this further.</p>

<p>In a recent blog post Eric Van de Velde asserts that the "open-access requirement for Electronic Theses and Dissertations (ETDs) should be a no-brainer" <a href="http://scitechsociety.blogspot.co.uk/2014/03/creative-problems.html">#vandevelde</a>, and he questions why Jennifer Sinor in her own post stated that she had to argue against such open access in order to protect the work of her students <a href="http://chronicle.com/article/One-Size-Doesn-t-Fit-All-in/145505/">#sinor</a>. In this disagreement the value (access to scholarly works) is articulated in virtually opposite ways; for Van de Velde, access to the work is a fundamental requirement of scholarship, whereas for Sinor what is most important is that the scholarly work is protected. The difference turns on the assumption that works of art (the works of the students of Sinor) are different from works of science, in that scientific works exist only to <i>communicate</i> value whereas it is the content of an art work itself that has value, and so mandating open access diminishes the potential of artists to profit from their works if they are made openly available when submitted as university course work. Further points are referenced by Van de Velde asserting that open access does not reduce publishing opportunities <a href="">#oaetd</a>, and he goes on to argue that the difference between science and art is not as great as Sinor claims, and that open access policies do not cause the situations Sinor describes.</p>

<p>It is clear from the discussion that neither Sinor nor her student were initially aware that their university operated an open access repository and had dissemination policies that the student had agreed to in order to receive supervision by Sinor. They were also unaware that publication under the open access policy of their university would allow others to publish the work of the student for profit and without consent. This highlights an interesting problem for the assertion of scholarly values - they represent a large community, many of whom perform different sorts of work and may be completely unaware (whether they should be or not) of the meaning of certain constructions of seemingly similar values. Similar examples can be found in the work of Heather Morrison, in which she argues that open access is beneficial to scholarship yet also that publishing scholarly works without a non-commercial clause is detrimental to scholarship <a href="">#morrison</a>.</p>

<p>These examples highlight occurrence of a delocalisation, around which value reconstruction must work hard to overcome. Unrestricted open access offers no control over what can be done with a work, whereas scholars often wish to have unrestricted access to what they need but also to control what others are allowed to do with the works they create. This may be the "hard problem of open access" - is it possible to have such a thing whilst also mandating the ways in which the work can be used? As it is not possible to predict and account for all possible uses, there can be no definitive answer. Such a problem is classically insoluble but despite this a value reconstruction <i>will</i> occur, and judgements will be made. The problem will be <i>solved</i> by a form of consensus, and it will stand until it becomes untenable.</p>

<p>During the final talks of the CERN Workshop on Innovations in Scholarly Communication (OAI8) <a href="">#oai8</a> in Geneva in 2013, this was well demonstrated when Bram Luyten asked a question to the panel during the last plenary session about open access infrastructure:</p>

<blockquote>Would you agree that one of the main cost components of the service a publisher provides resulting in APCs or Gold OA fees is the organisation of pre-publication peer review?<br><br>
(The panel indicates general agreement)<br><br>
So do we as a community really believe in the future of pre-publication peer review? Do researchers have the time to do more reviewing in a kind of closed process before everything becomes open, or should we try to acknowledge that the long term future is just that the stuff will be open on the web and somehow people will eliminate the bad stuff, promote the good stuff, and then publishers who have a brand like Nature will try to find these things and then just group the really good stuff together?<br>
<small>OAI8 Plenary session on the topic of infrastructure (1:03:53) -  <a href="">#oai8plenary</a></small>
</blockquote>

<p>The panel goes on to express vastly alternate values, such as one claiming that academic work without proper peer review should never be made available whereas another considers peer review to be a sacred cow. In the closing talk of the conference, Professor Deketelaere states his surprise that peer review was not viewed as a much greater problem during preceding talks, given that the various examples he sees of plagiarism, withdrawals, and retractions lead him to conclude that there is in fact something wrong with peer review (30:55) <a href="">#oai8keynote</a>.</p>

<p>There is now a well established and highly profitable market <a href="http://www.standard.co.uk/business/markets/market-roundup-reed-elsevier-sparks-interest-with-talk-of-a-1-billion-payout-8732014.html">#publisherprofit</a> in (controlling) the dissemination of scholarly material, and scholarly output has been assigned commercial value and has thus been productised, and the scale of institutional publishing costs is now estimated to be in the billions <a href="">#bjorn_slides</a> <a href="">#bjorn_post</a> <a href="">#poynder_springer</a>. But now that the web could enable better and cheaper publishing when compared to the very high costs still charged by traditional incumbents, protests such as the Cost of Knowledge campaign <a href="">#costofknowledge</a> occur, and new tools and practices of peer review begin to develop <a href="">#peerage</a> <a href="">#rubriq</a>.</p>

<p>The practice of the values of scholarship is now an act that involves scholars, institutions, publishers, legal experts, funding councils, and many more actors besides. Peer review is an exemplar practice that was clearly highly valued by the scholars followed in earlier studies of this thesis, and it has also clearly become the focus of great debate within the scholarly community. Developing new tools on the traditional understanding of peer review may result in mandating a tool that only processes peer reviewed material, whereas developing on the expressed value of peer review as a rewarding practice in itself may qualitatively improve the practice of peer review.</p>

<p>Whilst calculating which development is the best for the future of a given community is one of those insoluble problems, enabling a community to collaboratively choose which development to try next is no longer so far from reach, and such momentous value reconstructions may become easier to transcend.</p>

<h5>Socially computing the value of practice</h5>

<p>Some attempts at not only mandating certain sorts of community practice but of reflection upon the actual practices of the community have already begun, and these already show signs of socially computing the value of practice, in that the collaborative efforts of a community to reflect on practices has informed further statements on expected practice.</p>

<p>A recent example of such achievements comes from the OKF open access working group, to which Theo Andrew of the University of Edinburgh Library released a collaboratively editable spreadsheet of Wellcome Trust Article Processing Charges (APCs) for the purpose of collecting further data about the (monetary) value of such charges. By crowdsourcing the information, the working group was able to identify the split between hybrid and fully open access publications, the licensing conditions under which they were made available, and various statistics about which publishers provided a greater or lesser degree of accessibility for the costs incurred by the APCs. The work done by the community on improving this dataset led Wellcome to publish the following statement in a blog post thanking the community, detailing some of the findings, and reiterating the value of the practice expected:</p>

<blockquote>We expect <i>every</i> publisher who levies an open access fee to provide a first class service to our researchers and their institutions. We recognise that subscription-based publishers are actively developing their systems to accommodate the open access business model and we urge them to makes these changes as quickly as possible. Even though there are only a small number of articles that the Wellcome Trust has paid to be open access that have remained
behind a pay-wall, this is not an acceptable situation in any instance.<br>
<small>Wellcome Trust <a href="">#wellcomeapc</a></small>
</blockquote>

<p>This is an assertion of values stated by the Trust on behalf of a wider community, made possible by the collaborative efforts of that community, and without recourse to standardisation or classification (although making use of such technical standards within the solution, such as Digital Object Identifiers). The collaboratively editable dataset remains publicly available <a href="">#wellcomegs</a>, and can continue to be edited to drive new statements in future. This scholarly society is beginning to socially compute its own values of practice, and to reflexively augment them as new information becomes available. For interest, another interactive network visualisation was created from the data <a href="">#wellcomevis</a>, and this was shared with the working group and was useful in helping them to explore and understand the data and the relationships of actors to values by their observable practices.</p>

<h5>Configuring actor-networks for social computing</h5>

<p>The traditional approach to social computing states that people taking part in the computation must be independent of the opinions of those around them, and that too much communication in the group can make it less intelligent - and yet also that people must have access to local knowledge and specialisation, and that the right information must be available to the right people in the right place and time. This is problematic, if the studies of this thesis so far are anything to go by, because people (actors) gain knowledge by communication, and judge the value of it by the locality in which they encounter it, and how it is shared with them, and they will react to the community around them based on their personal judgement of the material to hand. So it seems that configuring a social network for compuation would denature it, making it just another artifice, fitting to an elite understanding of intellect and the artificiality of its manifestation in non-human entities.</p>

<p>To explore the possibility of comprehending the balance of an actor-network so that it may be capable of social computing, the application of the same visualisation tools already shown in earlier studies was developed further <a href="">#emails</a>. By processing the publicly available archives of the OKF open access, open science, and open bibliography working groups, a network visualisation of the connections between various actors was created to show how closely they relate to particular values, organisations, and other actors. Whilst this is purely a demonstration on data that was not specifically designed for the purposes, it does indicate the potential for identifying influential actors and contentious issues by appeal to their context rather than to external validation. This may allow for adjustment of the value statements made by such an actor-network in a social computation, rather than requiring the artificiality of authoritative configuration. From the perspective of ethnographic study, this is akin to the problem of directing survey respondents by the setting of certain types of question, which could perhaps be avoided by instead providing such a tool to a group under study and enabling them to share values and to form their own questions and orient themselves around them.</p>

<p>In the following figure, for example, the most influential actor between the three working groups can be easily identified, along with an estimation of influence compared to other actors within the same network (in this case, approximately seven times that of the next most active actor). The main terms of the communications this and other actors were involved in have been extracted, and can also be selected in the interactive visualisation to show which terms the actors relate on. So an estimation could be made about which values certain actors can influence, and this can be taken into consideration. Whilst it is not clear what sort of balance could be achieved, the key point is that it can be made obvious to collaborators in such an actor-network, if they were to use such a tool to examine their own community, and any actor interacting with the tool could augment their own judgements with the knowledge of which other actors that they may be in contact with have particular influence on a given value. This may be a first step in collaborative self-configuration of actor-networks for social computing.</p>

<p>If such a visualisation tool were used to collect specific data about value statements and orientations towards such statements of actors in a community, then reflexivity may be possible, perhaps allowing for the use of such data and tools to guide technosocial developments. By engaging actors in such a collaboration, the problems of delocalisation may be more easily exposed as all actors can become aware of and have an impact upon the configuration of their community network.</p>

<p class="hidden-print">To demonstrate further the abilities of the visualisation tool that has been developed through these studies, an interactive version is directly embedded within this document, and preset to show the relation of the aforementioned influential actor to the three working groups:</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<div id="listsgraph" style="height:700px;" class="hidden-print"></div>
<img src="/static/phd/lists.png" style="width:100%;" class="visible-print">
<p class="figure">A visualisation of the influence of actors upon working group community networks, based on the public archives of the working group email lists.<span class="hidden-print"> This visualisation is interactive, and it is possible to also explore the most popular terms in the conversations of actors.</span></p>
</div>
</div>
</div>

<p><hr class="embossed"></hr></p>
<h2>Outcomes</h2>

<blockquote>My propositions serve as elucidations in the following way: anyone who understands me eventually recognizes them as nonsensical, when he has used them - as steps - to climb up beyond them<br>
<small>proposition 6.54 of Wittgenstein's Tractatus Logico-Philosophicus <a href="">#tractatus</a></small>
</blockquote>

<p>The stated aim of this thesis was to advance a practice of technosocial development, to acknowledge problems of delocalisation by bringing them to the fore, and to provide conclusions of relevance to the four target audiences. Delocalisations were expected to occur naturally because localisation does, in the forming of communities around practices that express their values. So studies were proposed in the places where such delocalisations might be observed, where software developments create tools that enable new practices of digital scholarship, and where actors delocalise from their community by calling shared values into question as they reconstruct value practices. With the help of actor-network theory and valuation study, and mindful of the philosophy of open society, a reconstruction of the values of social computing delivers a tool for supporting technosocial development.</p>

<div class="row">
<div class="col-md-12">
<div class="thumbnail">
<img src="https://docs.google.com/drawings/d/1OUipw0q3gaiS2x5ZxufWKjtvxeijyZu0P9KQPjt58Pk/pub?w=813&amp;h=589">
<p class="figure">The technosocial development lifecycle. This presents community and development as integral, with no distinction between the two. The lifecycle is circular, so whilst any particular development project may be deemed to have a beginning or an end, it should be understood and evaluated as situated within the technosocial environment. Technosocial development processes can evolve to keep this in mind.</p>
</div>
</div>
</div>

<p>The studies in software development showed firstly that LEAPS acted to reconstruct values around the introduction of new technology via social encounters and collaborative engagement rather than authoritative introduction of new tools and practices in their community, indicating that technical development alone would delocalise them to a greater degree, whereas their acknowledgement of actor-network mitigated this. Then the Open Biblio project was observed demonstrating applicability of new technologies to community values, and engaging the community in collaboratively stating principles of value to advocate reconstruction of practice. This was followed by the Open Citations project exploring large citation datasets and seeking answers to problem statements with the application of data analysis and visualisation that highlighted relations between nodes in a scholarly community network. Finally G4HE showed further value of visualisation, of the network of scholarly works and actors related by research funding. A problem of delocalisation arose when the project attemtped to locally define terms and authoritatively export them. If the terms had been collaboratively created, the developments of the project may have been more easily accepted by the user community. From these studies it was demonstrated that small local technical developments are perhaps better placed to overcome delocalisation (or to suffer it less in the first place) and that as projects increase in scale such problems become harder to avoid, and the naive solution is easier to implement. If the collaborative reconstruction of smaller projects could be extended, delocalisation may be exposed on the larger scale. These studies also showed that it can be hard to export meaning from one locale to another, and that meaning can be down to the judgement of the consumer, so approaches that engage and support actors in that judging act of valuation may help make an unavoidable delocalisation less traumatic to the activity of development.</p>

<p>The studies in open scholarship showed that ethnography from the perspective of the subject of study relates well to software development user stories and advances such as co-realisation, but that it remains difficult to probe communities without also having an impact on their practices. There was evidence of a concern of black boxes obscuring true understanding in previous science studies described in the backgrounds, and of similar concerns for the approach of actor-network theory utilised in the studies of this thesis. However, these studies show actors taking part in a reconstruction of values, exploring and utilising new tools and practices, and incorporating them into their community - it seems that the black box may manifest itself <i>in</i> the practice of ethnography, as our understanding of what it is to know, to explain, and to be intelligent expects something more than just observation of value judgement. But such judgement is a valuable computation in itself, and the development of tools and practices that acknowledge that value can bring new understanding. Hybrid actors already make many statements that are judged and valued by community members, upon which important decisions are made, and the machinations of such decisions and the influence of different actors upon their community can also be discerned by analysis of the communications of an actor-network. Acknowledging the dynamics of localisation and delocalisation that occur in value reconstruction allows for an understanding of the emergent configuration of an actor-network, which supports an augmented approach to social computing that can be more readily applied in the natural world.</p>

<p>The research of this thesis was to be considered successful if the studies could generate outcomes that would be useful to future technosocial (rather than social, or technical, or software) development projects. This research has shown that open scholarship can be interpreted as a form of value reconstruction, where natural localisation and delocalisation takes place as actors reconstruct value practices; they do this as digital scholarship has presented them with new tools, but not necessarily equipped their communities to accept them into their scholarly tradition; and that localisation on practices of software development in what should be technosocial development projects can aggravate delocalisation. Now that this thesis has localised itself upon these backgrounds and studies, the reconstructive statement of these outcomes is to delocalise these findings into generalisations, and to present them for consideration by the wider community: open movements are value reconstructions, value reconstruction is technosocial development, and actor-networks are social computers.</p>

<h3>Open movements are value reconstructions</h3>

<p>Open movements are not a naive battle of good versus evil, or right and wrong, or even open versus closed. They signify a natural practice of healthy communities to reconstruct their traditions as new environments are encountered. This is only good if progress itself is inherently good - but it is not; progress can have subjectively positive or negative outcomes, and is at best incremental and testable; progress is however natural, as it is a form of change, of which all existent things partake. So in generalising (delocalising) the findings of this research in open scholarship to open movements in communities in general, the risk is only in assuming that such communities are natural and healthy. Where this is the case, value reconstructions occur and are advanced by both the opponents and proponents of an open movement.</p>

<p>A question might be raised as to whether it is a good idea to allow such open movements to take place - that perhaps it is a case of if something is not broken then it should not be meddled with. However, holding any community to such a rigid unchanging practice would be to shun the many interactions that actors in communities take part in, and to curtail their natural discovery of new tools, alternative practices, and additional collaborators. The simple fact is that value reconstruction is natural and already occurring, so to ignore it would be a far greater risk. It was reported anecdotally during the course of this research (but not followed up for inclusion in the studies) by numerous medical professionals that they regularly refer to Wikipedia in making decisions about patient treatment, so in some cases our lives are already depending on these new practices, whether we like it or not.</p>

<h4>The studied reconstruction of peer review</h4>

<p>Consider the practice of peer review in open scholarship, traditionally portrayed as being a question of whether a research article is suitable for progress from pre-publication to publication. Such a portrayal aligns current publication practice with the value of peer review, however it can be shown to be separate and only <i>coincidental</i> with the journal publication tradition. But scholars change their practice with digital scholarship, delocalising from their scholarly community, and <i>not</i> performing peer review as an evaluative step from pre-publication to publication. Instead, they express their value of peer review as a communal activity of scholarship, and they collaborate as widely as tools will allow.</p>

<p>In every case, the scholars studied in the research of this thesis made their own value judgements and used a combination of technologies - not one of which was specifically designed for the purpose - in order to seek out an accessible source for the information they sought, and to circumvent the intended function of those various technologies if necessary. Each actor questioned the practice of values instantiated in the technologies available to them, and in the local practices of their communities, and asserted an alternative. In many cases the steps required to gain access to any particular piece of information are exceedingly simple - find any other person with access, ask them for a copy, and repeat until successful. This can be achieved by asking at a meeting of colleagues (using the technology of built communal spaces) or by tweeting #icanhazpdf <a href="">#icanhazpdf</a>, and many other ways. Regardless of whether technologies were built to increase access (such as institutional repositories) or to restrict access (such as journal paywalls), they were used within an actor-network of other tools and actors to <i>gain</i> access.</p>

<p>From the traditional perspective, it may seem that a new software development in digital scholarship should build a tool that proves an article has been peer reviewed, or that only allows access to peer reviewed articles. But whilst peer review was considered valuable for identifying quality articles in some cases and via certain access routes, it was foregone in others where alternative value judgements could be made. And regardless of whether a work was peer reviewed or not, it was usually checked for quality by the consumer anyway, once access had been established. So instead, what is required (and what tools actually get used for) is a technosocial development that supports collaboration on opinions of quality of work that allows a consumer to judge quality for themselves. This is essentially still a peer review system development, but with project goals that include the reconstruction of practice in shifting judgement towards the consumer. Recommendations as to how best to transform peer review <a href="">#tpr</a> and perform <a href="">#shotton</a> <a href="">#oaspa</a> in an online journal marketplace are already occurring, and it seems that performing a valuation study would be a valuable practice for technosocial development, because technosocial developers must reconstruct values instead of constructing what value practices appear to dictate.</p>

<h4>This thesis reconstructs values</h4>

<p>This thesis is a practice of value reconstruction, starting out in an interdisciplinary research group rather than a traditional subject orientation, and attempting to perform a traditional practice of attaining a PhD whilst making use of some of the latest tools of digital scholarship. It was written on a collaboratively editable web page using etherpad <a href="">#etherpad</a> which implements an operational transform algorithm <a href="">#ot</a>, saving every state change of the document since inception (there are now almost 200,000 retrievable versions). Any one of these versions (usually the most recent) can be "published", shared publicly for review. With HTML and CSS the style and layout can be carefully controlled, and customised for display on different devices as well as optimised for PDF or print versions if desired. The public web document has built-in annotation tools, so that sections of text can be highlighted and commented by supervisors and reviewers. It is also possible to embed the videos created during the studies, and to use the d3 visualisation library <a href="">#d3paper</a> to create interactive visualisations of the research data. The data itself - including the text of the document and the annotations - is stored in an indexing system <a href="">#ES</a>, allowing for very fast and powerful searching across all content using algorithms such as levenshtein distance <a href="">#levenshtein</a>, and automated text mining and keyword analysis. All the data is backed up to multiple locations automatically, reducing risk of loss, and the metadata of the document includes machine readable statements of copy and re-use rights, and the author is uniquely identified with an ORCID. The metadata also includes web analytics code, providing statistics about the popularity of the published document, which could also be tracked with alt-metrics services such as Impact Story <a href="">#impactstory</a>.</p>

<p>This was all built using open source software at no cost, and the software developed during the research is in a version controlled public online repository <a href="">#phdrepo</a>. This custom software adds further capabilities such as API endpoints on all of the content, keyword query streams, and automated reference management including numbering and embedding, along with header numbering and contents lists generation. More can be achieved such as automated deposit to the institutional repository via SWORD <a href="">#sword2</a> <a href="">#sword2profile</a>, and automated Wikipedia linking and research topic suggestion. The concerns of Harvard about the use of Wikipedia can thus be mitigated by making reference to Wikipedia data clear, and storing a copy of the Wikipedia page (and the pages it references) at the time of reference if desired. As already mentioned, all Wikipedia pages also include edit histories, and the Internet Archive stores copies of web pages as they existed at given points in time, so there is already technology to take care of this.</p>

<p>In order to submit this thesis, it must be printed out. This tradition delocalises the thesis from its natural setting as a digital interactive document, and so in the traditional context the novel practices it employs may seem like failures to meet standards. But, if the scholarly values are to be reconstructed, then this experiment must be performed. In its true form as a digital document, this thesis supports the same values of scholarship that the traditions formed around - namely, it ensures that supporting materials are easily discoverable, that the data is available for critique, and that the referred material will be up to date with the latest changes, alterations, and retractions - whilst still being capable of referring to material as it stood at the time of "publication", if so desired. However, the transition to stability from editability is not a technical matter, but a matter of the value placed upon reference to supporting material - they are just as capable of changing for the better as they are for the worse.</p>

<p>Following the traditional practice will make it harder to perform, scrutinise, and re-use this work. It is time for reconstruction of this practice of value - and clearly, given the wealth of technical capability available and employed during this work, this is no longer a task of technical development, but of technosocial development - the practice and makeup of the scholarly community is changing.</p>

<h3>Value reconstruction is technosocial development</h3>

<h4>The localisation of software development</h4>

<p>The practices of software development such as requirements gathering, specification, user stories, and co-realisation are localising, so there is an inherent tendency for them to trigger problems of delocalisation.</p>

<p>When a librarian requires a signature in order to issue a copy of a book, there is a good reason. When the librarian is asked to list the requirements for issuing a book, having it signed for is one of them. When developing a technological improvement to library services, for example an online service to access digital copies, it would therefore seem appropriate to require identification of the person accessing those copies. When collecting requirements, or user stories, or use cases, such errors are possible. Of course, some requirements analysis and the previous experience of a good software developer may highlight these as opportunities to change practices. But this is where the practices of software engineering end, and where acknowledging value reconstruction offers most promise.</p>

<p>Consider the Open Biblio project - ostensibly software development projects that in fact sought to do more than develop software - they aimed to augment community practices by appeal to technological demonstrations and community endorsement of collaboratively formed and publicly stated principles. The software development was a technological enabler, and the production of standards such as bibJSON and of statements such as the Open Bibliographic Principles were social enablers where technology meets the user community, as had been suggested in background research.</p>

<p>But for the technosocial developer there is more work to be done, to reconstruct communities of practice.</p>

<h4>Delocalising into the technosocial</h4>

<p>The seeming necessities of signatures on paper, of restricted access to fragile resources, of hierarchies of authority, are not just requirements elicited from user stories to be re-instantiated in efficient, quicker, cheaper, form - they are opportunities to reconstruct value practices; to have digital copies available to everyone, to be able to keep originals safe whilst sharing copies, to remove the need to track locations of copies, to simplify permission requests. This does not mean change for the sake of change, or change regardless of value. The outcome is not idealistic or anarchic or revolutionary but incremental. In order to achieve some form of responsible incremental technosocial engineering, new practices, involving new tools and new techniques, are required. The traditions of software development, and of ethnographic study, must advance - we must delocalise ourselves from such narrow disciplines, and complete a process of reconstruction of practice.</p>

<p>Technosocial development projects are a natural extension to software development via actor-network theory, and as something beyond software engineering practices they require specific consideration and support.</p>

<p>In every case, the actors of the studies of this thesis demonstrate an intent to find what they believe may be useful <i>and then</i> to make a judgement as to how to value what they found. Of course this judgement is based on their knowledge, expertise, and community, and there is occasional use of traditional "totems" - when researching a field of which one has little experience, greater faith may be placed in the high quality journal article, for example, or a selection of traditional practices to follow when one is uncertain. Such totems fill gaps and allow valuation to continue, until such time as they can be thrown away, and it is evident from our history and superstitions that the power of totems to enable judgement is great.</p>

<p>The occasion to make judgements, for valuation judgements to be made beyond the "certainties" and practices of any one discipline, enters the realm of valuation studies, where valuations are made in excess of, in addition to, or despite what is already valued. This is the constructed value of a thing, and it emerges from the judgements of actors to put things to use. Judgement is powerful, and from it beliefs and traditions emerge - but its power is in a black box. This is where the black box that the sociologists were concerned with is located - in the judgements made by actors, that are made sensible in their actor-network, but otherwise appear as something else; from a historical perspective, they are paradigm shifts; from a moral perspective, they are the problem of relativism; from a modern perspective, they are the problem of privilege.</p>

<p>Performing these valuations, and reconstructing values and practices, is what an actor-network (a society) does. Opening up that black box, exposing the workings to find nothing but our own judgements, is our delocalisation problem. The cost is the tradition of explanatory power, but if we can complete this reconstruction we may find a new power.</p>

<h3>Actor-networks are social computers</h3>

<p>Judgement without explanatory power seems old-fashioned, like following totem poles. However judgement need not become powerless; instead, the understanding of judgement can be reconstructed, and empowered once more.</p>

<h4>Reconstructing social computing</h4>

<p>New ways of calculating results to questions are often proposed, and one such recent proposal was social computing. The studies of this thesis have demonstrated ways in which actor-networks already appear to perform social computations, for example in the collaborative creation of principles and mission statements. But these actor-networks do not meet the traditional configuration requirements of social computing. So how can the judgement of such an actor-network social computation be valued?</p>

<p>An interesting point of note is that to some extent it does not matter what is claimed as being necessary for treating such outputs as valuable, because value emerges in use - and use is <i>already</i> occurring. Doctors are already using Wikipedia, so our lives on occasion already depend on what Wikipedia says. So it is better to learn more about how these value judgements <i>are</i> made than to criticise them, or to try to enforce formality on what is already naturally occurring.</p>

<p>One form of social computing is to have simple problems with externally verifiable answers distributed to many individuals - the typical "how much does X weigh" scenario. But this setup relies on a boundary between what is factual - the weight of X - and what is valued - the estimates given by actors. Whilst this form of social computing may be useful, there is no need to rely on such a boundary - the values of the individuals are useful enough, without external verification. In fact, if it is possible to calculate such values by social computing, it would be an opportunity to learn more than what is valued. The configurational features of social computing that must be reconstructed are these:</p>

<ul>
<li>Independence: People's opinions aren't determined by the opinions of those around them.</li>
<li>Too much communication can make the group as a whole less intelligent.</li>
</ul>

<p>People's opinions <i>are</i> at least influenced (if not determined) by those around them, and people in groups <i>do</i> communicate. However, it is not necessary to configure these out - instead, they can be accepted as natural parts of the actor-network, and taken into consideration when utilising the output of such a social computer. All that is required is the ability to expose how actors influence each other, and to what extent actors are communicating with others in the network. This sort of exposure has already been demonstrated.</p>

<p>Consider the problem of sentiment analysis in the Open Citations project: computing the intended sentiment of an actor in a statement they made in the past using language that did not properly communicate their sentiment is hard for another human being, let alone a machine. It is possible to formalise a method for humans to indicate sentiment in the citations they make, but this relies on them doing so. Instead, if their citations <i>and</i> sentiments towards given works are all available for perusal, the consumer of the citations can also gain an understanding of the intended sentiment of the producer.</p>

<p>The simple act of exposing such detail is not enough to resolve moral issues, however. For example, if the sentiment of a producer of citations is wrong, the use of the citation may still be improperly inherited by the consumer. Similarly, if a contributor submits intentionally false or accidentally inaccurate data to a communication, it may still go unnoticed even if exposed. So, practices must continue to take account of such issues, where they are important to practitioners. However, exposure does allow for a wider range of judgement to be made by a consumer, and for any consumer to consider what is appropriate information for them to value given their particular practices and values.</p>

<p>From the studies the increasing application of visualisations also became clear. Much like the controversy mapping of ANT, visualisations are a useful way to simply portray complex situations. In addition to that, as with subitization (and to an extent in totemisation), visualisations are a useful tool for exploring without highly accurate representation, but with enough to support value judgement.</p>

<p>The two configurations of social computing could therefore be reconstructed into three, as follows:</p>

<ul>
<li>Dependence: actors are part of networks and communities of practice, upon which their opinion partly depends. This must be acknowledged.</li>
<li>Communication is what makes a group intelligent. Exposing the communications of an actor-network allows influence to be accounted for.</li>
<li>Representation of the actor-network therefore allows actors to perform reflexive judgement on their interactions.</li>
</ul>

<p>A tool for representing the actor-network is called for.<p>



<h4>Constructing a tool for practice</h4>

<p>When making valuative judgements actors often represent situations and justify decisions based on "sensible" representations. Actors are capable of self-identifying and self-organising, as well as posing their own value statements and making their own value judgements in response. They are equally capable of changing those statements and responses, and they are able to inter-relate both the values and other actors.</p>

<p>A simple tool, supported in implementation by user accounts, authorisation, and authentication, is sufficient to model relationships in rich actor-networks. For example, any user - be that a human being or a machine - can be represented as a node. Statements can then be made, either about nodes or values or otherwise, and any node can make response statements about other nodes, statements, values, etc. Thus, the only arising definition of "actor" is "a node that interacts via statements", and the relationships between actors are represented as mediated by certain connections characterised by statements - the ones with which those actors interacted. If those statements were "about" a node, then the relationship of actors to that node is also mapped. By implementing versioning and timestamping on statement and response submissions, it further becomes possible to map change over time. (See Appendix A for a copy of the README overview of the software demonstration that accompanies this thesis.)</p>

<p>Further development could support tracking which users (actors) viewed additional statement information, and also whether or not users viewed the network visualisation, before responding to any particular statement; and as users self-identify themselves and their relationships by their interactions (by showing which statements they have an interest in), this along with an understanding of their desire to seek supporting information or community awareness prior to submitting their responses can be used to add weighting to their answers - or to that of those influenced by them. Similarly, an understanding of how statements and responses from particular users change over time will allow for further characterisation of users. Also, users could submit supporting information about themselves which could include forms of validation.</p>

<p>These functions would allow, for example, for the following scenario:</p>

<ul>
<li>A set of statements about a particular entity (a research article, for example) could be made, tagged with relevant terms, and terms additionally extracted automatically</li>
<li>Users with interest in those terms could be notified by a subscription implementation, and could submit responses</li>
<li>User 2 may be such a user, and user 2 may also be "validated" for example by having a UK university email address or an ORCID</li>
<li>Subsequently, user 3 could search for users linked to statements with certain terms</li>
<li>If user 3 sought only statements from "validated" users, user 3 could further filter their result set</li>
<li>However users 4 and 5 could still search for any statement without filter</li>
<li>As users become accustomed to finding relevant statements by their preferred criteria, they could share those criteria with others as a set of "values"</li>
<li>User values - the statements they submit, the responses they make, the searches they perform, and how those all change over time - could all be tracked</li>
<li>Thus allowing for analysis and feedback for the purpose of valuation studies into issues such as software development requirements, or group polarisation <a href="">#wppolar</a>, for example</li>
</ul>

<p>Undoubtedly there are limitations to this - in particular, it only maps actors that use it. However, this is the case with all tools, and whilst it may be that certain values of relevance may be missed out in particular situations, it is still possible overall for such a service to be of value. The aim of this tool is not that it could be used to predict or manipulate any given actor-network or society at large, but to define a tool for use in certain situations, capable of being put to various forms of use - many of which, like the tools built before it, are probably beyond what can currently be thought of.</p>

<p>A demonstration has been made available to coincide with submission of this thesis <a href="">#mylv</a>, and in addition the software is available in an open source repository <a href="">#mylvrepo</a>. It has been called Leviathan. The demonstration can automatically visualise actors that use it, and their interactions with statements and responses, as well as automatically generating groups to represent hybrid entities - for example, all responses received from a given IP range could be used to characterise a grouping of responses as being the disposition of the University of Edinburgh. This tool could be used in specific settings such as in software developments, or in ethnographic studies of communities, where instead of the developer setting requirements or sociologist setting questions the actors and developers/sociologists could collaborate by setting and responding to statements, and adjusting depending on both responses and influence of respondents. (Whilst it cannot be guaranteed that the demonstration remains available, the software is also included with the deposit of this thesis.)</p>






<hr class="embossed"></hr>

<h2>Conclusions</h2>

<p>In this thesis a number of examples of delocalisation have been provided, both from the perspective of tool developments and of community actors. This has shown that any tool that is to be used by a community of users is likely to trigger a reconstruction of values to some degree, and that it is when delocalisation is overlooked that reconstruction becomes more difficult, because it is that much harder to reform around or without the delocalised entities. However with actor-network theory and collaborative development practices an alternative understanding of the technosocial development environment can be gained, and with an augmented form of social computing it is possible to begin considering an actor-network and its reconstructions as reflexive.</p>

<p>This thesis has demonstrated the sorts of tool that may potentially be useful for supporting a practice of technosocial development, by making use of visualisation libraries on top of modern methods of handling large amounts of unstructured data. These sorts of tool have also been put to use in the creation of this thesis itself, so advancing technosocial development and reconstructing scholarly values is what this thesis does. The main outcomes - open movements are value reconstructions, value reconstruction is technosocial development, actor-networks are social computers - have particular relevance to each of the four target audiences of software developers, stakeholders in developments, social scientists, and digital scholars. From the new opportunities these outcomes pose, there is also some scope for future work emerging, and a longer term philosophical issue to contemplate.</p>


<h3>For software developers</h3>

<p>For software development, Leviathan is likely to be a useful tool for eliciting and visualising requirements and issues. If used collaboratively, this could support co-realisation with social translucence, by allowing all actors involved in a development to understand and contribute to reconstruction of community values as development occurs. It has perhaps also been possible to defend software development against claims of lack of radical novelty - there are some radically novel practices proposed in this thesis. Whether or not this goes beyond software development rather than reconstructing development itself remains to be seen - will developers come to value the technosocial?</p>

<ul>
<li>Open movements are value reconstructions - software developers in such cases are technosocial developers, not just software developers. Their skillset and development scope must scale up to take account of this.</li>
<li>Value reconstruction is technosocial developmemt - there are already natural processes in place that the technosocial developer can work with. Understanding and promoting the creation of a tool as the reconstruction of a community of practice will shift the development problem space away from the technical, incentivising a larger audience to seek project success.</li>
<li>Actor-networks are social computers - localisation and delocalisation solutions are best delivered by the actor-network from which they emerge. Opening up the technosocial development stakeholder community to this, in a suitably observable manner, can provide the computability required to complete the technosocial development project.</li>
</ul>


<h3>For stakeholders in developments</h3>

<p>During this thesis there have already been some direct involvements with open movements, as the development projects were all open source projects creating tools at the request of projects that were themselves part of open movements. The software developed during this thesis may prove useful to others, and some services produced in collaboration with supportive organisations are already in use. Useful places to find more related software are the Cottage Labs and OKF accounts on github <a href="">#gitcl</a> <a href="">#gitokfn</a>. Also, open movements may defend against the perception that they are performed by people seeking only to further their own interests or to cause unnecessary disruption; instead, it is now clearer that they are signifiers of progress and of healthy reconstructions of value within and around communities of practice. To advocate for or against the ethos of any particular open movement is to take part in it, and to assist in advancing practice - a necessary function of a healthy open society.</p>

<ul>
<li>Open movements are value reconstructions - stakeholders in such movements cannot avoid the reconstructive consequences of their actions, and can instead justify their disruptive behaviour as valuable.</li>
<li>Value reconstruction is technosocial developmemt - an agreement upon value reconstruction must be met, but it cannot be mandated; it must be developed.</li>
<li>Actor-networks are social computers - with suitable configuration, the actor-network itself can provide greater insight into the reconstruction in progress.</li>
</ul>


<h3>For social scientists</h3>

<p>This thesis provides for social scientists a demonstration of an alternative application of actor-network theory to the studies of software development projects. Also, the Leviathan is a potentially useful new tool for ethnographic practice, capable of supporting an undirected survey, enabling actors to self-examine with the setting of and interaction with statements of value. This may be particularly useful to the new research area of valuation studies, where the network graph visualisations may assist in studying the values of actor-networks. If further application of technology can help to make actor-network theory more than a theory or didactic tool, perhaps the fundamentals of the formation of actor-networks can become better understood.</p>

<ul>
<li>Open movements are value reconstructions - these are good examples, and perhaps a useful way of describing certain situations, where social scientists may wish to perform further studies; they are societally interesting.</li>
<li>Value reconstruction is technosocial developmemt - the social scientist can observe such developments via the lens of valuation studies. This new research field contributes a useful meta-study approach to social science.</li>
<li>Actor-networks are social computers - use of more advanced technology to observe actor-networks can give further insight into the workings of a community. Social scientists may soon become equipped with a tool as major as the invention of telescopes to astronomers, and we may learn more about community structures and how they emerge.</li>
</ul>


<h3>For digital scholars</h3>

<p>All the software created for this thesis is publicly available under an open source licence, and there may be some useful tools there for others wishing to create and publish their own research documents in more interesting and collaborative ways. The studies, particularly the video surveys of scholars, show the commonalities across all sorts of scholar when faced with judging which source materials to value. Whilst these problems were not and cannot be solved in this thesis, bringing them to the fore is enough to prove that such issues emerge in many different communities, and that digital scholarship tests potentially useful evolutions of practice. There is a great deal more for the digital scholar to do, however, and this is perhaps where the greatest potential for future work lies.</p>

<ul>
<li>Open movements are value reconstructions - digital scholars are at the forefront of an open movement, where the meaning and value of scholarship is changing. It is OK to be here, you are not alone.</li>
<li>Value reconstruction is technosocial developmemt - and societies thrive on scholarly work, learning more, developing new theories, making small changes to past ideas.</li>
<li>Actor-networks are social computers - in addition to augmenting traditional research, becoming a digital scholar is to take part in the research of how scholarship is done. Nobody holds the right answer - there isn't one; but it will emerge.</li>
</ul>


<h3>Future work</h3>

<p>The Leviathan is soon to be put to use in collaboration with the ContentMine project <a href="">#cm</a> which is funded by the Shuttleworth Foundation <a href="">#shuttleworth</a>. Project workshop attendees have stated an interest in using the tools to analyse published research in their field, to identify how much work has been published about particular genes for example, and to use the visualisations to highlight genes that are highly relevant to numerous studies but are themselves lacking in direct research. It is hoped this approach will assist in securing research funding, as it overcomes the typical problem experienced by the attendees that funders prefer to fund the more common topics, by showing in fact that there are areas lacking research but where more effort must be made.</p>

<p>Through this and other potential collaborations, the Leviathan tools will be developed further and the extent to which it can be configured to provide valuable information will be explored. Collaborations will be sought with projects requiring ethnographic studies where Leviathan may serve as a survey aid, and also with projects where Leviathan may be used in place of traditional requirements gathering and issue tracking tools.</p>




<h3>Hunting for Leviathan</h3>

<p>We used to believe the world was flat, so at least in one way, it was. Eventually, it became round, but it was not the world that was reconstructed - it was the ability to perceive a round world, to work within it and make use of it.</p>

<p>People think, and people believe that people think. People also sometimes think that things think - their computers, their cars, the spambots that send them emails, furbies. But, if people think about it at all, they believe that networks or crowds do not think, or they believe that the mob is only as smart as the stupidest member. But with well configured social computing this may not always be the case. To date this has been attempted by configuring the input - what sorts of questions, to whom; but now there is an opportunity to configure the output - what qualifies as a valuable answer. In fact, more so than this, there is now the opportunity for configuration to be built in - for those involved in the crowd, the social computer, to configure both input and output. The task then becomes one of building and maintaining a method of observance. This is the natural social computer, and studying it is like studying the Earth, or the stars, or gravity. It is a study of a natural system in an attempt to learn about it - not an attempt to build it.</p>

<p>Hobbes wrote that the Leviathan could be a way for a community to self-govern and avoid the pitfalls of concentration of power to one individual (or one small subset of individuals). But he was limited in his conception of where such power could reside (a monarch). We have since reconstructed that understanding of power, and we can do so again. Latour also mentions the Leviathan, describing it as a skein of network. We do not need to construct a Leviathan because we are an actor-network, and as such we are already a social computer - we just do not yet fully acknowledge it. Our social world is flat. Our test asks "are you one of us" instead of asking "what do you have to say?".</p>

<p>Leviathan will not be found in a test of being human, nor can it be considered artificial, because what we must (re)construct is <i>our</i> understanding of intelligence. We perch atop the totems of our society, ever reaching that little bit higher, constructing that little bit more, pretending we are safe in the knowledge that solidity is but a few inches away. But it is just something we think somebody else knows. Now, with so much information and communication available to us all, we may soon become technophilosophers - wise enough to know how much we don't know, and also to know that nobody else knows either. Maybe the Leviathan is nothing more than an open totem, one that can be constructed <i>and</i> believed in by anyone, and once we follow it for long enough we will learn what we are without it.</p>




<hr class="embossed"></hr>

<h2 class="ignore">References</h2>
<div id="references"></div>




<!--
<hr class="embossed"></hr>

<h2 class="ignore">Appendix A: Supporting software and data</h2>

<p>This is a copy of the content of the README file provided on the disc submitted with this thesis, describing the content of said disc and the role it plays in supporting this thesis. Refer to the digital submission for further information.</p>

<p>THIS FOLDER CONTAINS VARIOUS DATASETS AND SOFTWARE SUPPORTING THE THESIS.</p>

<p>NOTE: NONE OF THIS IS REQUIRED FOR UNDERSTANDING THE THESIS, IT IS SUPPLIED AS SUPPORTING MATERIAL ONLY. IT NEED NOT BE INSTALLED UNLESS FOR ADDITIONAL INTEREST.</p>

<p>TECHNOSOCIAL_DEVELOPMENT_IN_AN_OPEN_SOCIETY.PDF</p>

<p>A PDF copy of the thesis.</p>

<p>LEVIATHAN_SOFTWARE</p>

<p>This folder contains a git repository called leviathan, which is the software that runs the leviathan demonstrator application as it stood at time of submission. This software relies on elasticsearch (copy included). The live  repo is at http://github.com/cottagelabs/leviathan and if that remains available a newer version can probably be found there.</p>

<p>PHD_WEBSITE_SOFTWARE</p>

<p>This folder contains a git repository called phd, which is the software that runs the phd website demonstration, and the code that processed and stored the research data for display. This software requires elasticsearch (copy included) and also a running instance of etherpad-lite to connect to (copy included). The original repo is at http://github.com/markmacgillivray/phd and if that remains available there may be a newer version there, but probably not. The demonstration is running at http://phd.cottagelabs.com at time of submission.</p>

<p>SURVEY_VIDEOS</p>

<p>This folder contains a folder for each survey subject, and in each is an mp4 file of their interview and a document containing the transcript. There is also a transcripts.csv file that contains all the questions and answers from all the interviews in one file.</p>

<p>WEBSITE_SERVER_DATA</p>

<p>This folder contains json files with the records of each datatype used by the website server to drive the visualisations, as processed by the website software into the elasticsearch index. If the phd software and elasticsearch are run locally, a simple script to import all of these records into their respective index names would recreate the data state at the time of submission.</p>

<p>ELASTICSEARCH-0.90.13.TAR.GZ</p>

<p>A compressed file containing a compatible (pre 1.0) version of elasticsearch which can be installed by decompressing and entering, then running bin/elasticsearch. See http://www.elastic.co for further information.</p>

<p>ETHERPAD-LITE</p>

<p>A git repository of the etherpad-lite software, which should be installed to provide editable web pages for the phd website software. Then the phd website software should be configured to point to the address of this instance. See the etherpadlite/README.md file for installation instructions.</p>

<p>PHD_WEBSITE_PAGES</p>

<p>This folder contains files containing the html that is required to generate the phd website pages. If the software is setup and etherpad-lite and elasticsearch are running, then creating a new page of the same name (or /index for the main page) will open an etherpad into which the contents of these files could be copied, to replicate the site at time of submission. Most of the pages are displays of data, so for them to show anything the data from the website_server_data folder would also need to be uploaded to the running elasticsearch index.</p>

<p>THESIS_HTML</p>

<p>A web browser complete save of the thesis web page.</p>

<p>SUBMISSION_FORMS</p>

<p>This folder contains copies of the administrative forms that had to be completed to submit the thesis.</p>
-->



</div> <!-- end of thesis content -->


